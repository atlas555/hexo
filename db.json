{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/robots.txt","path":"robots.txt","modified":1,"renderable":0},{"_id":"source/media/15078859694350.jpg","path":"media/15078859694350.jpg","modified":0,"renderable":0},{"_id":"themes/vexozxl/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/js/qrious.js","path":"js/qrious.js","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/images/escheres.png","path":"css/images/escheres.png","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/images/alipay.png","path":"css/images/alipay.png","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/images/logo.png","path":"css/images/logo.png","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/images/menu.png","path":"css/images/menu.png","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/images/top.png","path":"css/images/top.png","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/plugins/gitment.css","path":"css/plugins/gitment.css","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/css/images/wechat.png","path":"css/images/wechat.png","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"themes/vexozxl/source/fonts/SourceSansPro.ttf","path":"fonts/SourceSansPro.ttf","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"4676fa4f92fc83cdeb54392715dfe092b37548f0","modified":1509614977000},{"_id":"source/robots.txt","hash":"608255051a06d1b6d899fae38bee075fe6d5d0d6","modified":1512439148000},{"_id":"themes/vexozxl/_config.yml","hash":"12fcc65d91d8b723b91d26e01e3f7fd70087b12f","modified":1508136416000},{"_id":"themes/vexozxl/lint.sh","hash":"f580302e4aa9ccfb95a253851da6501d145613fe","modified":1507789940000},{"_id":"themes/vexozxl/package.json","hash":"4a839847a872079d154a4884182a7bc773e76535","modified":1507789940000},{"_id":"source/404/index.md","hash":"32f11379878375b0ececf4514cdd51c52a64d2db","modified":1506583280000},{"_id":"source/_posts/技术分享：深度学习之正则化.md","hash":"b99068453f34bb949bd8332da5e4fc9ade18bbaa","modified":1512438287000},{"_id":"source/_posts/machine_learning-GAN_tec_share.md","hash":"6223481019278f746c10340b75f73fcd3265605a","modified":1507882749000},{"_id":"source/_posts/elasticsearch_tec_book_v1.01.md","hash":"b70c580dc7b10269bc92125534e4e7ebb595e4b1","modified":1506764155000},{"_id":"source/_posts/wide_deep_learning_for_rec_sys.md","hash":"54a9910a9e94211f984d0ad9f3f3b26b6d304cac","modified":1507889175000},{"_id":"source/_posts/自己动手打造推荐系统-一-常用推荐算法优势分析.md","hash":"98749a3a4f98ed3ab2fb6c6ce6b7287c44bb315d","modified":1506761934000},{"_id":"source/about/index.md","hash":"68c8af0453deca5f6b03eb18ee6f89fb0c1f1f75","modified":1506395533000},{"_id":"source/bookshelf/index.md","hash":"76b2607a8e36d4f246b7a436f8bc96d89e71543d","modified":1512439121000},{"_id":"source/resys/index.md","hash":"7e334ad163cacac557114d8fcd78d3d25f443310","modified":1512438439000},{"_id":"source/tags/index.md","hash":"80a15f1b5daff22b04849109e976bc91a410b83e","modified":1506395533000},{"_id":"source/search/index.md","hash":"25f79ca74c25c2fd943e9761aebd1c9b45c92252","modified":1506395532000},{"_id":"source/media/15078859694350.jpg","hash":"e61f7c66ac9022dbda2154dad6b75280734c29ba","modified":1507885969000},{"_id":"themes/vexozxl/layout/404.ejs","hash":"080d537f81c877a2a326ab121608edc16cdd79dd","modified":1507789940000},{"_id":"themes/vexozxl/layout/index.ejs","hash":"6c7358c1d6a3fb5ae19dea47cea708c9896809a2","modified":1507789940000},{"_id":"themes/vexozxl/layout/archive.ejs","hash":"cb12abb19cb70e90d410a6233933eedb3f2c033a","modified":1507789940000},{"_id":"themes/vexozxl/layout/about.ejs","hash":"9d0f12efdc59859aec21fd7c4acab9ac150a4cd5","modified":1507789940000},{"_id":"themes/vexozxl/layout/layout.ejs","hash":"a7b8f1debdca12d667ecd1bcc3d4bc6e13a23d7b","modified":1507789940000},{"_id":"themes/vexozxl/layout/tags.ejs","hash":"5b326e2bd3292b3015d0666b796544d7126acfda","modified":1507789940000},{"_id":"themes/vexozxl/layout/page.ejs","hash":"26b2d3725496836867a702a55a91277991fec1a4","modified":1507789940000},{"_id":"themes/vexozxl/layout/toc.ejs","hash":"223cff923b7b177ed7d4db9bef6f326dd290ea4f","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.DS_Store","hash":"edce1e94103911a82fe55e19abc2330e6eaf59b9","modified":1507789940000},{"_id":"themes/vexozxl/_source/project/index.md","hash":"b8f5482c157514bd2df4ce8a4e4d01a957497924","modified":1507789940000},{"_id":"themes/vexozxl/_source/about/index.md","hash":"68c8af0453deca5f6b03eb18ee6f89fb0c1f1f75","modified":1507789940000},{"_id":"themes/vexozxl/_source/tags/index.md","hash":"80a15f1b5daff22b04849109e976bc91a410b83e","modified":1507789940000},{"_id":"themes/vexozxl/layout/_partial/archive.ejs","hash":"9abbf14034d581569c0b6c992fe22035cb5306b3","modified":1507789940000},{"_id":"themes/vexozxl/layout/_partial/footer.ejs","hash":"d1ba79eaf22b9a7b01dbf31e5dcbbfce8c6a6aa4","modified":1507789940000},{"_id":"themes/vexozxl/layout/_partial/head.ejs","hash":"465406914e057c0fcd7fdd399df97bf191229359","modified":1508135543000},{"_id":"themes/vexozxl/layout/_partial/header.ejs","hash":"e544f516b23bc609cc6367190f380c879b935c21","modified":1507789940000},{"_id":"themes/vexozxl/layout/_partial/pager.ejs","hash":"3a1b9680fbfa3baa76933c7c17216996381ad241","modified":1507789940000},{"_id":"themes/vexozxl/layout/_partial/tag.ejs","hash":"5d2a2c3f8ca7000945ab426a0c6939421974b224","modified":1507789940000},{"_id":"themes/vexozxl/layout/_partial/top.ejs","hash":"f09dea486246a580213005b21d4b38810dd16fb3","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_config.styl","hash":"ac69c720d1699d2c93982b81c233c02982fc01be","modified":1507789940000},{"_id":"themes/vexozxl/source/css/style.styl","hash":"f2a32891e67d53d0414daa3255167296204d7f51","modified":1507789940000},{"_id":"themes/vexozxl/source/js/qrious.js","hash":"a9271e81e2ac6a692b1c133811afa33f0f3d7dc5","modified":1507789940000},{"_id":"themes/vexozxl/source/js/script.js","hash":"a19ed5f3c1d9c64855f162bce7ec66b47aada780","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/LICENSE","hash":"2fe6c1073604f5c099a17a5ec183b9015b6e02f3","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/index.js","hash":"adcfa6488fb6ff5ddf31d13b589d22dbc176bf96","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/.npmignore","hash":"0a861f1091bff14f09cdcf55bf72d2a7f1b4bd1b","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/README.md","hash":"624f19c7ea6bfbf91d3d60be9bd6d28c004b2d76","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/package.json","hash":"1bd898af40f6c47ed08304b5e94c429a5575b518","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/about.styl","hash":"e4a9bffe9c44c3179c021e2d924386ff9f758399","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/archive.styl","hash":"e80ddf26f2af3523632afeabd57f81592537985a","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/footer.styl","hash":"acc26664e5b3bdb40534496234a66fca2994e905","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/header.styl","hash":"def3a6938d925c585a7da6256a6f2e90f3b7d61e","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/markdown.styl","hash":"fead738af515436dff5c05bffcae4da929ae75e9","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/pager.styl","hash":"888384c67429c7568aa38b5ebe5acae3cc4de367","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/project.styl","hash":"e9b6faadf4852bce3a4141cba0a102a7afb81e9f","modified":1507789940000},{"_id":"themes/vexozxl/source/css/_partial/tags.styl","hash":"5198a7f7c221341138ae5c65185e86b6e13e8e26","modified":1507789940000},{"_id":"themes/vexozxl/source/css/images/escheres.png","hash":"55deece3236dcc2fb44c28dec3e8bacbb7b46542","modified":1507789940000},{"_id":"themes/vexozxl/source/css/images/alipay.png","hash":"9f6b1c1389daf4d4725e1dd0649882463940687b","modified":1507789940000},{"_id":"themes/vexozxl/source/css/images/logo.png","hash":"3cc2fc6bda241e89369e1cef5887c31368528663","modified":1507789940000},{"_id":"themes/vexozxl/source/css/images/menu.png","hash":"bdaa35eb1ed119caeb934e15a05b9f4a5396d957","modified":1507789940000},{"_id":"themes/vexozxl/source/css/images/top.png","hash":"611a257907474ca02828319f81b006c1d818bb84","modified":1507789940000},{"_id":"themes/vexozxl/source/css/plugins/gitment.css","hash":"541ff18d7f3542b5663dc6aad06d43e135332b71","modified":1507789940000},{"_id":"themes/vexozxl/source/css/images/wechat.png","hash":"047933e41de09c3931e5c16fb008038bb55aec8e","modified":1507789940000},{"_id":"themes/vexozxl/source/js/gitment.js","hash":"376446d9c5930576016f97dd63e5e6616c94d8d4","modified":1507789940000},{"_id":"themes/vexozxl/source/fonts/SourceSansPro.ttf","hash":"1e9f0372c269da205fdbac8cf27cb9cf59f6ad45","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/lib/M.js","hash":"e1cad9515147544f2a3edaf6e999033275ea4f26","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/lib/Z.js","hash":"794d12289bb4ac77e1b97b3965a69c35813ef289","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/lib/N.js","hash":"35a5ff6b2274af2ade459d0141cf53275ed81b4c","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/lib/uslug.js","hash":"5ebbff4e7c9f8dd8b629dd674ec50c5790d6cfc7","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/test/test.js","hash":"845d084bde9afb8d49274c1fbee586544e7f81ad","modified":1507789940000},{"_id":"themes/vexozxl/node_modules/.staging/uslug-d79edd83/lib/L.js","hash":"c5f59c21644741cd89efa99b86dc13b9e64a4abe","modified":1507789940000},{"_id":"public/baidusitemap.xml","hash":"95b3456d56ecbc598dc59030c22e19f1dfa80bb3","modified":1512439174306},{"_id":"public/atom.xml","hash":"33ee30e05a60a474c23830725c539138beff062b","modified":1512438292590},{"_id":"public/sitemap.xml","hash":"2365d47e0a10157c71ba25be57e601f97851b16a","modified":1512439174381},{"_id":"public/404.html","hash":"6894b6c6c024e353b8ef8c45ee3fcd605ccfc85e","modified":1512438217065},{"_id":"public/about/index.html","hash":"2e4516d26cc5f69dc1bd732720a3f29475bc9941","modified":1512438217068},{"_id":"public/bookshelf/index.html","hash":"9583304d255af9c7d391fb68fa3d9a29f79ceb72","modified":1512439174389},{"_id":"public/resys/index.html","hash":"3049036b638b774a6c49e407e4f623b1ee1cc12a","modified":1512438446197},{"_id":"public/tags/index.html","hash":"2cd46aed8ba7c80953240cafdbc1f58f7510a67a","modified":1512439174390},{"_id":"public/search/index.html","hash":"2c81caa8221c405418336f6f15bb6c29867e3063","modified":1512438217069},{"_id":"public/deep-learning/deep_learning_gan_tec_share.html","hash":"2df4feba2e99b2952261139d6855b55e489e7628","modified":1512438217069},{"_id":"public/archives/index.html","hash":"a2d891a88b3dbbb0edb739a9ac8762f198a16827","modified":1512438217069},{"_id":"public/archives/2017/index.html","hash":"6bac6cc37aa82b433d455034a7636849bb58d410","modified":1512438217069},{"_id":"public/archives/2017/09/index.html","hash":"32a7e4e00cccea8511dc2850bdb8c0f318afd785","modified":1512438217069},{"_id":"public/archives/2017/10/index.html","hash":"930b8006c018bad4ab5b68adbdd8b769ba9d905b","modified":1512438217069},{"_id":"public/categories/deep-learning/index.html","hash":"5b3c63089bc6622dc1c59366266db1a3ba039725","modified":1512438217069},{"_id":"public/categories/resys/index.html","hash":"72a1e957813963facf4714cd667a3dae318f26e1","modified":1512438217069},{"_id":"public/categories/search/index.html","hash":"ef7f3e35f29ae456985c688f43f67a05b8dbaa06","modified":1512438217069},{"_id":"public/index.html","hash":"cc5559ca36d5fc4b7f3e399cac965ae8ff728cde","modified":1512438217070},{"_id":"public/tags/deep-learning/index.html","hash":"22c6cc14c02f407e7081f4dd4d417f09eb1040ad","modified":1512438217070},{"_id":"public/tags/regularization/index.html","hash":"a106110960a7b270bb746998d5f44aea56a26550","modified":1512438217070},{"_id":"public/tags/GAN/index.html","hash":"97b290bd3ef2275a801c7534796816ec51b0bc70","modified":1512438217070},{"_id":"public/tags/search/index.html","hash":"72113955098dd87d75b7cf58249423847c057eb9","modified":1512438217070},{"_id":"public/tags/resys/index.html","hash":"aa93ce0803afd02f8d37b5c9ad4715ac5e38abdb","modified":1512438217070},{"_id":"public/deep-learning/deep_learning_regularization_tec_share.html","hash":"170d6ff965f955368d0e1de2c0ae6676876c0d88","modified":1512438292595},{"_id":"public/resys/wide_deep_learning_for_rec_sys.html","hash":"dfbb2430ab97ebcc7e2bdde632f7303fe7f007be","modified":1512438217070},{"_id":"public/resys/resys_common_algorithm_analyse.html","hash":"5b290359b98efbf11b46610c24ff89816eae7ddc","modified":1512438217070},{"_id":"public/search/elasticsearch_tec_book.html","hash":"93d0762864b215110c4ab53c4b63955746ce9c1e","modified":1512438217070},{"_id":"public/archives/2017/12/index.html","hash":"5deeaadae33a1255f8453f7d3177e16456f818dd","modified":1512438217073},{"_id":"public/robots.txt","hash":"608255051a06d1b6d899fae38bee075fe6d5d0d6","modified":1512439174391},{"_id":"public/media/15078859694350.jpg","hash":"e61f7c66ac9022dbda2154dad6b75280734c29ba","modified":1512438217076},{"_id":"public/css/images/escheres.png","hash":"55deece3236dcc2fb44c28dec3e8bacbb7b46542","modified":1512438217076},{"_id":"public/css/images/alipay.png","hash":"9f6b1c1389daf4d4725e1dd0649882463940687b","modified":1512438217076},{"_id":"public/css/images/logo.png","hash":"3cc2fc6bda241e89369e1cef5887c31368528663","modified":1512438217076},{"_id":"public/css/images/menu.png","hash":"bdaa35eb1ed119caeb934e15a05b9f4a5396d957","modified":1512438217077},{"_id":"public/css/images/top.png","hash":"611a257907474ca02828319f81b006c1d818bb84","modified":1512438217077},{"_id":"public/css/images/wechat.png","hash":"047933e41de09c3931e5c16fb008038bb55aec8e","modified":1512438217077},{"_id":"public/js/script.js","hash":"a19ed5f3c1d9c64855f162bce7ec66b47aada780","modified":1512438217427},{"_id":"public/css/style.css","hash":"91599cbf54c671afad2ec2c6a23bafc132e54050","modified":1512438217427},{"_id":"public/js/qrious.js","hash":"a9271e81e2ac6a692b1c133811afa33f0f3d7dc5","modified":1512438217427},{"_id":"public/css/plugins/gitment.css","hash":"541ff18d7f3542b5663dc6aad06d43e135332b71","modified":1512438217427},{"_id":"public/js/gitment.js","hash":"376446d9c5930576016f97dd63e5e6616c94d8d4","modified":1512438217427},{"_id":"public/fonts/SourceSansPro.ttf","hash":"1e9f0372c269da205fdbac8cf27cb9cf59f6ad45","modified":1512438217428}],"Category":[{"name":"deep learning","_id":"cjasynwp800046ls6cwq8w193"},{"name":"resys","_id":"cjasynwpi000g6ls6f0hdiai3"},{"name":"search","_id":"cjasynwpj000l6ls6og5zb59x"}],"Data":[],"Page":[{"title":"404 Not Found：该页无法显示","date":"2017-09-27T10:57:29.000Z","comments":0,"layout":"404","_content":"","source":"404/index.md","raw":"---\npermalink: /404\ntitle: 404 Not Found：该页无法显示\ndate: 2017-09-27 18:57:29\ncomments: false\nlayout: 404\n---\n","updated":"2017-09-28T07:21:20.000Z","path":"/404.html","_id":"cjasynwp000006ls61slg9787","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"About","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: About\nlayout: about\n---","date":"2017-09-26T03:12:13.000Z","updated":"2017-09-26T03:12:13.000Z","path":"about/index.html","comments":1,"_id":"cjasynwp600026ls6ochjgo43","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Bookshelf","id":1949,"comment":false,"date":"2017-06-03T06:12:10.000Z","_content":"\n# 那些年度过的时间\n\n# Book\n\n## 2017年\n（1）**信息检索导论**  - 读了70%\n（2）**项亮：推荐系统实践** - 读了两遍\n（3）**Lan GoodeFellow:deep learning** - 读到了模型优化部分\n（4）**周志华：机器学习** - 挑着读几部分\n\n## 2017前\n(1) **Tcp/ip 详解（卷1）** – 评分：7.0\n(2)**Java编程思想**\n(3)**Python CookBook**\n(4)** 大型网站技术架构 – 核心原理与案例分析**\n(4) **本色（乐嘉）**\n(6) **请给我结果**\n(7)**番茄工作方法图解**\n(8)**黑客与画家**\n(9) **刘慈欣：三体** \n(10) **趁年轻，折腾吧（袁岳）**\n(11) **看见（柴静）**\n\n# Paper\n## 2017\n（1）Wide & Deep Learning for Recommender Systems\n（2）An introduction to ROC analysis\n（3）Deep Neural Networks for YouTube Recommendations\n（4）DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\n（5）Large-Scale Sparse Logistic Regression\n（6）Practical Lessons from Predicting Clicks on Ads at Facebook\n\n## 2017前\n(1) Bigtable:A Distributed Storage System for Structured Data ([download](http://document-save.qiniudn.com/zhangxiaolong.orgbigtable-osdi06.pdf))\n(2) GFS:The Google File System ([download](http://document-save.qiniudn.com/zhangxiaolong.orggfs-sosp2003.pdf))\n(3) MapReduce:MapReduce: Simplied Data Processing on Large Clusters ([download](http://document-save.qiniudn.com/zhangxiaolong.orgmapreduce-osdi04.pdf))\n(4) Spanner:Google’s Globally-Distributed Database ([download](http://document-save.qiniudn.com/zhangxiaolong.orgspanner-osdi2012.pdf))\n(5) Megastore: Providing Scalable, Highly Available Storage for Interactive Services ([download](http://document-save.qiniudn.com/zhangxiaolong.orgmegastore.pdf))\n(6) Pregel:A System for Large-Scale Graph Processing ([download](http://document-save.qiniudn.com/zhangxiaolong.orgPregel.pdf))\n(7) Dremel: Interactive Analysis of Web-Scale Datasets ([download](http://document-save.qiniudn.com/zhangxiaolong.orgDremel.pdf))\n(8) Caffeine: Large-scale Incremental Processing Using Distributed Transactions and Notications ([download](http://document-save.qiniudn.com/zhangxiaolong.orgCaffeine%20-1.pdf))\n(9) Search The Web ([download](http://document-save.qiniudn.com/web%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%A6%82%E8%AE%BA%E2%80%94%E2%80%94searching_the_web.pdf))\n\n# 年终总结\n（1）2017年自我总结-Atlas","source":"bookshelf/index.md","raw":"---\ntitle: Bookshelf\nid: 1949\ncomment: false\ndate: 2017-06-03 14:12:10\n---\n\n# 那些年度过的时间\n\n# Book\n\n## 2017年\n（1）**信息检索导论**  - 读了70%\n（2）**项亮：推荐系统实践** - 读了两遍\n（3）**Lan GoodeFellow:deep learning** - 读到了模型优化部分\n（4）**周志华：机器学习** - 挑着读几部分\n\n## 2017前\n(1) **Tcp/ip 详解（卷1）** – 评分：7.0\n(2)**Java编程思想**\n(3)**Python CookBook**\n(4)** 大型网站技术架构 – 核心原理与案例分析**\n(4) **本色（乐嘉）**\n(6) **请给我结果**\n(7)**番茄工作方法图解**\n(8)**黑客与画家**\n(9) **刘慈欣：三体** \n(10) **趁年轻，折腾吧（袁岳）**\n(11) **看见（柴静）**\n\n# Paper\n## 2017\n（1）Wide & Deep Learning for Recommender Systems\n（2）An introduction to ROC analysis\n（3）Deep Neural Networks for YouTube Recommendations\n（4）DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\n（5）Large-Scale Sparse Logistic Regression\n（6）Practical Lessons from Predicting Clicks on Ads at Facebook\n\n## 2017前\n(1) Bigtable:A Distributed Storage System for Structured Data ([download](http://document-save.qiniudn.com/zhangxiaolong.orgbigtable-osdi06.pdf))\n(2) GFS:The Google File System ([download](http://document-save.qiniudn.com/zhangxiaolong.orggfs-sosp2003.pdf))\n(3) MapReduce:MapReduce: Simplied Data Processing on Large Clusters ([download](http://document-save.qiniudn.com/zhangxiaolong.orgmapreduce-osdi04.pdf))\n(4) Spanner:Google’s Globally-Distributed Database ([download](http://document-save.qiniudn.com/zhangxiaolong.orgspanner-osdi2012.pdf))\n(5) Megastore: Providing Scalable, Highly Available Storage for Interactive Services ([download](http://document-save.qiniudn.com/zhangxiaolong.orgmegastore.pdf))\n(6) Pregel:A System for Large-Scale Graph Processing ([download](http://document-save.qiniudn.com/zhangxiaolong.orgPregel.pdf))\n(7) Dremel: Interactive Analysis of Web-Scale Datasets ([download](http://document-save.qiniudn.com/zhangxiaolong.orgDremel.pdf))\n(8) Caffeine: Large-scale Incremental Processing Using Distributed Transactions and Notications ([download](http://document-save.qiniudn.com/zhangxiaolong.orgCaffeine%20-1.pdf))\n(9) Search The Web ([download](http://document-save.qiniudn.com/web%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%A6%82%E8%AE%BA%E2%80%94%E2%80%94searching_the_web.pdf))\n\n# 年终总结\n（1）2017年自我总结-Atlas","updated":"2017-12-05T01:58:41.000Z","path":"bookshelf/index.html","_id":"cjasynwpa00066ls6be9f9flc","comments":1,"layout":"page","content":"<h1 id=\"那些年度过的时间\"><a href=\"#那些年度过的时间\" class=\"headerlink\" title=\"那些年度过的时间\"></a>那些年度过的时间</h1><h1 id=\"Book\"><a href=\"#Book\" class=\"headerlink\" title=\"Book\"></a>Book</h1><h2 id=\"2017年\"><a href=\"#2017年\" class=\"headerlink\" title=\"2017年\"></a>2017年</h2><p>（1）<strong>信息检索导论</strong>  - 读了70%<br>（2）<strong>项亮：推荐系统实践</strong> - 读了两遍<br>（3）<strong>Lan GoodeFellow:deep learning</strong> - 读到了模型优化部分<br>（4）<strong>周志华：机器学习</strong> - 挑着读几部分</p>\n<h2 id=\"2017前\"><a href=\"#2017前\" class=\"headerlink\" title=\"2017前\"></a>2017前</h2><p>(1) <strong>Tcp/ip 详解（卷1）</strong> – 评分：7.0<br>(2)<strong>Java编程思想</strong><br>(3)<strong>Python CookBook</strong><br>(4)<strong> 大型网站技术架构 – 核心原理与案例分析</strong><br>(4) <strong>本色（乐嘉）</strong><br>(6) <strong>请给我结果</strong><br>(7)<strong>番茄工作方法图解</strong><br>(8)<strong>黑客与画家</strong><br>(9) <strong>刘慈欣：三体</strong><br>(10) <strong>趁年轻，折腾吧（袁岳）</strong><br>(11) <strong>看见（柴静）</strong></p>\n<h1 id=\"Paper\"><a href=\"#Paper\" class=\"headerlink\" title=\"Paper\"></a>Paper</h1><h2 id=\"2017\"><a href=\"#2017\" class=\"headerlink\" title=\"2017\"></a>2017</h2><p>（1）Wide &amp; Deep Learning for Recommender Systems<br>（2）An introduction to ROC analysis<br>（3）Deep Neural Networks for YouTube Recommendations<br>（4）DeepFM: A Factorization-Machine based Neural Network for CTR Prediction<br>（5）Large-Scale Sparse Logistic Regression<br>（6）Practical Lessons from Predicting Clicks on Ads at Facebook</p>\n<h2 id=\"2017前-1\"><a href=\"#2017前-1\" class=\"headerlink\" title=\"2017前\"></a>2017前</h2><p>(1) Bigtable:A Distributed Storage System for Structured Data (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgbigtable-osdi06.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(2) GFS:The Google File System (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orggfs-sosp2003.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(3) MapReduce:MapReduce: Simplied Data Processing on Large Clusters (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgmapreduce-osdi04.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(4) Spanner:Google’s Globally-Distributed Database (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgspanner-osdi2012.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(5) Megastore: Providing Scalable, Highly Available Storage for Interactive Services (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgmegastore.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(6) Pregel:A System for Large-Scale Graph Processing (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgPregel.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(7) Dremel: Interactive Analysis of Web-Scale Datasets (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgDremel.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(8) Caffeine: Large-scale Incremental Processing Using Distributed Transactions and Notications (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgCaffeine%20-1.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(9) Search The Web (<a href=\"http://document-save.qiniudn.com/web%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%A6%82%E8%AE%BA%E2%80%94%E2%80%94searching_the_web.pdf\" target=\"_blank\" rel=\"external\">download</a>)</p>\n<h1 id=\"年终总结\"><a href=\"#年终总结\" class=\"headerlink\" title=\"年终总结\"></a>年终总结</h1><p>（1）2017年自我总结-Atlas</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"那些年度过的时间\"><a href=\"#那些年度过的时间\" class=\"headerlink\" title=\"那些年度过的时间\"></a>那些年度过的时间</h1><h1 id=\"Book\"><a href=\"#Book\" class=\"headerlink\" title=\"Book\"></a>Book</h1><h2 id=\"2017年\"><a href=\"#2017年\" class=\"headerlink\" title=\"2017年\"></a>2017年</h2><p>（1）<strong>信息检索导论</strong>  - 读了70%<br>（2）<strong>项亮：推荐系统实践</strong> - 读了两遍<br>（3）<strong>Lan GoodeFellow:deep learning</strong> - 读到了模型优化部分<br>（4）<strong>周志华：机器学习</strong> - 挑着读几部分</p>\n<h2 id=\"2017前\"><a href=\"#2017前\" class=\"headerlink\" title=\"2017前\"></a>2017前</h2><p>(1) <strong>Tcp/ip 详解（卷1）</strong> – 评分：7.0<br>(2)<strong>Java编程思想</strong><br>(3)<strong>Python CookBook</strong><br>(4)<strong> 大型网站技术架构 – 核心原理与案例分析</strong><br>(4) <strong>本色（乐嘉）</strong><br>(6) <strong>请给我结果</strong><br>(7)<strong>番茄工作方法图解</strong><br>(8)<strong>黑客与画家</strong><br>(9) <strong>刘慈欣：三体</strong><br>(10) <strong>趁年轻，折腾吧（袁岳）</strong><br>(11) <strong>看见（柴静）</strong></p>\n<h1 id=\"Paper\"><a href=\"#Paper\" class=\"headerlink\" title=\"Paper\"></a>Paper</h1><h2 id=\"2017\"><a href=\"#2017\" class=\"headerlink\" title=\"2017\"></a>2017</h2><p>（1）Wide &amp; Deep Learning for Recommender Systems<br>（2）An introduction to ROC analysis<br>（3）Deep Neural Networks for YouTube Recommendations<br>（4）DeepFM: A Factorization-Machine based Neural Network for CTR Prediction<br>（5）Large-Scale Sparse Logistic Regression<br>（6）Practical Lessons from Predicting Clicks on Ads at Facebook</p>\n<h2 id=\"2017前-1\"><a href=\"#2017前-1\" class=\"headerlink\" title=\"2017前\"></a>2017前</h2><p>(1) Bigtable:A Distributed Storage System for Structured Data (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgbigtable-osdi06.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(2) GFS:The Google File System (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orggfs-sosp2003.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(3) MapReduce:MapReduce: Simplied Data Processing on Large Clusters (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgmapreduce-osdi04.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(4) Spanner:Google’s Globally-Distributed Database (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgspanner-osdi2012.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(5) Megastore: Providing Scalable, Highly Available Storage for Interactive Services (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgmegastore.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(6) Pregel:A System for Large-Scale Graph Processing (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgPregel.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(7) Dremel: Interactive Analysis of Web-Scale Datasets (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgDremel.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(8) Caffeine: Large-scale Incremental Processing Using Distributed Transactions and Notications (<a href=\"http://document-save.qiniudn.com/zhangxiaolong.orgCaffeine%20-1.pdf\" target=\"_blank\" rel=\"external\">download</a>)<br>(9) Search The Web (<a href=\"http://document-save.qiniudn.com/web%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%A6%82%E8%AE%BA%E2%80%94%E2%80%94searching_the_web.pdf\" target=\"_blank\" rel=\"external\">download</a>)</p>\n<h1 id=\"年终总结\"><a href=\"#年终总结\" class=\"headerlink\" title=\"年终总结\"></a>年终总结</h1><p>（1）2017年自我总结-Atlas</p>\n"},{"title":"resys","date":"2017-09-26T01:52:24.000Z","_content":"### 自己动手实现推荐系统系列\n\n[自己动手实现推荐系统：一.常用推荐算法优势分析](http://www.zhangxiaolong.org/resys/resys_common_algorithm_analyse.html)\n[自己动手实现推荐系统：二.google论文wide and deep learning for rec system分析](http://www.zhangxiaolong.org/resys/wide_deep_learning_for_rec_sys.html)\n\n\n### 深度学习系列\n[深度学习：一.深度学习之GAN分享](http://www.zhangxiaolong.org/deep-learning/deep_learning_gan_tec_share.html)\n[深度学习：二.深度学习中的正则化](http://www.zhangxiaolong.org/deep-learning/deep_learning_regularization_tec_share.html)","source":"resys/index.md","raw":"title: resys\ndate: 2017-09-26 09:52:24\n---\n### 自己动手实现推荐系统系列\n\n[自己动手实现推荐系统：一.常用推荐算法优势分析](http://www.zhangxiaolong.org/resys/resys_common_algorithm_analyse.html)\n[自己动手实现推荐系统：二.google论文wide and deep learning for rec system分析](http://www.zhangxiaolong.org/resys/wide_deep_learning_for_rec_sys.html)\n\n\n### 深度学习系列\n[深度学习：一.深度学习之GAN分享](http://www.zhangxiaolong.org/deep-learning/deep_learning_gan_tec_share.html)\n[深度学习：二.深度学习中的正则化](http://www.zhangxiaolong.org/deep-learning/deep_learning_regularization_tec_share.html)","updated":"2017-12-05T01:47:19.000Z","path":"resys/index.html","_id":"cjasynwpc00086ls65k6a5nqe","comments":1,"layout":"page","content":"<h3 id=\"自己动手实现推荐系统系列\"><a href=\"#自己动手实现推荐系统系列\" class=\"headerlink\" title=\"自己动手实现推荐系统系列\"></a>自己动手实现推荐系统系列</h3><p><a href=\"http://www.zhangxiaolong.org/resys/resys_common_algorithm_analyse.html\">自己动手实现推荐系统：一.常用推荐算法优势分析</a><br><a href=\"http://www.zhangxiaolong.org/resys/wide_deep_learning_for_rec_sys.html\">自己动手实现推荐系统：二.google论文wide and deep learning for rec system分析</a></p>\n<h3 id=\"深度学习系列\"><a href=\"#深度学习系列\" class=\"headerlink\" title=\"深度学习系列\"></a>深度学习系列</h3><p><a href=\"http://www.zhangxiaolong.org/deep-learning/deep_learning_gan_tec_share.html\">深度学习：一.深度学习之GAN分享</a><br><a href=\"http://www.zhangxiaolong.org/deep-learning/deep_learning_regularization_tec_share.html\">深度学习：二.深度学习中的正则化</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"自己动手实现推荐系统系列\"><a href=\"#自己动手实现推荐系统系列\" class=\"headerlink\" title=\"自己动手实现推荐系统系列\"></a>自己动手实现推荐系统系列</h3><p><a href=\"http://www.zhangxiaolong.org/resys/resys_common_algorithm_analyse.html\">自己动手实现推荐系统：一.常用推荐算法优势分析</a><br><a href=\"http://www.zhangxiaolong.org/resys/wide_deep_learning_for_rec_sys.html\">自己动手实现推荐系统：二.google论文wide and deep learning for rec system分析</a></p>\n<h3 id=\"深度学习系列\"><a href=\"#深度学习系列\" class=\"headerlink\" title=\"深度学习系列\"></a>深度学习系列</h3><p><a href=\"http://www.zhangxiaolong.org/deep-learning/deep_learning_gan_tec_share.html\">深度学习：一.深度学习之GAN分享</a><br><a href=\"http://www.zhangxiaolong.org/deep-learning/deep_learning_regularization_tec_share.html\">深度学习：二.深度学习中的正则化</a></p>\n"},{"title":"Tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\nlayout: tags\n---\n","date":"2017-09-26T03:12:13.000Z","updated":"2017-09-26T03:12:13.000Z","path":"tags/index.html","comments":1,"_id":"cjasynwpd000a6ls62en73hgb","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Search","id":1842,"comment":false,"date":"2017-05-22T04:05:33.000Z","_content":"","source":"search/index.md","raw":"---\ntitle: Search\nid: 1842\ncomment: false\ndate: 2017-05-22 12:05:33\n---\n","updated":"2017-09-26T03:12:12.000Z","path":"search/index.html","comments":1,"layout":"page","_id":"cjasynwph000e6ls680noootp","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"深度学习之正则化技术分享","date":"2017-12-05T01:43:17.000Z","english_title":"deep_learning_regularization_tec_share","toc":false,"_content":"\n本文是团队内部本人的一个技术分享-深度学习最近的热门之一：深度学习中心问题之一正则化\n\n## 背景介绍\n\n机器学习的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。 在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。 这些策略被统称为正则化。 正则化的目的就是防止出现过拟合。事实上，开发更有效的正则化策略已成为本领域的主要研究工作之一。\n\n## 我的PPT分享\n\n{% pdf http://image.zhangxiaolong.org/file/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E4%BA%AB.pdf %}\n\n### ppt内容：\n目录\n1. 机器学习基本概念回顾\n\n2. 正则化（Regularization）\n\n3. 模型优化（Model  Optimization）\n\nWhat  is  Machine  Learning?\n\nMachine Learningis afiled of study that  computers  the  ability  to  learn  without  being  explicity  programmed   -‐  Arthur  Samuel  (1959)\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.     --   Tom Mitchell (1988)\n\nMachine  learning  is  the  study  of  computer  algorithms  that  improve  automatically  through  experience             --   Tom  Mitchell  (1998)\n\ntasks T、 experience E、 measure P\n\n-Task  T：分类（f  :  R^n->(1,…k)）、聚类、回归（f:R^n->R）、转录（表达、word2Vec）、机器翻译、结构化输出、异常检测（信用卡检测、阿里支付）、合成和采样（gan、音频）、缺失值填补、去噪（马赛克、条件概率分布）、密度估计或概率质量函数估计\n\n-Measure  P：评估学习算法的能力，设计其性能的度量（准确率、样本上概率对数的平均值），P是特定于系统执行的任务T的\n\n-Experience  E：\n\n-数据集、样本、特征\n\n-无监督算法：概率分布P(x)\n\n-监督算法：估计P(y|x)\n\n-\n\n构建机器学习算法\n\n-机器学习算法：特定的数据集、代价函数、优化过程和模型\n\n-Ex: 线性回归算法组成成分有X和y构成的数据集，损失函数是\n\n\n\n-模型是 ，在多数情况下优化算法可以定义为求解损失函数梯度为0的解\n\n-最常见的损失函数是负对数似然，最小化损失函数导致的最大似然估计\n\n-损失函数有可能有附加选，如正则化，\n\n-\n\n\n-非线性的模型，需要选择迭代数值优化过程（SGD.et）\n\n机器学习领域中心问题\n\n-\n\n1.Regularization（正则化）\n\n●\n\n2.Model  Optimization（模型优化）\n\n-为什么需要正则化？- 防止过拟合\n\n\n-什么是正则化？目的是什么？\n\n-对学习算法的一些修改策略----旨在减少泛化误差而不是训练误差\n\n-\n\n-正则化策略会对估计进行正则化\n\n-估计的正则化以偏差的增加换取方差的减少\n\n-\n\nBias(偏差)，Error(误差)，和Variance(方差)\n\nError = Bias + Variance + Noise\n\nbias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距\n\n要想在bias上表现好，low bias，就是复杂化模型（过拟合）\n\n要想在varience上表现好，low varience，就要简化模型，减少模型的参数（欠拟合）\n\nbias和variance的选择是一个tradeoff\n\nRegularization（正则化策略）\n\n1.参数范数惩罚(L1、L2)\n\n2.多任务学习\n\n3.提前终止\n\n4.参数绑定和参数共享\n\n5.稀疏表示\n\n●\n\n●\n\n参数范数惩罚（L1范数，L2范数）\n\n通过对目标函数J添加一个参数范数惩罚Ω(θ) \n\nL1正则化和L2正则化可以看做是损失函数的惩罚项\n\nL1正则化是指权值向量w中各个元素的绝对值之和，通常表示为〖∥w∥〗_1\n\nL2正则化是指权值向量w中各个元素的平方和然后再求平方根，通常表示为〖∥w∥〗_2\n\nL1和L2正则化的作用\n\nL1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择\n\nL2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合\n\nL1正则化和特征选择\n\n带L1正则化的损失函数\n\n在原始损失函数J0后添加L1正则化项时，相当于对J_0做了一个约束。\n\n令L=α∑16_w▒〖|w|〗，则J= J_0 +L，此时我们的任务变成在L约束下求出J_0取最小值的解\n\n二维的情况(只有两个权值w1和w2)，求解J_0的过程可以画出等值线，L1正则化的函数L在二维平面上画出\n\n当J_0等值线与L图形首次相交的地方就是最优解，顶点的值是(w^1,  w^2)=(0,w)\n\n正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大\n\n带L2正则化的损失函数\n\n二维平面下L2正则化的函数图形是个圆\n\nJ_0与L相交时使得w1或w2等于零的机率小许多\n\n为什么L2正则化不具有稀疏性的原因\n\nL2正则化和过拟合\n\n拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型\n\n线性回归中的梯度下降法为例，线性回归的代价函数如下\n\n在梯度下降法中，最终用于迭代计算参数θ的迭代式为\n\n每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的（权重衰减收缩的）\n\n多任务学习\n\n多任务学习是一种归纳迁移机制基本目标是提高泛化性能。\n\n多任务学习在学习一个问题的同时，可以通过使用共享表示来获得其他相关问题的知识\n\n多任务学习的基本假设是多个任务之间具有相关性，因此能够利用任务之间的相关性互相促进。\n\n多任务深度学习已经广泛应用于人脸识别、细粒度车辆分类、面部关键点定位与属性分类等多个领域\n\n提前终止\n\nEarly stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。\n\n具体算法（“No-improvement-in-n”）：\n\n在每一个Epoch结束时计算validation  data的accuracy，当accuracy不再提高时，就停止训练。\n\n怎样才认为validation  accurary不再提高？\n\n在训练的过程中，记录到目前为止最好的validation  accuracy\n\n当连续10次Epoch（或者更多次）没达到最佳accuracy时，可以认为accuracy不再提高。此时便可以停止迭代了\n\n参数绑定、参数共享\n\n全连接层缺点是参数很多，卷积层可以减少参数，减少计算量，因为卷积层的参数共享特性。\n\n卷积神经网络中的参数共享\n\nCNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。\n\nhttp://blog.csdn.net/zouxy09/article/details/8781543\n\nhttps://www.zhihu.com/question/47158818\n\n稀疏表示\n\n稀疏表示的本质：用尽可能少的资源表示尽可能多的知识，这种表示还能带来一个附加的好处，即计算速度快。\n\n稀疏表示就是指训练出的参数是稀疏的，即包含很多的0\n\n适应性（泛化能力）和稀疏性之间找平衡，最优取决于代价函数。\n\n稀疏表达的意义在于降维\n\n这种降维主要表现于虽然原始信号 的维度很高，但实际的有效信息集中在一个低维的空间里。这种性质使得不适定的问题变得适定(well-posed)，进而获得“好的解”成为可能\n\nReLU  - 神经网络的一种激活函数，可以做到稀疏表示\n\nL0、L1、L2\n\nBagging、集成方法\n\nBagging结合多个模型降低泛化误差的技术\n\n分别训练几个不同模型，所有模型表决测试样例输出，称为模型平均，采用这种策略的技术称为集成方法\n\n模型平均有效的原因\n\n不同模型通常不会在测试集上产生完全相同的误差\n\nBagging是一种允许重复多次使用同一种模型、训练算法和目标函数的方法\n\nBagging涉及构造 k 个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。\n\nBagging、集成方法\n\nDropout\n\nDropout是hintion最近2年提出的\n\n源于其文章Improving neural networks by preventing co-adaptation of feature detectors（通过阻止特征检测器的共同作用来提高神经网络的性能）\n\nL1、L2正则化是通过修改代价函数来实现的\n\nDropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）\n\nDropout\n\n一次迭代的过程\n\n在训练开始时，我们随机地“删除”一半的隐层单元，视它们为不存在\n\n保持输入输出层不变，按照BP算法更新上图神经网络中的权值\n\nDropout\n\nDropout优点\n\n计算方便。训练过程中使用Dropout产生 n 个随机二进制数与状态相乘即可。每个样本每次更新的时间复杂度： O(n)，空间复杂度： O(n)。\n\n适用广。Dropout不怎么限制适用的模型或训练过程，几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。包括：前馈神经网络、概率模型、受限波尔兹曼机、循环神经网络等。\n\n相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效。也可与其他形式的正则化合并，得到进一步提升。\n\nDropout缺点\n\n不适合宽度太窄的网络。否则大部分网络没有输入到输出的路径。\n\n不适合训练数据太小（如小于5000）的网络。训练数据太小时，Dropout没有其他方法表现好。\n\n不适合非常大的数据集。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处。\n\n对抗训练\n\n深度学习中对抗样本问题\n\n数据集中通过故意添加细微的干扰所形成的输入样本，受干扰之后的输入导致模型以高置信度给出一个错误的输出\n\n深度学习的对抗训练\n\n所谓深度学习对抗训练，就是通过在对抗样本上训练模型\n\nIan Goodfellow：对抗样本是一个非常难的问题 \n\n对抗训练\n\n切面距离、正切传播和  流行正切分类器\n\n流形 (manifold) 指连接在一起的区域 \n\n数学上，它是指一组点，且每个点都有 其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间 \n\n每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置 \n\n机器学习算法学习 Rn 上的所有感兴趣的函数，很多机器学 习问题看上去都是不可解的。\n\n流形学习 (manifold learning) 算法通过一个假设来克服这个障碍\n\n该假设认为 Rn 中大部分区域都是无效的输入，感兴趣的输入只分布在包含少量点的子集构成的一组流形中，而学习函数中感兴趣输出的变动只位于流形中的方向，或者感兴趣的变动只发生在我们从一个流形移动到另一个流形的时候 \n\n关键假设仍然是概率质量高度集中 \n\n一个利用流形假设的早期尝试是切面距离 (tangent distance) 算法 (Simard et al., 1993, 1998)。\n\n正切传播 (tangent prop) 算法 (Simard et al., 1992)训练 带有额外惩罚的神经网络分类器，使神经网络的每个输出 f(x) 对已知的变化因素是局部不变的。这些变化因素对应于沿着的相同样本聚集的流形的移动。\n\n局部不变性 是通过要求 ∇xf(x) 与已知流形的切向 v(i) 正交实现的，或者等价地通过正则化惩罚Ω使f 在x的v(i) 方向的导数是小的:\n","source":"_posts/技术分享：深度学习之正则化.md","raw":"---\ntitle: '深度学习之正则化技术分享'\ndate: 2017-12-05 09:43:17\ntags:\n  - deep learning\n  - regularization\ncategories:\n  - deep learning\nenglish_title: deep_learning_regularization_tec_share\ntoc: false\n---\n\n本文是团队内部本人的一个技术分享-深度学习最近的热门之一：深度学习中心问题之一正则化\n\n## 背景介绍\n\n机器学习的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。 在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。 这些策略被统称为正则化。 正则化的目的就是防止出现过拟合。事实上，开发更有效的正则化策略已成为本领域的主要研究工作之一。\n\n## 我的PPT分享\n\n{% pdf http://image.zhangxiaolong.org/file/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E4%BA%AB.pdf %}\n\n### ppt内容：\n目录\n1. 机器学习基本概念回顾\n\n2. 正则化（Regularization）\n\n3. 模型优化（Model  Optimization）\n\nWhat  is  Machine  Learning?\n\nMachine Learningis afiled of study that  computers  the  ability  to  learn  without  being  explicity  programmed   -‐  Arthur  Samuel  (1959)\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.     --   Tom Mitchell (1988)\n\nMachine  learning  is  the  study  of  computer  algorithms  that  improve  automatically  through  experience             --   Tom  Mitchell  (1998)\n\ntasks T、 experience E、 measure P\n\n-Task  T：分类（f  :  R^n->(1,…k)）、聚类、回归（f:R^n->R）、转录（表达、word2Vec）、机器翻译、结构化输出、异常检测（信用卡检测、阿里支付）、合成和采样（gan、音频）、缺失值填补、去噪（马赛克、条件概率分布）、密度估计或概率质量函数估计\n\n-Measure  P：评估学习算法的能力，设计其性能的度量（准确率、样本上概率对数的平均值），P是特定于系统执行的任务T的\n\n-Experience  E：\n\n-数据集、样本、特征\n\n-无监督算法：概率分布P(x)\n\n-监督算法：估计P(y|x)\n\n-\n\n构建机器学习算法\n\n-机器学习算法：特定的数据集、代价函数、优化过程和模型\n\n-Ex: 线性回归算法组成成分有X和y构成的数据集，损失函数是\n\n\n\n-模型是 ，在多数情况下优化算法可以定义为求解损失函数梯度为0的解\n\n-最常见的损失函数是负对数似然，最小化损失函数导致的最大似然估计\n\n-损失函数有可能有附加选，如正则化，\n\n-\n\n\n-非线性的模型，需要选择迭代数值优化过程（SGD.et）\n\n机器学习领域中心问题\n\n-\n\n1.Regularization（正则化）\n\n●\n\n2.Model  Optimization（模型优化）\n\n-为什么需要正则化？- 防止过拟合\n\n\n-什么是正则化？目的是什么？\n\n-对学习算法的一些修改策略----旨在减少泛化误差而不是训练误差\n\n-\n\n-正则化策略会对估计进行正则化\n\n-估计的正则化以偏差的增加换取方差的减少\n\n-\n\nBias(偏差)，Error(误差)，和Variance(方差)\n\nError = Bias + Variance + Noise\n\nbias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距\n\n要想在bias上表现好，low bias，就是复杂化模型（过拟合）\n\n要想在varience上表现好，low varience，就要简化模型，减少模型的参数（欠拟合）\n\nbias和variance的选择是一个tradeoff\n\nRegularization（正则化策略）\n\n1.参数范数惩罚(L1、L2)\n\n2.多任务学习\n\n3.提前终止\n\n4.参数绑定和参数共享\n\n5.稀疏表示\n\n●\n\n●\n\n参数范数惩罚（L1范数，L2范数）\n\n通过对目标函数J添加一个参数范数惩罚Ω(θ) \n\nL1正则化和L2正则化可以看做是损失函数的惩罚项\n\nL1正则化是指权值向量w中各个元素的绝对值之和，通常表示为〖∥w∥〗_1\n\nL2正则化是指权值向量w中各个元素的平方和然后再求平方根，通常表示为〖∥w∥〗_2\n\nL1和L2正则化的作用\n\nL1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择\n\nL2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合\n\nL1正则化和特征选择\n\n带L1正则化的损失函数\n\n在原始损失函数J0后添加L1正则化项时，相当于对J_0做了一个约束。\n\n令L=α∑16_w▒〖|w|〗，则J= J_0 +L，此时我们的任务变成在L约束下求出J_0取最小值的解\n\n二维的情况(只有两个权值w1和w2)，求解J_0的过程可以画出等值线，L1正则化的函数L在二维平面上画出\n\n当J_0等值线与L图形首次相交的地方就是最优解，顶点的值是(w^1,  w^2)=(0,w)\n\n正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大\n\n带L2正则化的损失函数\n\n二维平面下L2正则化的函数图形是个圆\n\nJ_0与L相交时使得w1或w2等于零的机率小许多\n\n为什么L2正则化不具有稀疏性的原因\n\nL2正则化和过拟合\n\n拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型\n\n线性回归中的梯度下降法为例，线性回归的代价函数如下\n\n在梯度下降法中，最终用于迭代计算参数θ的迭代式为\n\n每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的（权重衰减收缩的）\n\n多任务学习\n\n多任务学习是一种归纳迁移机制基本目标是提高泛化性能。\n\n多任务学习在学习一个问题的同时，可以通过使用共享表示来获得其他相关问题的知识\n\n多任务学习的基本假设是多个任务之间具有相关性，因此能够利用任务之间的相关性互相促进。\n\n多任务深度学习已经广泛应用于人脸识别、细粒度车辆分类、面部关键点定位与属性分类等多个领域\n\n提前终止\n\nEarly stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。\n\n具体算法（“No-improvement-in-n”）：\n\n在每一个Epoch结束时计算validation  data的accuracy，当accuracy不再提高时，就停止训练。\n\n怎样才认为validation  accurary不再提高？\n\n在训练的过程中，记录到目前为止最好的validation  accuracy\n\n当连续10次Epoch（或者更多次）没达到最佳accuracy时，可以认为accuracy不再提高。此时便可以停止迭代了\n\n参数绑定、参数共享\n\n全连接层缺点是参数很多，卷积层可以减少参数，减少计算量，因为卷积层的参数共享特性。\n\n卷积神经网络中的参数共享\n\nCNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。\n\nhttp://blog.csdn.net/zouxy09/article/details/8781543\n\nhttps://www.zhihu.com/question/47158818\n\n稀疏表示\n\n稀疏表示的本质：用尽可能少的资源表示尽可能多的知识，这种表示还能带来一个附加的好处，即计算速度快。\n\n稀疏表示就是指训练出的参数是稀疏的，即包含很多的0\n\n适应性（泛化能力）和稀疏性之间找平衡，最优取决于代价函数。\n\n稀疏表达的意义在于降维\n\n这种降维主要表现于虽然原始信号 的维度很高，但实际的有效信息集中在一个低维的空间里。这种性质使得不适定的问题变得适定(well-posed)，进而获得“好的解”成为可能\n\nReLU  - 神经网络的一种激活函数，可以做到稀疏表示\n\nL0、L1、L2\n\nBagging、集成方法\n\nBagging结合多个模型降低泛化误差的技术\n\n分别训练几个不同模型，所有模型表决测试样例输出，称为模型平均，采用这种策略的技术称为集成方法\n\n模型平均有效的原因\n\n不同模型通常不会在测试集上产生完全相同的误差\n\nBagging是一种允许重复多次使用同一种模型、训练算法和目标函数的方法\n\nBagging涉及构造 k 个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。\n\nBagging、集成方法\n\nDropout\n\nDropout是hintion最近2年提出的\n\n源于其文章Improving neural networks by preventing co-adaptation of feature detectors（通过阻止特征检测器的共同作用来提高神经网络的性能）\n\nL1、L2正则化是通过修改代价函数来实现的\n\nDropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）\n\nDropout\n\n一次迭代的过程\n\n在训练开始时，我们随机地“删除”一半的隐层单元，视它们为不存在\n\n保持输入输出层不变，按照BP算法更新上图神经网络中的权值\n\nDropout\n\nDropout优点\n\n计算方便。训练过程中使用Dropout产生 n 个随机二进制数与状态相乘即可。每个样本每次更新的时间复杂度： O(n)，空间复杂度： O(n)。\n\n适用广。Dropout不怎么限制适用的模型或训练过程，几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。包括：前馈神经网络、概率模型、受限波尔兹曼机、循环神经网络等。\n\n相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效。也可与其他形式的正则化合并，得到进一步提升。\n\nDropout缺点\n\n不适合宽度太窄的网络。否则大部分网络没有输入到输出的路径。\n\n不适合训练数据太小（如小于5000）的网络。训练数据太小时，Dropout没有其他方法表现好。\n\n不适合非常大的数据集。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处。\n\n对抗训练\n\n深度学习中对抗样本问题\n\n数据集中通过故意添加细微的干扰所形成的输入样本，受干扰之后的输入导致模型以高置信度给出一个错误的输出\n\n深度学习的对抗训练\n\n所谓深度学习对抗训练，就是通过在对抗样本上训练模型\n\nIan Goodfellow：对抗样本是一个非常难的问题 \n\n对抗训练\n\n切面距离、正切传播和  流行正切分类器\n\n流形 (manifold) 指连接在一起的区域 \n\n数学上，它是指一组点，且每个点都有 其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间 \n\n每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置 \n\n机器学习算法学习 Rn 上的所有感兴趣的函数，很多机器学 习问题看上去都是不可解的。\n\n流形学习 (manifold learning) 算法通过一个假设来克服这个障碍\n\n该假设认为 Rn 中大部分区域都是无效的输入，感兴趣的输入只分布在包含少量点的子集构成的一组流形中，而学习函数中感兴趣输出的变动只位于流形中的方向，或者感兴趣的变动只发生在我们从一个流形移动到另一个流形的时候 \n\n关键假设仍然是概率质量高度集中 \n\n一个利用流形假设的早期尝试是切面距离 (tangent distance) 算法 (Simard et al., 1993, 1998)。\n\n正切传播 (tangent prop) 算法 (Simard et al., 1992)训练 带有额外惩罚的神经网络分类器，使神经网络的每个输出 f(x) 对已知的变化因素是局部不变的。这些变化因素对应于沿着的相同样本聚集的流形的移动。\n\n局部不变性 是通过要求 ∇xf(x) 与已知流形的切向 v(i) 正交实现的，或者等价地通过正则化惩罚Ω使f 在x的v(i) 方向的导数是小的:\n","slug":"技术分享：深度学习之正则化","published":1,"updated":"2017-12-05T01:44:47.000Z","_id":"cjasynwp200016ls69klkmpnc","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文是团队内部本人的一个技术分享-深度学习最近的热门之一：深度学习中心问题之一正则化</p>\n<h2 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h2><p>机器学习的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。 在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。 这些策略被统称为正则化。 正则化的目的就是防止出现过拟合。事实上，开发更有效的正则化策略已成为本领域的主要研究工作之一。</p>\n<h2 id=\"我的PPT分享\"><a href=\"#我的PPT分享\" class=\"headerlink\" title=\"我的PPT分享\"></a>我的PPT分享</h2>\n\n\t<div class=\"row\">\n    <embed src=\"http://image.zhangxiaolong.org/file/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E4%BA%AB.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n<h3 id=\"ppt内容：\"><a href=\"#ppt内容：\" class=\"headerlink\" title=\"ppt内容：\"></a>ppt内容：</h3><p>目录</p>\n<ol>\n<li><p>机器学习基本概念回顾</p>\n</li>\n<li><p>正则化（Regularization）</p>\n</li>\n<li><p>模型优化（Model  Optimization）</p>\n</li>\n</ol>\n<p>What  is  Machine  Learning?</p>\n<p>Machine Learningis afiled of study that  computers  the  ability  to  learn  without  being  explicity  programmed   -‐  Arthur  Samuel  (1959)</p>\n<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.     –   Tom Mitchell (1988)</p>\n<p>Machine  learning  is  the  study  of  computer  algorithms  that  improve  automatically  through  experience             –   Tom  Mitchell  (1998)</p>\n<p>tasks T、 experience E、 measure P</p>\n<p>-Task  T：分类（f  :  R^n-&gt;(1,…k)）、聚类、回归（f:R^n-&gt;R）、转录（表达、word2Vec）、机器翻译、结构化输出、异常检测（信用卡检测、阿里支付）、合成和采样（gan、音频）、缺失值填补、去噪（马赛克、条件概率分布）、密度估计或概率质量函数估计</p>\n<p>-Measure  P：评估学习算法的能力，设计其性能的度量（准确率、样本上概率对数的平均值），P是特定于系统执行的任务T的</p>\n<p>-Experience  E：</p>\n<p>-数据集、样本、特征</p>\n<p>-无监督算法：概率分布P(x)</p>\n<p>-监督算法：估计P(y|x)</p>\n<p>-</p>\n<p>构建机器学习算法</p>\n<p>-机器学习算法：特定的数据集、代价函数、优化过程和模型</p>\n<p>-Ex: 线性回归算法组成成分有X和y构成的数据集，损失函数是</p>\n<p>-模型是 ，在多数情况下优化算法可以定义为求解损失函数梯度为0的解</p>\n<p>-最常见的损失函数是负对数似然，最小化损失函数导致的最大似然估计</p>\n<p>-损失函数有可能有附加选，如正则化，</p>\n<p>-</p>\n<p>-非线性的模型，需要选择迭代数值优化过程（SGD.et）</p>\n<p>机器学习领域中心问题</p>\n<p>-</p>\n<p>1.Regularization（正则化）</p>\n<p>●</p>\n<p>2.Model  Optimization（模型优化）</p>\n<p>-为什么需要正则化？- 防止过拟合</p>\n<p>-什么是正则化？目的是什么？</p>\n<p>-对学习算法的一些修改策略—-旨在减少泛化误差而不是训练误差</p>\n<p>-</p>\n<p>-正则化策略会对估计进行正则化</p>\n<p>-估计的正则化以偏差的增加换取方差的减少</p>\n<p>-</p>\n<p>Bias(偏差)，Error(误差)，和Variance(方差)</p>\n<p>Error = Bias + Variance + Noise</p>\n<p>bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距</p>\n<p>要想在bias上表现好，low bias，就是复杂化模型（过拟合）</p>\n<p>要想在varience上表现好，low varience，就要简化模型，减少模型的参数（欠拟合）</p>\n<p>bias和variance的选择是一个tradeoff</p>\n<p>Regularization（正则化策略）</p>\n<p>1.参数范数惩罚(L1、L2)</p>\n<p>2.多任务学习</p>\n<p>3.提前终止</p>\n<p>4.参数绑定和参数共享</p>\n<p>5.稀疏表示</p>\n<p>●</p>\n<p>●</p>\n<p>参数范数惩罚（L1范数，L2范数）</p>\n<p>通过对目标函数J添加一个参数范数惩罚Ω(θ) </p>\n<p>L1正则化和L2正则化可以看做是损失函数的惩罚项</p>\n<p>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为〖∥w∥〗_1</p>\n<p>L2正则化是指权值向量w中各个元素的平方和然后再求平方根，通常表示为〖∥w∥〗_2</p>\n<p>L1和L2正则化的作用</p>\n<p>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</p>\n<p>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</p>\n<p>L1正则化和特征选择</p>\n<p>带L1正则化的损失函数</p>\n<p>在原始损失函数J0后添加L1正则化项时，相当于对J_0做了一个约束。</p>\n<p>令L=α∑16_w▒〖|w|〗，则J= J_0 +L，此时我们的任务变成在L约束下求出J_0取最小值的解</p>\n<p>二维的情况(只有两个权值w1和w2)，求解J_0的过程可以画出等值线，L1正则化的函数L在二维平面上画出</p>\n<p>当J_0等值线与L图形首次相交的地方就是最优解，顶点的值是(w^1,  w^2)=(0,w)</p>\n<p>正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大</p>\n<p>带L2正则化的损失函数</p>\n<p>二维平面下L2正则化的函数图形是个圆</p>\n<p>J_0与L相交时使得w1或w2等于零的机率小许多</p>\n<p>为什么L2正则化不具有稀疏性的原因</p>\n<p>L2正则化和过拟合</p>\n<p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型</p>\n<p>线性回归中的梯度下降法为例，线性回归的代价函数如下</p>\n<p>在梯度下降法中，最终用于迭代计算参数θ的迭代式为</p>\n<p>每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的（权重衰减收缩的）</p>\n<p>多任务学习</p>\n<p>多任务学习是一种归纳迁移机制基本目标是提高泛化性能。</p>\n<p>多任务学习在学习一个问题的同时，可以通过使用共享表示来获得其他相关问题的知识</p>\n<p>多任务学习的基本假设是多个任务之间具有相关性，因此能够利用任务之间的相关性互相促进。</p>\n<p>多任务深度学习已经广泛应用于人脸识别、细粒度车辆分类、面部关键点定位与属性分类等多个领域</p>\n<p>提前终止</p>\n<p>Early stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p>\n<p>具体算法（“No-improvement-in-n”）：</p>\n<p>在每一个Epoch结束时计算validation  data的accuracy，当accuracy不再提高时，就停止训练。</p>\n<p>怎样才认为validation  accurary不再提高？</p>\n<p>在训练的过程中，记录到目前为止最好的validation  accuracy</p>\n<p>当连续10次Epoch（或者更多次）没达到最佳accuracy时，可以认为accuracy不再提高。此时便可以停止迭代了</p>\n<p>参数绑定、参数共享</p>\n<p>全连接层缺点是参数很多，卷积层可以减少参数，减少计算量，因为卷积层的参数共享特性。</p>\n<p>卷积神经网络中的参数共享</p>\n<p>CNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。</p>\n<p><a href=\"http://blog.csdn.net/zouxy09/article/details/8781543\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zouxy09/article/details/8781543</a></p>\n<p><a href=\"https://www.zhihu.com/question/47158818\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/47158818</a></p>\n<p>稀疏表示</p>\n<p>稀疏表示的本质：用尽可能少的资源表示尽可能多的知识，这种表示还能带来一个附加的好处，即计算速度快。</p>\n<p>稀疏表示就是指训练出的参数是稀疏的，即包含很多的0</p>\n<p>适应性（泛化能力）和稀疏性之间找平衡，最优取决于代价函数。</p>\n<p>稀疏表达的意义在于降维</p>\n<p>这种降维主要表现于虽然原始信号 的维度很高，但实际的有效信息集中在一个低维的空间里。这种性质使得不适定的问题变得适定(well-posed)，进而获得“好的解”成为可能</p>\n<p>ReLU  - 神经网络的一种激活函数，可以做到稀疏表示</p>\n<p>L0、L1、L2</p>\n<p>Bagging、集成方法</p>\n<p>Bagging结合多个模型降低泛化误差的技术</p>\n<p>分别训练几个不同模型，所有模型表决测试样例输出，称为模型平均，采用这种策略的技术称为集成方法</p>\n<p>模型平均有效的原因</p>\n<p>不同模型通常不会在测试集上产生完全相同的误差</p>\n<p>Bagging是一种允许重复多次使用同一种模型、训练算法和目标函数的方法</p>\n<p>Bagging涉及构造 k 个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。</p>\n<p>Bagging、集成方法</p>\n<p>Dropout</p>\n<p>Dropout是hintion最近2年提出的</p>\n<p>源于其文章Improving neural networks by preventing co-adaptation of feature detectors（通过阻止特征检测器的共同作用来提高神经网络的性能）</p>\n<p>L1、L2正则化是通过修改代价函数来实现的</p>\n<p>Dropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）</p>\n<p>Dropout</p>\n<p>一次迭代的过程</p>\n<p>在训练开始时，我们随机地“删除”一半的隐层单元，视它们为不存在</p>\n<p>保持输入输出层不变，按照BP算法更新上图神经网络中的权值</p>\n<p>Dropout</p>\n<p>Dropout优点</p>\n<p>计算方便。训练过程中使用Dropout产生 n 个随机二进制数与状态相乘即可。每个样本每次更新的时间复杂度： O(n)，空间复杂度： O(n)。</p>\n<p>适用广。Dropout不怎么限制适用的模型或训练过程，几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。包括：前馈神经网络、概率模型、受限波尔兹曼机、循环神经网络等。</p>\n<p>相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效。也可与其他形式的正则化合并，得到进一步提升。</p>\n<p>Dropout缺点</p>\n<p>不适合宽度太窄的网络。否则大部分网络没有输入到输出的路径。</p>\n<p>不适合训练数据太小（如小于5000）的网络。训练数据太小时，Dropout没有其他方法表现好。</p>\n<p>不适合非常大的数据集。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处。</p>\n<p>对抗训练</p>\n<p>深度学习中对抗样本问题</p>\n<p>数据集中通过故意添加细微的干扰所形成的输入样本，受干扰之后的输入导致模型以高置信度给出一个错误的输出</p>\n<p>深度学习的对抗训练</p>\n<p>所谓深度学习对抗训练，就是通过在对抗样本上训练模型</p>\n<p>Ian Goodfellow：对抗样本是一个非常难的问题 </p>\n<p>对抗训练</p>\n<p>切面距离、正切传播和  流行正切分类器</p>\n<p>流形 (manifold) 指连接在一起的区域 </p>\n<p>数学上，它是指一组点，且每个点都有 其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间 </p>\n<p>每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置 </p>\n<p>机器学习算法学习 Rn 上的所有感兴趣的函数，很多机器学 习问题看上去都是不可解的。</p>\n<p>流形学习 (manifold learning) 算法通过一个假设来克服这个障碍</p>\n<p>该假设认为 Rn 中大部分区域都是无效的输入，感兴趣的输入只分布在包含少量点的子集构成的一组流形中，而学习函数中感兴趣输出的变动只位于流形中的方向，或者感兴趣的变动只发生在我们从一个流形移动到另一个流形的时候 </p>\n<p>关键假设仍然是概率质量高度集中 </p>\n<p>一个利用流形假设的早期尝试是切面距离 (tangent distance) 算法 (Simard et al., 1993, 1998)。</p>\n<p>正切传播 (tangent prop) 算法 (Simard et al., 1992)训练 带有额外惩罚的神经网络分类器，使神经网络的每个输出 f(x) 对已知的变化因素是局部不变的。这些变化因素对应于沿着的相同样本聚集的流形的移动。</p>\n<p>局部不变性 是通过要求 ∇xf(x) 与已知流形的切向 v(i) 正交实现的，或者等价地通过正则化惩罚Ω使f 在x的v(i) 方向的导数是小的:</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文是团队内部本人的一个技术分享-深度学习最近的热门之一：深度学习中心问题之一正则化</p>\n<h2 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h2><p>机器学习的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。 在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。 这些策略被统称为正则化。 正则化的目的就是防止出现过拟合。事实上，开发更有效的正则化策略已成为本领域的主要研究工作之一。</p>\n<h2 id=\"我的PPT分享\"><a href=\"#我的PPT分享\" class=\"headerlink\" title=\"我的PPT分享\"></a>我的PPT分享</h2>\n\n\t<div class=\"row\">\n    <embed src=\"http://image.zhangxiaolong.org/file/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E4%BA%AB.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n<h3 id=\"ppt内容：\"><a href=\"#ppt内容：\" class=\"headerlink\" title=\"ppt内容：\"></a>ppt内容：</h3><p>目录</p>\n<ol>\n<li><p>机器学习基本概念回顾</p>\n</li>\n<li><p>正则化（Regularization）</p>\n</li>\n<li><p>模型优化（Model  Optimization）</p>\n</li>\n</ol>\n<p>What  is  Machine  Learning?</p>\n<p>Machine Learningis afiled of study that  computers  the  ability  to  learn  without  being  explicity  programmed   -‐  Arthur  Samuel  (1959)</p>\n<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.     –   Tom Mitchell (1988)</p>\n<p>Machine  learning  is  the  study  of  computer  algorithms  that  improve  automatically  through  experience             –   Tom  Mitchell  (1998)</p>\n<p>tasks T、 experience E、 measure P</p>\n<p>-Task  T：分类（f  :  R^n-&gt;(1,…k)）、聚类、回归（f:R^n-&gt;R）、转录（表达、word2Vec）、机器翻译、结构化输出、异常检测（信用卡检测、阿里支付）、合成和采样（gan、音频）、缺失值填补、去噪（马赛克、条件概率分布）、密度估计或概率质量函数估计</p>\n<p>-Measure  P：评估学习算法的能力，设计其性能的度量（准确率、样本上概率对数的平均值），P是特定于系统执行的任务T的</p>\n<p>-Experience  E：</p>\n<p>-数据集、样本、特征</p>\n<p>-无监督算法：概率分布P(x)</p>\n<p>-监督算法：估计P(y|x)</p>\n<p>-</p>\n<p>构建机器学习算法</p>\n<p>-机器学习算法：特定的数据集、代价函数、优化过程和模型</p>\n<p>-Ex: 线性回归算法组成成分有X和y构成的数据集，损失函数是</p>\n<p>-模型是 ，在多数情况下优化算法可以定义为求解损失函数梯度为0的解</p>\n<p>-最常见的损失函数是负对数似然，最小化损失函数导致的最大似然估计</p>\n<p>-损失函数有可能有附加选，如正则化，</p>\n<p>-</p>\n<p>-非线性的模型，需要选择迭代数值优化过程（SGD.et）</p>\n<p>机器学习领域中心问题</p>\n<p>-</p>\n<p>1.Regularization（正则化）</p>\n<p>●</p>\n<p>2.Model  Optimization（模型优化）</p>\n<p>-为什么需要正则化？- 防止过拟合</p>\n<p>-什么是正则化？目的是什么？</p>\n<p>-对学习算法的一些修改策略—-旨在减少泛化误差而不是训练误差</p>\n<p>-</p>\n<p>-正则化策略会对估计进行正则化</p>\n<p>-估计的正则化以偏差的增加换取方差的减少</p>\n<p>-</p>\n<p>Bias(偏差)，Error(误差)，和Variance(方差)</p>\n<p>Error = Bias + Variance + Noise</p>\n<p>bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距</p>\n<p>要想在bias上表现好，low bias，就是复杂化模型（过拟合）</p>\n<p>要想在varience上表现好，low varience，就要简化模型，减少模型的参数（欠拟合）</p>\n<p>bias和variance的选择是一个tradeoff</p>\n<p>Regularization（正则化策略）</p>\n<p>1.参数范数惩罚(L1、L2)</p>\n<p>2.多任务学习</p>\n<p>3.提前终止</p>\n<p>4.参数绑定和参数共享</p>\n<p>5.稀疏表示</p>\n<p>●</p>\n<p>●</p>\n<p>参数范数惩罚（L1范数，L2范数）</p>\n<p>通过对目标函数J添加一个参数范数惩罚Ω(θ) </p>\n<p>L1正则化和L2正则化可以看做是损失函数的惩罚项</p>\n<p>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为〖∥w∥〗_1</p>\n<p>L2正则化是指权值向量w中各个元素的平方和然后再求平方根，通常表示为〖∥w∥〗_2</p>\n<p>L1和L2正则化的作用</p>\n<p>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</p>\n<p>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</p>\n<p>L1正则化和特征选择</p>\n<p>带L1正则化的损失函数</p>\n<p>在原始损失函数J0后添加L1正则化项时，相当于对J_0做了一个约束。</p>\n<p>令L=α∑16_w▒〖|w|〗，则J= J_0 +L，此时我们的任务变成在L约束下求出J_0取最小值的解</p>\n<p>二维的情况(只有两个权值w1和w2)，求解J_0的过程可以画出等值线，L1正则化的函数L在二维平面上画出</p>\n<p>当J_0等值线与L图形首次相交的地方就是最优解，顶点的值是(w^1,  w^2)=(0,w)</p>\n<p>正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大</p>\n<p>带L2正则化的损失函数</p>\n<p>二维平面下L2正则化的函数图形是个圆</p>\n<p>J_0与L相交时使得w1或w2等于零的机率小许多</p>\n<p>为什么L2正则化不具有稀疏性的原因</p>\n<p>L2正则化和过拟合</p>\n<p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型</p>\n<p>线性回归中的梯度下降法为例，线性回归的代价函数如下</p>\n<p>在梯度下降法中，最终用于迭代计算参数θ的迭代式为</p>\n<p>每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的（权重衰减收缩的）</p>\n<p>多任务学习</p>\n<p>多任务学习是一种归纳迁移机制基本目标是提高泛化性能。</p>\n<p>多任务学习在学习一个问题的同时，可以通过使用共享表示来获得其他相关问题的知识</p>\n<p>多任务学习的基本假设是多个任务之间具有相关性，因此能够利用任务之间的相关性互相促进。</p>\n<p>多任务深度学习已经广泛应用于人脸识别、细粒度车辆分类、面部关键点定位与属性分类等多个领域</p>\n<p>提前终止</p>\n<p>Early stopping是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p>\n<p>具体算法（“No-improvement-in-n”）：</p>\n<p>在每一个Epoch结束时计算validation  data的accuracy，当accuracy不再提高时，就停止训练。</p>\n<p>怎样才认为validation  accurary不再提高？</p>\n<p>在训练的过程中，记录到目前为止最好的validation  accuracy</p>\n<p>当连续10次Epoch（或者更多次）没达到最佳accuracy时，可以认为accuracy不再提高。此时便可以停止迭代了</p>\n<p>参数绑定、参数共享</p>\n<p>全连接层缺点是参数很多，卷积层可以减少参数，减少计算量，因为卷积层的参数共享特性。</p>\n<p>卷积神经网络中的参数共享</p>\n<p>CNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。</p>\n<p><a href=\"http://blog.csdn.net/zouxy09/article/details/8781543\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zouxy09/article/details/8781543</a></p>\n<p><a href=\"https://www.zhihu.com/question/47158818\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/47158818</a></p>\n<p>稀疏表示</p>\n<p>稀疏表示的本质：用尽可能少的资源表示尽可能多的知识，这种表示还能带来一个附加的好处，即计算速度快。</p>\n<p>稀疏表示就是指训练出的参数是稀疏的，即包含很多的0</p>\n<p>适应性（泛化能力）和稀疏性之间找平衡，最优取决于代价函数。</p>\n<p>稀疏表达的意义在于降维</p>\n<p>这种降维主要表现于虽然原始信号 的维度很高，但实际的有效信息集中在一个低维的空间里。这种性质使得不适定的问题变得适定(well-posed)，进而获得“好的解”成为可能</p>\n<p>ReLU  - 神经网络的一种激活函数，可以做到稀疏表示</p>\n<p>L0、L1、L2</p>\n<p>Bagging、集成方法</p>\n<p>Bagging结合多个模型降低泛化误差的技术</p>\n<p>分别训练几个不同模型，所有模型表决测试样例输出，称为模型平均，采用这种策略的技术称为集成方法</p>\n<p>模型平均有效的原因</p>\n<p>不同模型通常不会在测试集上产生完全相同的误差</p>\n<p>Bagging是一种允许重复多次使用同一种模型、训练算法和目标函数的方法</p>\n<p>Bagging涉及构造 k 个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。</p>\n<p>Bagging、集成方法</p>\n<p>Dropout</p>\n<p>Dropout是hintion最近2年提出的</p>\n<p>源于其文章Improving neural networks by preventing co-adaptation of feature detectors（通过阻止特征检测器的共同作用来提高神经网络的性能）</p>\n<p>L1、L2正则化是通过修改代价函数来实现的</p>\n<p>Dropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）</p>\n<p>Dropout</p>\n<p>一次迭代的过程</p>\n<p>在训练开始时，我们随机地“删除”一半的隐层单元，视它们为不存在</p>\n<p>保持输入输出层不变，按照BP算法更新上图神经网络中的权值</p>\n<p>Dropout</p>\n<p>Dropout优点</p>\n<p>计算方便。训练过程中使用Dropout产生 n 个随机二进制数与状态相乘即可。每个样本每次更新的时间复杂度： O(n)，空间复杂度： O(n)。</p>\n<p>适用广。Dropout不怎么限制适用的模型或训练过程，几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。包括：前馈神经网络、概率模型、受限波尔兹曼机、循环神经网络等。</p>\n<p>相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效。也可与其他形式的正则化合并，得到进一步提升。</p>\n<p>Dropout缺点</p>\n<p>不适合宽度太窄的网络。否则大部分网络没有输入到输出的路径。</p>\n<p>不适合训练数据太小（如小于5000）的网络。训练数据太小时，Dropout没有其他方法表现好。</p>\n<p>不适合非常大的数据集。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处。</p>\n<p>对抗训练</p>\n<p>深度学习中对抗样本问题</p>\n<p>数据集中通过故意添加细微的干扰所形成的输入样本，受干扰之后的输入导致模型以高置信度给出一个错误的输出</p>\n<p>深度学习的对抗训练</p>\n<p>所谓深度学习对抗训练，就是通过在对抗样本上训练模型</p>\n<p>Ian Goodfellow：对抗样本是一个非常难的问题 </p>\n<p>对抗训练</p>\n<p>切面距离、正切传播和  流行正切分类器</p>\n<p>流形 (manifold) 指连接在一起的区域 </p>\n<p>数学上，它是指一组点，且每个点都有 其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间 </p>\n<p>每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置 </p>\n<p>机器学习算法学习 Rn 上的所有感兴趣的函数，很多机器学 习问题看上去都是不可解的。</p>\n<p>流形学习 (manifold learning) 算法通过一个假设来克服这个障碍</p>\n<p>该假设认为 Rn 中大部分区域都是无效的输入，感兴趣的输入只分布在包含少量点的子集构成的一组流形中，而学习函数中感兴趣输出的变动只位于流形中的方向，或者感兴趣的变动只发生在我们从一个流形移动到另一个流形的时候 </p>\n<p>关键假设仍然是概率质量高度集中 </p>\n<p>一个利用流形假设的早期尝试是切面距离 (tangent distance) 算法 (Simard et al., 1993, 1998)。</p>\n<p>正切传播 (tangent prop) 算法 (Simard et al., 1992)训练 带有额外惩罚的神经网络分类器，使神经网络的每个输出 f(x) 对已知的变化因素是局部不变的。这些变化因素对应于沿着的相同样本聚集的流形的移动。</p>\n<p>局部不变性 是通过要求 ∇xf(x) 与已知流形的切向 v(i) 正交实现的，或者等价地通过正则化惩罚Ω使f 在x的v(i) 方向的导数是小的:</p>\n"},{"title":"深度学习之GAN技术分享","date":"2017-10-13T02:15:17.000Z","english_title":"deep_learning_gan_tec_share","toc":false,"_content":"\n本文是团队内部本人的一个技术分享-深度学习最近的热门之一：生成式对抗网络GAN\n\n## 背景介绍\nGAN是“生成对抗网络”（Generative Adversarial Networks）的简称，由2014年还在蒙特利尔读博士的Ian Goodfellow引入深度学习领域。2016年，GAN热潮席卷AI领域顶级会议，从ICLR到NIPS，大量高质量论文被发表和探讨。Yann LeCun曾评价GAN是“20年来机器学习领域最酷的想法”。\n\n## 我的PPT分享\n\n{% pdf http://image.zhangxiaolong.org/file/GAN%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB.pdf %}\n\n参考文献：\n（0）[Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014.]()\n（1）[生成对抗网络nips2016课程](https://zhuanlan.zhihu.com/uai-rocks)\n（2）[伯克利深度学习专题课程：对抗生成网络创始人首次剖析训练实例](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651988584&idx=3&sn=e42bbbe2c447de44f465e568d467cebe&chksm=f1215699c656df8f46bbb8275ccb86dc55d63c018acba3f0f14ed55d0b9a9916bc1325cca7e9&mpshare=1&scene=1&srcid=1025xB3CzclBqrVMBZMxSgs1#rd)\n（3）[生成式对抗网络GAN研究进展](http://blog.csdn.net/Solomon1558/article/details/52537114)\n（4）[简述生成式对抗网络](https://chenrudan.github.io/blog/2016/11/12/gan.html)\n","source":"_posts/machine_learning-GAN_tec_share.md","raw":"---\ntitle: '深度学习之GAN技术分享'\ndate: 2017-10-13 10:15:17\ntags:\n  - deep learning\n  - GAN\ncategories:\n  - deep learning\nenglish_title: deep_learning_gan_tec_share\ntoc: false\n---\n\n本文是团队内部本人的一个技术分享-深度学习最近的热门之一：生成式对抗网络GAN\n\n## 背景介绍\nGAN是“生成对抗网络”（Generative Adversarial Networks）的简称，由2014年还在蒙特利尔读博士的Ian Goodfellow引入深度学习领域。2016年，GAN热潮席卷AI领域顶级会议，从ICLR到NIPS，大量高质量论文被发表和探讨。Yann LeCun曾评价GAN是“20年来机器学习领域最酷的想法”。\n\n## 我的PPT分享\n\n{% pdf http://image.zhangxiaolong.org/file/GAN%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB.pdf %}\n\n参考文献：\n（0）[Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014.]()\n（1）[生成对抗网络nips2016课程](https://zhuanlan.zhihu.com/uai-rocks)\n（2）[伯克利深度学习专题课程：对抗生成网络创始人首次剖析训练实例](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2651988584&idx=3&sn=e42bbbe2c447de44f465e568d467cebe&chksm=f1215699c656df8f46bbb8275ccb86dc55d63c018acba3f0f14ed55d0b9a9916bc1325cca7e9&mpshare=1&scene=1&srcid=1025xB3CzclBqrVMBZMxSgs1#rd)\n（3）[生成式对抗网络GAN研究进展](http://blog.csdn.net/Solomon1558/article/details/52537114)\n（4）[简述生成式对抗网络](https://chenrudan.github.io/blog/2016/11/12/gan.html)\n","slug":"machine_learning-GAN_tec_share","published":1,"updated":"2017-10-13T08:19:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjasynwp600036ls6xxm7ii55","content":"<p>本文是团队内部本人的一个技术分享-深度学习最近的热门之一：生成式对抗网络GAN</p>\n<h2 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h2><p>GAN是“生成对抗网络”（Generative Adversarial Networks）的简称，由2014年还在蒙特利尔读博士的Ian Goodfellow引入深度学习领域。2016年，GAN热潮席卷AI领域顶级会议，从ICLR到NIPS，大量高质量论文被发表和探讨。Yann LeCun曾评价GAN是“20年来机器学习领域最酷的想法”。</p>\n<h2 id=\"我的PPT分享\"><a href=\"#我的PPT分享\" class=\"headerlink\" title=\"我的PPT分享\"></a>我的PPT分享</h2>\n\n\t<div class=\"row\">\n    <embed src=\"http://image.zhangxiaolong.org/file/GAN%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n<p>参考文献：<br>（0）<a href=\"\">Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in neural information processing systems. 2014.</a><br>（1）<a href=\"https://zhuanlan.zhihu.com/uai-rocks\" target=\"_blank\" rel=\"external\">生成对抗网络nips2016课程</a><br>（2）<a href=\"https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2651988584&amp;idx=3&amp;sn=e42bbbe2c447de44f465e568d467cebe&amp;chksm=f1215699c656df8f46bbb8275ccb86dc55d63c018acba3f0f14ed55d0b9a9916bc1325cca7e9&amp;mpshare=1&amp;scene=1&amp;srcid=1025xB3CzclBqrVMBZMxSgs1#rd\" target=\"_blank\" rel=\"external\">伯克利深度学习专题课程：对抗生成网络创始人首次剖析训练实例</a><br>（3）<a href=\"http://blog.csdn.net/Solomon1558/article/details/52537114\" target=\"_blank\" rel=\"external\">生成式对抗网络GAN研究进展</a><br>（4）<a href=\"https://chenrudan.github.io/blog/2016/11/12/gan.html\" target=\"_blank\" rel=\"external\">简述生成式对抗网络</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文是团队内部本人的一个技术分享-深度学习最近的热门之一：生成式对抗网络GAN</p>\n<h2 id=\"背景介绍\"><a href=\"#背景介绍\" class=\"headerlink\" title=\"背景介绍\"></a>背景介绍</h2><p>GAN是“生成对抗网络”（Generative Adversarial Networks）的简称，由2014年还在蒙特利尔读博士的Ian Goodfellow引入深度学习领域。2016年，GAN热潮席卷AI领域顶级会议，从ICLR到NIPS，大量高质量论文被发表和探讨。Yann LeCun曾评价GAN是“20年来机器学习领域最酷的想法”。</p>\n<h2 id=\"我的PPT分享\"><a href=\"#我的PPT分享\" class=\"headerlink\" title=\"我的PPT分享\"></a>我的PPT分享</h2>\n\n\t<div class=\"row\">\n    <embed src=\"http://image.zhangxiaolong.org/file/GAN%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB.pdf\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n\n<p>参考文献：<br>（0）<a href=\"\">Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in neural information processing systems. 2014.</a><br>（1）<a href=\"https://zhuanlan.zhihu.com/uai-rocks\" target=\"_blank\" rel=\"external\">生成对抗网络nips2016课程</a><br>（2）<a href=\"https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2651988584&amp;idx=3&amp;sn=e42bbbe2c447de44f465e568d467cebe&amp;chksm=f1215699c656df8f46bbb8275ccb86dc55d63c018acba3f0f14ed55d0b9a9916bc1325cca7e9&amp;mpshare=1&amp;scene=1&amp;srcid=1025xB3CzclBqrVMBZMxSgs1#rd\" target=\"_blank\" rel=\"external\">伯克利深度学习专题课程：对抗生成网络创始人首次剖析训练实例</a><br>（3）<a href=\"http://blog.csdn.net/Solomon1558/article/details/52537114\" target=\"_blank\" rel=\"external\">生成式对抗网络GAN研究进展</a><br>（4）<a href=\"https://chenrudan.github.io/blog/2016/11/12/gan.html\" target=\"_blank\" rel=\"external\">简述生成式对抗网络</a></p>\n"},{"title":"Wide & Deep learning for rec sys 阅读分析","date":"2017-10-13T09:15:17.000Z","english_title":"wide_deep_learning_for_rec_sys","toc":true,"_content":"\n这篇《Wide & Deep Learning for Recommender Systems》论文是在2016年就已经发表出来了，通过检索相关，发现已经有一些业务在使用这个框架或者改良的版本。最近在我们的产品首页做一个课程推荐，找到了这篇论文，所以就顺手学习参考做个记录，文章分为以下两个部分：\n\n(1) 论文阅读分析\n(2) 相关知识的引入\n\n## 1. 论文阅读（混合翻译和理解）\n\n### （1）背景介绍\n推荐系统可以被看成一个搜索排序系统，我们之前很多的推荐就是利用搜索去做，设计一个scorer，然后rank所有候选的docs，取topK返回给用户，点击转换率也不错（需要基于对业务和场景的深入理解）。同搜索系统一样，同样面对两个问题：memorization（记忆）和 generalization(泛化) 。memorization是基于历史数据，学习到频繁同现的item或者feature，挖掘一些相关性的东西；generalization是基于上一步学习到的特性进行迁移和挖掘一些新feature（很少出现或者没有出现过的特性）。\n\n基于memorization的推荐系统还是比较典型的，因为是基于之前准确学习到的特征进行推荐的，例如常见的CF算法，Item-CF中发现物品于物品之间的联系给用户推荐，或者基于内容（或tag）的推荐也是类似的，而generalization策略的使用，则可以提升推荐系统的迁移性（多样性），比如使用神经网络等等。\n\n开始介绍之前，我们先预习下内容:\n\n- 1> 先分析了下目前使用广泛的LR线性模型，由于模型比较简单、可扩展、可解释性好，所以应用比较多。LR模型通常在使用one-hot编码的二值化稀疏特征上进行训练，比如论文中关于app下载的特征，user_installed_app=netflix时，设置值为1，通过使用向量积（叉乘）转换稀疏特征之后，memorization可以设置当出现 AND(user_installed_app=netflix, impres- sion_app=pandora”)时，也设置值为1，不过，向量积（叉乘）转换不能泛化出之前没有出现过的query-item特征对。而generalization可以通过更粗粒度的特征去泛化一些新特征。\n- 2> Embedding-based模型，例如FM算法、深度神经网络可以通过学习一个dense embedding vector（低纬稠密嵌入向量）去泛化出之前没有出现过的query-item特征，使用了更少的特征工程，但是，学习到一个低纬去代表一个有稀疏特征和high-rank的query和item是非常困难的，比如特殊爱好的用户或者相关的item数比较少，这些情况下当使用dense embedding去计算所有的query-items的时候，会出现很多非零的预测，就会导致推荐一些不相关的item。但是，使用向量积转换的线性模型（更少参数）是可以记住这些特殊规则的特征。\n\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/widemodel.png!origin)\n\n这篇论文（上图model）就是从memorization、generalization做设计和拓展，做了三个方面的贡献，\n\n- wide和deep模型框架联合训练带embedding的前馈神经网络，以及对于稀疏输入的常用推荐系统所使用的带特征转化的线性模型\n- 这个推荐系统已经应用于google play\n- 这套推荐系统已经在tensorflow上实现了高等级的API，详见[google tensorflow](https://www.tensorflow.org/tutorials/wide_and_deep)；\n\n\n### （2）推荐系统介绍\n如下图大概看一下推荐系统的结构，用户输入query（用户操作、上下文特征），推荐系统返回一个根据点击或者购买等指标优化之后的结果列表，结果列表中每一条数据的出现就是曝光，用户操作（点击等），item展现log输入模型学习器实时学习，更新现有的打分模型。\n\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/resys.png)\n\n实际上，这个图上可以简单的看出这个推荐系统包含两个部分，retrieval和ranking系统。检索系统可以用机器学习或者人工规则去获取用户的相关的候选集，我们项目之前曾采用的是搜索系统（基于lucene的搜索系统）召回一个千级别数量级的候选集，然后在排序推荐的。ranking系统打分排序经常使用P(y|x)，即当用户有x特征的时候y标签出现的概率，x特征包括用户的人口特征、国家、语言、上下文特征（设备、时间特征）、展现特征（app上架时间、历史统计）。这个论文主要使用Wide & Deep learning框架实现ranking模型。\n\n### （3）Wide 和Deep learning模型\n**1> Wide模型**\n\nwide模型通常是一个 y = wT x + b的线性模型，是一个高维特征+特征组合的LR，在图1中左边部分，y是预测值，x代表特征向量，w是参数，b是bais。\n\n一个重要的向量积转换定义：\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172730.png)\n，其中cki 为一个boolean变量，如果第i个特征是第k个变换ϕk的一部分，那么为1; 否则为0。\n\n**2> Deep learning 模型**\n\nDeep 模型是一个前馈神经网络，在图1的右边展示。每一个稀疏、高维的特征首先转为低纬、稠密有值的embedding vector，向量随机初始化，然后训练迭代最小化损失函数，低纬的向量向前传播到神经网络的隐藏层，隐藏层进行如下计算a(l+1) = f(W(l)a(l) + b(l)) ，特征首l表示层数，f表示激活函数，通常采用ReLUs， a(l) , b(l) , and W (l) 是activations、bais，和模型权重。\n\n**3> Wide & Deep模型联合训练**\n\n论文中模型的训练采用联合训练，即两个模型通过logistic loss function 连接一起进行训练迭代。Wide & Deep模型的联合训练是通过梯度的BP（反向传播）算法、nini-batch SGD优化完成的。论文的实验中，使用的是Follow- the-regularized-leader (FTRL)算法，使用L1正则作为wide部分的优化器，Deep部分使用AdaGrad优化。\n联合模型在图1中间展示，LR模型的公式为：\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172910.png)\n\n### （4） 推荐系统实现\n推荐系统实现包括3个部分：数据生成，模型训练，模型serving，如下图所示，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/pipeline.png)\n\n**数据生成**：用户action数据和app曝光数据作为训练数据；\n**模型训练**：如图4中所示方法，在训练期间，输入层接受训练数据和词汇表的输入，一起为一个label生成sparse和dense特征。wide组件包含了用户安装app和曝光app的cross-product tansformation。对于模型的deep组件，会为每个类别型特征学到一个32维的embedding向量。我们将所有embeddings联接起来形成dense features，产生一个接近1200维的dense vector。联接向量接着输入到3个ReLU层，以及最终的logistic输出单元。\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/app%20rec.png!origin)\n**模型serving**：论文中说到，作者首先实现了一个warm-starting系统，会使用前一个模型的embeddings和线性模型权重来初始化一个新的模型，在验证之后在线上做AB测试。\n\n### （5）实验效果\n可以看到采用这个模型之后，离线AUC提升不多，但是线上效果提升了3.9%，还是可以的\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172547.png)\n\n\n## 2. 相关知识引入\n\n### [1] nonlinear feature 和 feature transformations \n我们一般讲到的分类模型（线性分类、LR等等）基本都是线性的（假设数据空间线性可分），但是实际上有时候不是简单的一条线分分开，比如圆形分割线，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173057.png)\n\n更多内容可以参考（相关链接：http://beader.me/2014/08/30/nonlinear-transformation/），我们一般说的非线性变换，指的是多项式变换(Polynomial Transform)。\n\n用符号ϕQ来表示Q次多项式变换:\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173333.png)\n\n### [2] feed-forward neural network\n前馈网络中各个神经元按接受信息的先后分为不同的组。每一组可以看 作一个神经层。每一层中的神经元接受前一层神经元的输出，并输出到下一层 神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播。前馈网 络可以用一个有向无环路图表示。前馈网络可以看作一个函数，通过简单非线 性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单， 易于实现。前馈网络包括全连接前馈网络和卷积神经网络等。\n\n前馈神经网络也经常称为多层感知器（Multilayer Perceptron，MLP）。但 多层感知器的叫法并不是十分合理，因为前馈神经网络其实是由多层的logistic 回归模型（连续的非线性函数）组成，而不是由多层的感知器（不连续的非线 性函数）组成，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173518.png)\n更多内容参考（https://nndl.github.io/ch5.pdf）\n\n### [3] embeded vector\nEmbedding在数学上表示一个maping, f: X -> Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。\n\n通俗来讲，embeded vector是将词向量化的概念，将一个词映射到一个多维空间向量，比如 苹果 = （0.1，0.6，-0.5...），说到词嵌入，不得不说word2vec，它是谷歌提出一种word embedding 的工具或者算法集合，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。\n\n更多相关（[词嵌入原理](https://d0a31e64-a-62cb3a1a-s-sites.googlegroups.com/site/shaohua03/intro-word-embedding.pdf?attachauth=ANoY7crX2AorNaDwF5V6k05ZL8YaBdb21hwJttgKlbZushozFyGpgN2fKm7thjg8SwtUjy-qsZoGrEP8epdQ7vI4HUMD6gJhfRdaomV-I3XqrJyi08hiYJZk3pAd63vydiftrgIBfGh1jZorpzCmEzou6cZRo1i0j3GaZXLeaJkIIXihfvXXyWCA1bmT9mpB4QcWxDcMdWDIS0mFZx-jw4ZTT4tHK50QVfp08kJuag7dddk4lQjrGHI%3D&attredirects=0)）\n\n### [4] ReLU（Rectified Linear Units）激活函数\n神经网络中经常使用的激活函数有：Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid），2001年根据生物学角度提出了新的激活函数，即ReLU，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173736.png!origin)\n图出自：https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf\n\n这个模型对比Sigmoid系主要变化有三点：①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性，这三个变化更加准确，\n\nsigmoid系的导数根据图中可以看到，斜率变化速度比较小，梯度更新可能丢失。\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173747.png!origin)\n\nReLU相比sigmoid系的激活函数，有以下的优点：\n\n- 梯度不饱和。梯度计算公式为：x>0则为1,因此在反向传播过程中，减轻了梯度弥散的问题，神经网络前几层的参数也可以很快的更新。\n- 计算速度快。正向传播过程中，sigmoid和tanh函数计算激活值时需要计算指数，而Relu函数仅需要设置阈值。如果x<0,f(x)=0，如果x>0,f(x)=x，加快了正向传播的计算速度。\n\n更多参考（http://www.cnblogs.com/neopenx/p/4453161.html）\n\n### [5] Follow-the-regularized-leader(FTRL)\n由Google的H. Brendan McMahan在2010年提出的[2]，作者后来在2011年发表了一篇关于FTRL和AOGD、FOBOS、RDA比较的论文[3]，2013年又和Gary Holt, D. Sculley, Michael Young等人发表了一篇关于FTRL工程化实现的论文[4]。如论文[4]的内容所述，FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，\n\n下面这两篇文章不错，移步读一下：http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm，http://www.cnblogs.com/EE-NovRain/p/3810737.html\n\n### [6] L1正则化\n机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制，防止某一个参数变化太大影响其他参数的拟合。\n\n- L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1\n- L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2\n- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择\n- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合\n\n更多参考（http://blog.csdn.net/zouxy09/article/details/24971995）\n\n### [7] AdaGrad自适应学习率调整\n这个算法是非常nb的，神经网络中有经典五大超参数:学习率(Leraning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)单层神经元数(Units)、正则惩罚项（Regularizer|Normalization），调整这些参数是非常困难的，Matthew D. Zeiler在2012年提出这个方法，AdaGrad思路基本是借鉴L2 Regularizer，不过此时调节的不是W，是Gradient，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png)\n，AdaGrad过程，是一个递推过程，每次从τ=1，推到τ=t，把沿路的Gradient的平方根，作为Regularizer。分母作为Regularizer项的工作机制如下，\n\n- 训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]\n- 训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]，\n    由于Regularizer是专门针对Gradient的，所以有利于解决Gradient Vanish/Expoloding问题。当然也有缺点，\n\n- 初始化W影响初始化梯度，初始化W过大，会导致初始梯度被惩罚得很小。\n此时可以人工加大η的值，但过大的η会使得Regularizer过于敏感，调节幅度很大。\n- 训练到中后期，递推路径上累加的梯度平方和越打越多，迅速使得Gradinet被惩罚逼近0，提前结束训练。\n\n更多（http://www.cnblogs.com/neopenx/p/4768388.html、https://zhuanlan.zhihu.com/p/22252270）\n\n论文：http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n\n### [8] factorization machine(FM)隐因子分解机\nFM旨在解决是稀疏矩阵下的特征组合问题，上面论文中用户点击采用one-hot编码之后，生成的举证是稀疏的，所以有采用这个方法，举例来说，线性模型有如下式子，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174148.png)\n红箭头所指的两个互异特征分量相互作用也加入进去了，用factor来描述特征分量之间的关系，与LR中的交叉组合特征类似。\n由于特征矩阵是稀疏的，所以将方程式改写为：\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png)\n上下两个公式比较，得到W=VT,通过学习隐含主题向量间接来学习W矩阵，减少了学习参数的个数，同时由主成分分析PCA推出VVT能表达W，针对数据不足或者矩阵稀疏时参数过多的问题。\n\n更多参考（https://my.oschina.net/keyven/blog/648747）\n\n论文（http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf）\n\n## 参考文档\n（1）[论文：Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf)\n\n(完)\n\n","source":"_posts/wide_deep_learning_for_rec_sys.md","raw":"---\ntitle: 'Wide & Deep learning for rec sys 阅读分析'\ndate: 2017-10-13 17:15:17\ntags:\n  - deep learning\ncategories:\n  - resys\nenglish_title: wide_deep_learning_for_rec_sys\ntoc: true\n---\n\n这篇《Wide & Deep Learning for Recommender Systems》论文是在2016年就已经发表出来了，通过检索相关，发现已经有一些业务在使用这个框架或者改良的版本。最近在我们的产品首页做一个课程推荐，找到了这篇论文，所以就顺手学习参考做个记录，文章分为以下两个部分：\n\n(1) 论文阅读分析\n(2) 相关知识的引入\n\n## 1. 论文阅读（混合翻译和理解）\n\n### （1）背景介绍\n推荐系统可以被看成一个搜索排序系统，我们之前很多的推荐就是利用搜索去做，设计一个scorer，然后rank所有候选的docs，取topK返回给用户，点击转换率也不错（需要基于对业务和场景的深入理解）。同搜索系统一样，同样面对两个问题：memorization（记忆）和 generalization(泛化) 。memorization是基于历史数据，学习到频繁同现的item或者feature，挖掘一些相关性的东西；generalization是基于上一步学习到的特性进行迁移和挖掘一些新feature（很少出现或者没有出现过的特性）。\n\n基于memorization的推荐系统还是比较典型的，因为是基于之前准确学习到的特征进行推荐的，例如常见的CF算法，Item-CF中发现物品于物品之间的联系给用户推荐，或者基于内容（或tag）的推荐也是类似的，而generalization策略的使用，则可以提升推荐系统的迁移性（多样性），比如使用神经网络等等。\n\n开始介绍之前，我们先预习下内容:\n\n- 1> 先分析了下目前使用广泛的LR线性模型，由于模型比较简单、可扩展、可解释性好，所以应用比较多。LR模型通常在使用one-hot编码的二值化稀疏特征上进行训练，比如论文中关于app下载的特征，user_installed_app=netflix时，设置值为1，通过使用向量积（叉乘）转换稀疏特征之后，memorization可以设置当出现 AND(user_installed_app=netflix, impres- sion_app=pandora”)时，也设置值为1，不过，向量积（叉乘）转换不能泛化出之前没有出现过的query-item特征对。而generalization可以通过更粗粒度的特征去泛化一些新特征。\n- 2> Embedding-based模型，例如FM算法、深度神经网络可以通过学习一个dense embedding vector（低纬稠密嵌入向量）去泛化出之前没有出现过的query-item特征，使用了更少的特征工程，但是，学习到一个低纬去代表一个有稀疏特征和high-rank的query和item是非常困难的，比如特殊爱好的用户或者相关的item数比较少，这些情况下当使用dense embedding去计算所有的query-items的时候，会出现很多非零的预测，就会导致推荐一些不相关的item。但是，使用向量积转换的线性模型（更少参数）是可以记住这些特殊规则的特征。\n\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/widemodel.png!origin)\n\n这篇论文（上图model）就是从memorization、generalization做设计和拓展，做了三个方面的贡献，\n\n- wide和deep模型框架联合训练带embedding的前馈神经网络，以及对于稀疏输入的常用推荐系统所使用的带特征转化的线性模型\n- 这个推荐系统已经应用于google play\n- 这套推荐系统已经在tensorflow上实现了高等级的API，详见[google tensorflow](https://www.tensorflow.org/tutorials/wide_and_deep)；\n\n\n### （2）推荐系统介绍\n如下图大概看一下推荐系统的结构，用户输入query（用户操作、上下文特征），推荐系统返回一个根据点击或者购买等指标优化之后的结果列表，结果列表中每一条数据的出现就是曝光，用户操作（点击等），item展现log输入模型学习器实时学习，更新现有的打分模型。\n\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/resys.png)\n\n实际上，这个图上可以简单的看出这个推荐系统包含两个部分，retrieval和ranking系统。检索系统可以用机器学习或者人工规则去获取用户的相关的候选集，我们项目之前曾采用的是搜索系统（基于lucene的搜索系统）召回一个千级别数量级的候选集，然后在排序推荐的。ranking系统打分排序经常使用P(y|x)，即当用户有x特征的时候y标签出现的概率，x特征包括用户的人口特征、国家、语言、上下文特征（设备、时间特征）、展现特征（app上架时间、历史统计）。这个论文主要使用Wide & Deep learning框架实现ranking模型。\n\n### （3）Wide 和Deep learning模型\n**1> Wide模型**\n\nwide模型通常是一个 y = wT x + b的线性模型，是一个高维特征+特征组合的LR，在图1中左边部分，y是预测值，x代表特征向量，w是参数，b是bais。\n\n一个重要的向量积转换定义：\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172730.png)\n，其中cki 为一个boolean变量，如果第i个特征是第k个变换ϕk的一部分，那么为1; 否则为0。\n\n**2> Deep learning 模型**\n\nDeep 模型是一个前馈神经网络，在图1的右边展示。每一个稀疏、高维的特征首先转为低纬、稠密有值的embedding vector，向量随机初始化，然后训练迭代最小化损失函数，低纬的向量向前传播到神经网络的隐藏层，隐藏层进行如下计算a(l+1) = f(W(l)a(l) + b(l)) ，特征首l表示层数，f表示激活函数，通常采用ReLUs， a(l) , b(l) , and W (l) 是activations、bais，和模型权重。\n\n**3> Wide & Deep模型联合训练**\n\n论文中模型的训练采用联合训练，即两个模型通过logistic loss function 连接一起进行训练迭代。Wide & Deep模型的联合训练是通过梯度的BP（反向传播）算法、nini-batch SGD优化完成的。论文的实验中，使用的是Follow- the-regularized-leader (FTRL)算法，使用L1正则作为wide部分的优化器，Deep部分使用AdaGrad优化。\n联合模型在图1中间展示，LR模型的公式为：\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172910.png)\n\n### （4） 推荐系统实现\n推荐系统实现包括3个部分：数据生成，模型训练，模型serving，如下图所示，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/pipeline.png)\n\n**数据生成**：用户action数据和app曝光数据作为训练数据；\n**模型训练**：如图4中所示方法，在训练期间，输入层接受训练数据和词汇表的输入，一起为一个label生成sparse和dense特征。wide组件包含了用户安装app和曝光app的cross-product tansformation。对于模型的deep组件，会为每个类别型特征学到一个32维的embedding向量。我们将所有embeddings联接起来形成dense features，产生一个接近1200维的dense vector。联接向量接着输入到3个ReLU层，以及最终的logistic输出单元。\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/app%20rec.png!origin)\n**模型serving**：论文中说到，作者首先实现了一个warm-starting系统，会使用前一个模型的embeddings和线性模型权重来初始化一个新的模型，在验证之后在线上做AB测试。\n\n### （5）实验效果\n可以看到采用这个模型之后，离线AUC提升不多，但是线上效果提升了3.9%，还是可以的\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172547.png)\n\n\n## 2. 相关知识引入\n\n### [1] nonlinear feature 和 feature transformations \n我们一般讲到的分类模型（线性分类、LR等等）基本都是线性的（假设数据空间线性可分），但是实际上有时候不是简单的一条线分分开，比如圆形分割线，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173057.png)\n\n更多内容可以参考（相关链接：http://beader.me/2014/08/30/nonlinear-transformation/），我们一般说的非线性变换，指的是多项式变换(Polynomial Transform)。\n\n用符号ϕQ来表示Q次多项式变换:\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173333.png)\n\n### [2] feed-forward neural network\n前馈网络中各个神经元按接受信息的先后分为不同的组。每一组可以看 作一个神经层。每一层中的神经元接受前一层神经元的输出，并输出到下一层 神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播。前馈网 络可以用一个有向无环路图表示。前馈网络可以看作一个函数，通过简单非线 性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单， 易于实现。前馈网络包括全连接前馈网络和卷积神经网络等。\n\n前馈神经网络也经常称为多层感知器（Multilayer Perceptron，MLP）。但 多层感知器的叫法并不是十分合理，因为前馈神经网络其实是由多层的logistic 回归模型（连续的非线性函数）组成，而不是由多层的感知器（不连续的非线 性函数）组成，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173518.png)\n更多内容参考（https://nndl.github.io/ch5.pdf）\n\n### [3] embeded vector\nEmbedding在数学上表示一个maping, f: X -> Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。\n\n通俗来讲，embeded vector是将词向量化的概念，将一个词映射到一个多维空间向量，比如 苹果 = （0.1，0.6，-0.5...），说到词嵌入，不得不说word2vec，它是谷歌提出一种word embedding 的工具或者算法集合，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。\n\n更多相关（[词嵌入原理](https://d0a31e64-a-62cb3a1a-s-sites.googlegroups.com/site/shaohua03/intro-word-embedding.pdf?attachauth=ANoY7crX2AorNaDwF5V6k05ZL8YaBdb21hwJttgKlbZushozFyGpgN2fKm7thjg8SwtUjy-qsZoGrEP8epdQ7vI4HUMD6gJhfRdaomV-I3XqrJyi08hiYJZk3pAd63vydiftrgIBfGh1jZorpzCmEzou6cZRo1i0j3GaZXLeaJkIIXihfvXXyWCA1bmT9mpB4QcWxDcMdWDIS0mFZx-jw4ZTT4tHK50QVfp08kJuag7dddk4lQjrGHI%3D&attredirects=0)）\n\n### [4] ReLU（Rectified Linear Units）激活函数\n神经网络中经常使用的激活函数有：Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid），2001年根据生物学角度提出了新的激活函数，即ReLU，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173736.png!origin)\n图出自：https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf\n\n这个模型对比Sigmoid系主要变化有三点：①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性，这三个变化更加准确，\n\nsigmoid系的导数根据图中可以看到，斜率变化速度比较小，梯度更新可能丢失。\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173747.png!origin)\n\nReLU相比sigmoid系的激活函数，有以下的优点：\n\n- 梯度不饱和。梯度计算公式为：x>0则为1,因此在反向传播过程中，减轻了梯度弥散的问题，神经网络前几层的参数也可以很快的更新。\n- 计算速度快。正向传播过程中，sigmoid和tanh函数计算激活值时需要计算指数，而Relu函数仅需要设置阈值。如果x<0,f(x)=0，如果x>0,f(x)=x，加快了正向传播的计算速度。\n\n更多参考（http://www.cnblogs.com/neopenx/p/4453161.html）\n\n### [5] Follow-the-regularized-leader(FTRL)\n由Google的H. Brendan McMahan在2010年提出的[2]，作者后来在2011年发表了一篇关于FTRL和AOGD、FOBOS、RDA比较的论文[3]，2013年又和Gary Holt, D. Sculley, Michael Young等人发表了一篇关于FTRL工程化实现的论文[4]。如论文[4]的内容所述，FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，\n\n下面这两篇文章不错，移步读一下：http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm，http://www.cnblogs.com/EE-NovRain/p/3810737.html\n\n### [6] L1正则化\n机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制，防止某一个参数变化太大影响其他参数的拟合。\n\n- L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1\n- L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2\n- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择\n- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合\n\n更多参考（http://blog.csdn.net/zouxy09/article/details/24971995）\n\n### [7] AdaGrad自适应学习率调整\n这个算法是非常nb的，神经网络中有经典五大超参数:学习率(Leraning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)单层神经元数(Units)、正则惩罚项（Regularizer|Normalization），调整这些参数是非常困难的，Matthew D. Zeiler在2012年提出这个方法，AdaGrad思路基本是借鉴L2 Regularizer，不过此时调节的不是W，是Gradient，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png)\n，AdaGrad过程，是一个递推过程，每次从τ=1，推到τ=t，把沿路的Gradient的平方根，作为Regularizer。分母作为Regularizer项的工作机制如下，\n\n- 训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]\n- 训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]，\n    由于Regularizer是专门针对Gradient的，所以有利于解决Gradient Vanish/Expoloding问题。当然也有缺点，\n\n- 初始化W影响初始化梯度，初始化W过大，会导致初始梯度被惩罚得很小。\n此时可以人工加大η的值，但过大的η会使得Regularizer过于敏感，调节幅度很大。\n- 训练到中后期，递推路径上累加的梯度平方和越打越多，迅速使得Gradinet被惩罚逼近0，提前结束训练。\n\n更多（http://www.cnblogs.com/neopenx/p/4768388.html、https://zhuanlan.zhihu.com/p/22252270）\n\n论文：http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n\n### [8] factorization machine(FM)隐因子分解机\nFM旨在解决是稀疏矩阵下的特征组合问题，上面论文中用户点击采用one-hot编码之后，生成的举证是稀疏的，所以有采用这个方法，举例来说，线性模型有如下式子，\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174148.png)\n红箭头所指的两个互异特征分量相互作用也加入进去了，用factor来描述特征分量之间的关系，与LR中的交叉组合特征类似。\n由于特征矩阵是稀疏的，所以将方程式改写为：\n![](http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png)\n上下两个公式比较，得到W=VT,通过学习隐含主题向量间接来学习W矩阵，减少了学习参数的个数，同时由主成分分析PCA推出VVT能表达W，针对数据不足或者矩阵稀疏时参数过多的问题。\n\n更多参考（https://my.oschina.net/keyven/blog/648747）\n\n论文（http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf）\n\n## 参考文档\n（1）[论文：Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf)\n\n(完)\n\n","slug":"wide_deep_learning_for_rec_sys","published":1,"updated":"2017-10-13T10:06:15.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjasynwpb00076ls6rnrnie0a","content":"<p>这篇《Wide &amp; Deep Learning for Recommender Systems》论文是在2016年就已经发表出来了，通过检索相关，发现已经有一些业务在使用这个框架或者改良的版本。最近在我们的产品首页做一个课程推荐，找到了这篇论文，所以就顺手学习参考做个记录，文章分为以下两个部分：</p>\n<p>(1) 论文阅读分析<br>(2) 相关知识的引入</p>\n<h2 id=\"1-论文阅读（混合翻译和理解）\"><a href=\"#1-论文阅读（混合翻译和理解）\" class=\"headerlink\" title=\"1. 论文阅读（混合翻译和理解）\"></a>1. 论文阅读（混合翻译和理解）</h2><h3 id=\"（1）背景介绍\"><a href=\"#（1）背景介绍\" class=\"headerlink\" title=\"（1）背景介绍\"></a>（1）背景介绍</h3><p>推荐系统可以被看成一个搜索排序系统，我们之前很多的推荐就是利用搜索去做，设计一个scorer，然后rank所有候选的docs，取topK返回给用户，点击转换率也不错（需要基于对业务和场景的深入理解）。同搜索系统一样，同样面对两个问题：memorization（记忆）和 generalization(泛化) 。memorization是基于历史数据，学习到频繁同现的item或者feature，挖掘一些相关性的东西；generalization是基于上一步学习到的特性进行迁移和挖掘一些新feature（很少出现或者没有出现过的特性）。</p>\n<p>基于memorization的推荐系统还是比较典型的，因为是基于之前准确学习到的特征进行推荐的，例如常见的CF算法，Item-CF中发现物品于物品之间的联系给用户推荐，或者基于内容（或tag）的推荐也是类似的，而generalization策略的使用，则可以提升推荐系统的迁移性（多样性），比如使用神经网络等等。</p>\n<p>开始介绍之前，我们先预习下内容:</p>\n<ul>\n<li>1&gt; 先分析了下目前使用广泛的LR线性模型，由于模型比较简单、可扩展、可解释性好，所以应用比较多。LR模型通常在使用one-hot编码的二值化稀疏特征上进行训练，比如论文中关于app下载的特征，user_installed_app=netflix时，设置值为1，通过使用向量积（叉乘）转换稀疏特征之后，memorization可以设置当出现 AND(user_installed_app=netflix, impres- sion_app=pandora”)时，也设置值为1，不过，向量积（叉乘）转换不能泛化出之前没有出现过的query-item特征对。而generalization可以通过更粗粒度的特征去泛化一些新特征。</li>\n<li>2&gt; Embedding-based模型，例如FM算法、深度神经网络可以通过学习一个dense embedding vector（低纬稠密嵌入向量）去泛化出之前没有出现过的query-item特征，使用了更少的特征工程，但是，学习到一个低纬去代表一个有稀疏特征和high-rank的query和item是非常困难的，比如特殊爱好的用户或者相关的item数比较少，这些情况下当使用dense embedding去计算所有的query-items的时候，会出现很多非零的预测，就会导致推荐一些不相关的item。但是，使用向量积转换的线性模型（更少参数）是可以记住这些特殊规则的特征。</li>\n</ul>\n<p><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/widemodel.png!origin\" alt=\"\"></p>\n<p>这篇论文（上图model）就是从memorization、generalization做设计和拓展，做了三个方面的贡献，</p>\n<ul>\n<li>wide和deep模型框架联合训练带embedding的前馈神经网络，以及对于稀疏输入的常用推荐系统所使用的带特征转化的线性模型</li>\n<li>这个推荐系统已经应用于google play</li>\n<li>这套推荐系统已经在tensorflow上实现了高等级的API，详见<a href=\"https://www.tensorflow.org/tutorials/wide_and_deep\" target=\"_blank\" rel=\"external\">google tensorflow</a>；</li>\n</ul>\n<h3 id=\"（2）推荐系统介绍\"><a href=\"#（2）推荐系统介绍\" class=\"headerlink\" title=\"（2）推荐系统介绍\"></a>（2）推荐系统介绍</h3><p>如下图大概看一下推荐系统的结构，用户输入query（用户操作、上下文特征），推荐系统返回一个根据点击或者购买等指标优化之后的结果列表，结果列表中每一条数据的出现就是曝光，用户操作（点击等），item展现log输入模型学习器实时学习，更新现有的打分模型。</p>\n<p><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/resys.png\" alt=\"\"></p>\n<p>实际上，这个图上可以简单的看出这个推荐系统包含两个部分，retrieval和ranking系统。检索系统可以用机器学习或者人工规则去获取用户的相关的候选集，我们项目之前曾采用的是搜索系统（基于lucene的搜索系统）召回一个千级别数量级的候选集，然后在排序推荐的。ranking系统打分排序经常使用P(y|x)，即当用户有x特征的时候y标签出现的概率，x特征包括用户的人口特征、国家、语言、上下文特征（设备、时间特征）、展现特征（app上架时间、历史统计）。这个论文主要使用Wide &amp; Deep learning框架实现ranking模型。</p>\n<h3 id=\"（3）Wide-和Deep-learning模型\"><a href=\"#（3）Wide-和Deep-learning模型\" class=\"headerlink\" title=\"（3）Wide 和Deep learning模型\"></a>（3）Wide 和Deep learning模型</h3><p><strong>1&gt; Wide模型</strong></p>\n<p>wide模型通常是一个 y = wT x + b的线性模型，是一个高维特征+特征组合的LR，在图1中左边部分，y是预测值，x代表特征向量，w是参数，b是bais。</p>\n<p>一个重要的向量积转换定义：<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172730.png\" alt=\"\"><br>，其中cki 为一个boolean变量，如果第i个特征是第k个变换ϕk的一部分，那么为1; 否则为0。</p>\n<p><strong>2&gt; Deep learning 模型</strong></p>\n<p>Deep 模型是一个前馈神经网络，在图1的右边展示。每一个稀疏、高维的特征首先转为低纬、稠密有值的embedding vector，向量随机初始化，然后训练迭代最小化损失函数，低纬的向量向前传播到神经网络的隐藏层，隐藏层进行如下计算a(l+1) = f(W(l)a(l) + b(l)) ，特征首l表示层数，f表示激活函数，通常采用ReLUs， a(l) , b(l) , and W (l) 是activations、bais，和模型权重。</p>\n<p><strong>3&gt; Wide &amp; Deep模型联合训练</strong></p>\n<p>论文中模型的训练采用联合训练，即两个模型通过logistic loss function 连接一起进行训练迭代。Wide &amp; Deep模型的联合训练是通过梯度的BP（反向传播）算法、nini-batch SGD优化完成的。论文的实验中，使用的是Follow- the-regularized-leader (FTRL)算法，使用L1正则作为wide部分的优化器，Deep部分使用AdaGrad优化。<br>联合模型在图1中间展示，LR模型的公式为：<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172910.png\" alt=\"\"></p>\n<h3 id=\"（4）-推荐系统实现\"><a href=\"#（4）-推荐系统实现\" class=\"headerlink\" title=\"（4） 推荐系统实现\"></a>（4） 推荐系统实现</h3><p>推荐系统实现包括3个部分：数据生成，模型训练，模型serving，如下图所示，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/pipeline.png\" alt=\"\"></p>\n<p><strong>数据生成</strong>：用户action数据和app曝光数据作为训练数据；<br><strong>模型训练</strong>：如图4中所示方法，在训练期间，输入层接受训练数据和词汇表的输入，一起为一个label生成sparse和dense特征。wide组件包含了用户安装app和曝光app的cross-product tansformation。对于模型的deep组件，会为每个类别型特征学到一个32维的embedding向量。我们将所有embeddings联接起来形成dense features，产生一个接近1200维的dense vector。联接向量接着输入到3个ReLU层，以及最终的logistic输出单元。<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/app%20rec.png!origin\" alt=\"\"><br><strong>模型serving</strong>：论文中说到，作者首先实现了一个warm-starting系统，会使用前一个模型的embeddings和线性模型权重来初始化一个新的模型，在验证之后在线上做AB测试。</p>\n<h3 id=\"（5）实验效果\"><a href=\"#（5）实验效果\" class=\"headerlink\" title=\"（5）实验效果\"></a>（5）实验效果</h3><p>可以看到采用这个模型之后，离线AUC提升不多，但是线上效果提升了3.9%，还是可以的<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172547.png\" alt=\"\"></p>\n<h2 id=\"2-相关知识引入\"><a href=\"#2-相关知识引入\" class=\"headerlink\" title=\"2. 相关知识引入\"></a>2. 相关知识引入</h2><h3 id=\"1-nonlinear-feature-和-feature-transformations\"><a href=\"#1-nonlinear-feature-和-feature-transformations\" class=\"headerlink\" title=\"[1] nonlinear feature 和 feature transformations\"></a>[1] nonlinear feature 和 feature transformations</h3><p>我们一般讲到的分类模型（线性分类、LR等等）基本都是线性的（假设数据空间线性可分），但是实际上有时候不是简单的一条线分分开，比如圆形分割线，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173057.png\" alt=\"\"></p>\n<p>更多内容可以参考（相关链接：<a href=\"http://beader.me/2014/08/30/nonlinear-transformation/），我们一般说的非线性变换，指的是多项式变换(Polynomial\" target=\"_blank\" rel=\"external\">http://beader.me/2014/08/30/nonlinear-transformation/），我们一般说的非线性变换，指的是多项式变换(Polynomial</a> Transform)。</p>\n<p>用符号ϕQ来表示Q次多项式变换:<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173333.png\" alt=\"\"></p>\n<h3 id=\"2-feed-forward-neural-network\"><a href=\"#2-feed-forward-neural-network\" class=\"headerlink\" title=\"[2] feed-forward neural network\"></a>[2] feed-forward neural network</h3><p>前馈网络中各个神经元按接受信息的先后分为不同的组。每一组可以看 作一个神经层。每一层中的神经元接受前一层神经元的输出，并输出到下一层 神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播。前馈网 络可以用一个有向无环路图表示。前馈网络可以看作一个函数，通过简单非线 性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单， 易于实现。前馈网络包括全连接前馈网络和卷积神经网络等。</p>\n<p>前馈神经网络也经常称为多层感知器（Multilayer Perceptron，MLP）。但 多层感知器的叫法并不是十分合理，因为前馈神经网络其实是由多层的logistic 回归模型（连续的非线性函数）组成，而不是由多层的感知器（不连续的非线 性函数）组成，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173518.png\" alt=\"\"><br>更多内容参考（<a href=\"https://nndl.github.io/ch5.pdf）\" target=\"_blank\" rel=\"external\">https://nndl.github.io/ch5.pdf）</a></p>\n<h3 id=\"3-embeded-vector\"><a href=\"#3-embeded-vector\" class=\"headerlink\" title=\"[3] embeded vector\"></a>[3] embeded vector</h3><p>Embedding在数学上表示一个maping, f: X -&gt; Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 &lt; X2,那么映射后在Y所属空间上同理 Y1 &lt; Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。</p>\n<p>通俗来讲，embeded vector是将词向量化的概念，将一个词映射到一个多维空间向量，比如 苹果 = （0.1，0.6，-0.5…），说到词嵌入，不得不说word2vec，它是谷歌提出一种word embedding 的工具或者算法集合，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。</p>\n<p>更多相关（<a href=\"https://d0a31e64-a-62cb3a1a-s-sites.googlegroups.com/site/shaohua03/intro-word-embedding.pdf?attachauth=ANoY7crX2AorNaDwF5V6k05ZL8YaBdb21hwJttgKlbZushozFyGpgN2fKm7thjg8SwtUjy-qsZoGrEP8epdQ7vI4HUMD6gJhfRdaomV-I3XqrJyi08hiYJZk3pAd63vydiftrgIBfGh1jZorpzCmEzou6cZRo1i0j3GaZXLeaJkIIXihfvXXyWCA1bmT9mpB4QcWxDcMdWDIS0mFZx-jw4ZTT4tHK50QVfp08kJuag7dddk4lQjrGHI%3D&amp;attredirects=0\" target=\"_blank\" rel=\"external\">词嵌入原理</a>）</p>\n<h3 id=\"4-ReLU（Rectified-Linear-Units）激活函数\"><a href=\"#4-ReLU（Rectified-Linear-Units）激活函数\" class=\"headerlink\" title=\"[4] ReLU（Rectified Linear Units）激活函数\"></a>[4] ReLU（Rectified Linear Units）激活函数</h3><p>神经网络中经常使用的激活函数有：Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid），2001年根据生物学角度提出了新的激活函数，即ReLU，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173736.png!origin\" alt=\"\"><br>图出自：<a href=\"https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf\" target=\"_blank\" rel=\"external\">https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf</a></p>\n<p>这个模型对比Sigmoid系主要变化有三点：①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性，这三个变化更加准确，</p>\n<p>sigmoid系的导数根据图中可以看到，斜率变化速度比较小，梯度更新可能丢失。<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173747.png!origin\" alt=\"\"></p>\n<p>ReLU相比sigmoid系的激活函数，有以下的优点：</p>\n<ul>\n<li>梯度不饱和。梯度计算公式为：x&gt;0则为1,因此在反向传播过程中，减轻了梯度弥散的问题，神经网络前几层的参数也可以很快的更新。</li>\n<li>计算速度快。正向传播过程中，sigmoid和tanh函数计算激活值时需要计算指数，而Relu函数仅需要设置阈值。如果x<0,f(x)=0，如果x>0,f(x)=x，加快了正向传播的计算速度。</0,f(x)=0，如果x></li>\n</ul>\n<p>更多参考（<a href=\"http://www.cnblogs.com/neopenx/p/4453161.html）\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/neopenx/p/4453161.html）</a></p>\n<h3 id=\"5-Follow-the-regularized-leader-FTRL\"><a href=\"#5-Follow-the-regularized-leader-FTRL\" class=\"headerlink\" title=\"[5] Follow-the-regularized-leader(FTRL)\"></a>[5] Follow-the-regularized-leader(FTRL)</h3><p>由Google的H. Brendan McMahan在2010年提出的[2]，作者后来在2011年发表了一篇关于FTRL和AOGD、FOBOS、RDA比较的论文[3]，2013年又和Gary Holt, D. Sculley, Michael Young等人发表了一篇关于FTRL工程化实现的论文[4]。如论文[4]的内容所述，FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，</p>\n<p>下面这两篇文章不错，移步读一下：<a href=\"http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm，http://www.cnblogs.com/EE-NovRain/p/3810737.html\" target=\"_blank\" rel=\"external\">http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm，http://www.cnblogs.com/EE-NovRain/p/3810737.html</a></p>\n<h3 id=\"6-L1正则化\"><a href=\"#6-L1正则化\" class=\"headerlink\" title=\"[6] L1正则化\"></a>[6] L1正则化</h3><p>机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制，防止某一个参数变化太大影响其他参数的拟合。</p>\n<ul>\n<li>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1</li>\n<li>L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2</li>\n<li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</li>\n<li>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</li>\n</ul>\n<p>更多参考（<a href=\"http://blog.csdn.net/zouxy09/article/details/24971995）\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zouxy09/article/details/24971995）</a></p>\n<h3 id=\"7-AdaGrad自适应学习率调整\"><a href=\"#7-AdaGrad自适应学习率调整\" class=\"headerlink\" title=\"[7] AdaGrad自适应学习率调整\"></a>[7] AdaGrad自适应学习率调整</h3><p>这个算法是非常nb的，神经网络中有经典五大超参数:学习率(Leraning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)单层神经元数(Units)、正则惩罚项（Regularizer|Normalization），调整这些参数是非常困难的，Matthew D. Zeiler在2012年提出这个方法，AdaGrad思路基本是借鉴L2 Regularizer，不过此时调节的不是W，是Gradient，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png\" alt=\"\"><br>，AdaGrad过程，是一个递推过程，每次从τ=1，推到τ=t，把沿路的Gradient的平方根，作为Regularizer。分母作为Regularizer项的工作机制如下，</p>\n<ul>\n<li>训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]</li>\n<li><p>训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]，<br>  由于Regularizer是专门针对Gradient的，所以有利于解决Gradient Vanish/Expoloding问题。当然也有缺点，</p>\n</li>\n<li><p>初始化W影响初始化梯度，初始化W过大，会导致初始梯度被惩罚得很小。<br>此时可以人工加大η的值，但过大的η会使得Regularizer过于敏感，调节幅度很大。</p>\n</li>\n<li>训练到中后期，递推路径上累加的梯度平方和越打越多，迅速使得Gradinet被惩罚逼近0，提前结束训练。</li>\n</ul>\n<p>更多（<a href=\"http://www.cnblogs.com/neopenx/p/4768388.html、https://zhuanlan.zhihu.com/p/22252270）\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/neopenx/p/4768388.html、https://zhuanlan.zhihu.com/p/22252270）</a></p>\n<p>论文：<a href=\"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\" target=\"_blank\" rel=\"external\">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></p>\n<h3 id=\"8-factorization-machine-FM-隐因子分解机\"><a href=\"#8-factorization-machine-FM-隐因子分解机\" class=\"headerlink\" title=\"[8] factorization machine(FM)隐因子分解机\"></a>[8] factorization machine(FM)隐因子分解机</h3><p>FM旨在解决是稀疏矩阵下的特征组合问题，上面论文中用户点击采用one-hot编码之后，生成的举证是稀疏的，所以有采用这个方法，举例来说，线性模型有如下式子，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174148.png\" alt=\"\"><br>红箭头所指的两个互异特征分量相互作用也加入进去了，用factor来描述特征分量之间的关系，与LR中的交叉组合特征类似。<br>由于特征矩阵是稀疏的，所以将方程式改写为：<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png\" alt=\"\"><br>上下两个公式比较，得到W=VT,通过学习隐含主题向量间接来学习W矩阵，减少了学习参数的个数，同时由主成分分析PCA推出VVT能表达W，针对数据不足或者矩阵稀疏时参数过多的问题。</p>\n<p>更多参考（<a href=\"https://my.oschina.net/keyven/blog/648747）\" target=\"_blank\" rel=\"external\">https://my.oschina.net/keyven/blog/648747）</a></p>\n<p>论文（<a href=\"http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf）\" target=\"_blank\" rel=\"external\">http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf）</a></p>\n<h2 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h2><p>（1）<a href=\"https://arxiv.org/pdf/1606.07792.pdf\" target=\"_blank\" rel=\"external\">论文：Wide &amp; Deep Learning for Recommender Systems</a></p>\n<p>(完)</p>\n","site":{"data":{}},"excerpt":"","more":"<p>这篇《Wide &amp; Deep Learning for Recommender Systems》论文是在2016年就已经发表出来了，通过检索相关，发现已经有一些业务在使用这个框架或者改良的版本。最近在我们的产品首页做一个课程推荐，找到了这篇论文，所以就顺手学习参考做个记录，文章分为以下两个部分：</p>\n<p>(1) 论文阅读分析<br>(2) 相关知识的引入</p>\n<h2 id=\"1-论文阅读（混合翻译和理解）\"><a href=\"#1-论文阅读（混合翻译和理解）\" class=\"headerlink\" title=\"1. 论文阅读（混合翻译和理解）\"></a>1. 论文阅读（混合翻译和理解）</h2><h3 id=\"（1）背景介绍\"><a href=\"#（1）背景介绍\" class=\"headerlink\" title=\"（1）背景介绍\"></a>（1）背景介绍</h3><p>推荐系统可以被看成一个搜索排序系统，我们之前很多的推荐就是利用搜索去做，设计一个scorer，然后rank所有候选的docs，取topK返回给用户，点击转换率也不错（需要基于对业务和场景的深入理解）。同搜索系统一样，同样面对两个问题：memorization（记忆）和 generalization(泛化) 。memorization是基于历史数据，学习到频繁同现的item或者feature，挖掘一些相关性的东西；generalization是基于上一步学习到的特性进行迁移和挖掘一些新feature（很少出现或者没有出现过的特性）。</p>\n<p>基于memorization的推荐系统还是比较典型的，因为是基于之前准确学习到的特征进行推荐的，例如常见的CF算法，Item-CF中发现物品于物品之间的联系给用户推荐，或者基于内容（或tag）的推荐也是类似的，而generalization策略的使用，则可以提升推荐系统的迁移性（多样性），比如使用神经网络等等。</p>\n<p>开始介绍之前，我们先预习下内容:</p>\n<ul>\n<li>1&gt; 先分析了下目前使用广泛的LR线性模型，由于模型比较简单、可扩展、可解释性好，所以应用比较多。LR模型通常在使用one-hot编码的二值化稀疏特征上进行训练，比如论文中关于app下载的特征，user_installed_app=netflix时，设置值为1，通过使用向量积（叉乘）转换稀疏特征之后，memorization可以设置当出现 AND(user_installed_app=netflix, impres- sion_app=pandora”)时，也设置值为1，不过，向量积（叉乘）转换不能泛化出之前没有出现过的query-item特征对。而generalization可以通过更粗粒度的特征去泛化一些新特征。</li>\n<li>2&gt; Embedding-based模型，例如FM算法、深度神经网络可以通过学习一个dense embedding vector（低纬稠密嵌入向量）去泛化出之前没有出现过的query-item特征，使用了更少的特征工程，但是，学习到一个低纬去代表一个有稀疏特征和high-rank的query和item是非常困难的，比如特殊爱好的用户或者相关的item数比较少，这些情况下当使用dense embedding去计算所有的query-items的时候，会出现很多非零的预测，就会导致推荐一些不相关的item。但是，使用向量积转换的线性模型（更少参数）是可以记住这些特殊规则的特征。</li>\n</ul>\n<p><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/widemodel.png!origin\" alt=\"\"></p>\n<p>这篇论文（上图model）就是从memorization、generalization做设计和拓展，做了三个方面的贡献，</p>\n<ul>\n<li>wide和deep模型框架联合训练带embedding的前馈神经网络，以及对于稀疏输入的常用推荐系统所使用的带特征转化的线性模型</li>\n<li>这个推荐系统已经应用于google play</li>\n<li>这套推荐系统已经在tensorflow上实现了高等级的API，详见<a href=\"https://www.tensorflow.org/tutorials/wide_and_deep\" target=\"_blank\" rel=\"external\">google tensorflow</a>；</li>\n</ul>\n<h3 id=\"（2）推荐系统介绍\"><a href=\"#（2）推荐系统介绍\" class=\"headerlink\" title=\"（2）推荐系统介绍\"></a>（2）推荐系统介绍</h3><p>如下图大概看一下推荐系统的结构，用户输入query（用户操作、上下文特征），推荐系统返回一个根据点击或者购买等指标优化之后的结果列表，结果列表中每一条数据的出现就是曝光，用户操作（点击等），item展现log输入模型学习器实时学习，更新现有的打分模型。</p>\n<p><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/resys.png\" alt=\"\"></p>\n<p>实际上，这个图上可以简单的看出这个推荐系统包含两个部分，retrieval和ranking系统。检索系统可以用机器学习或者人工规则去获取用户的相关的候选集，我们项目之前曾采用的是搜索系统（基于lucene的搜索系统）召回一个千级别数量级的候选集，然后在排序推荐的。ranking系统打分排序经常使用P(y|x)，即当用户有x特征的时候y标签出现的概率，x特征包括用户的人口特征、国家、语言、上下文特征（设备、时间特征）、展现特征（app上架时间、历史统计）。这个论文主要使用Wide &amp; Deep learning框架实现ranking模型。</p>\n<h3 id=\"（3）Wide-和Deep-learning模型\"><a href=\"#（3）Wide-和Deep-learning模型\" class=\"headerlink\" title=\"（3）Wide 和Deep learning模型\"></a>（3）Wide 和Deep learning模型</h3><p><strong>1&gt; Wide模型</strong></p>\n<p>wide模型通常是一个 y = wT x + b的线性模型，是一个高维特征+特征组合的LR，在图1中左边部分，y是预测值，x代表特征向量，w是参数，b是bais。</p>\n<p>一个重要的向量积转换定义：<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172730.png\" alt=\"\"><br>，其中cki 为一个boolean变量，如果第i个特征是第k个变换ϕk的一部分，那么为1; 否则为0。</p>\n<p><strong>2&gt; Deep learning 模型</strong></p>\n<p>Deep 模型是一个前馈神经网络，在图1的右边展示。每一个稀疏、高维的特征首先转为低纬、稠密有值的embedding vector，向量随机初始化，然后训练迭代最小化损失函数，低纬的向量向前传播到神经网络的隐藏层，隐藏层进行如下计算a(l+1) = f(W(l)a(l) + b(l)) ，特征首l表示层数，f表示激活函数，通常采用ReLUs， a(l) , b(l) , and W (l) 是activations、bais，和模型权重。</p>\n<p><strong>3&gt; Wide &amp; Deep模型联合训练</strong></p>\n<p>论文中模型的训练采用联合训练，即两个模型通过logistic loss function 连接一起进行训练迭代。Wide &amp; Deep模型的联合训练是通过梯度的BP（反向传播）算法、nini-batch SGD优化完成的。论文的实验中，使用的是Follow- the-regularized-leader (FTRL)算法，使用L1正则作为wide部分的优化器，Deep部分使用AdaGrad优化。<br>联合模型在图1中间展示，LR模型的公式为：<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172910.png\" alt=\"\"></p>\n<h3 id=\"（4）-推荐系统实现\"><a href=\"#（4）-推荐系统实现\" class=\"headerlink\" title=\"（4） 推荐系统实现\"></a>（4） 推荐系统实现</h3><p>推荐系统实现包括3个部分：数据生成，模型训练，模型serving，如下图所示，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/pipeline.png\" alt=\"\"></p>\n<p><strong>数据生成</strong>：用户action数据和app曝光数据作为训练数据；<br><strong>模型训练</strong>：如图4中所示方法，在训练期间，输入层接受训练数据和词汇表的输入，一起为一个label生成sparse和dense特征。wide组件包含了用户安装app和曝光app的cross-product tansformation。对于模型的deep组件，会为每个类别型特征学到一个32维的embedding向量。我们将所有embeddings联接起来形成dense features，产生一个接近1200维的dense vector。联接向量接着输入到3个ReLU层，以及最终的logistic输出单元。<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/app%20rec.png!origin\" alt=\"\"><br><strong>模型serving</strong>：论文中说到，作者首先实现了一个warm-starting系统，会使用前一个模型的embeddings和线性模型权重来初始化一个新的模型，在验证之后在线上做AB测试。</p>\n<h3 id=\"（5）实验效果\"><a href=\"#（5）实验效果\" class=\"headerlink\" title=\"（5）实验效果\"></a>（5）实验效果</h3><p>可以看到采用这个模型之后，离线AUC提升不多，但是线上效果提升了3.9%，还是可以的<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-172547.png\" alt=\"\"></p>\n<h2 id=\"2-相关知识引入\"><a href=\"#2-相关知识引入\" class=\"headerlink\" title=\"2. 相关知识引入\"></a>2. 相关知识引入</h2><h3 id=\"1-nonlinear-feature-和-feature-transformations\"><a href=\"#1-nonlinear-feature-和-feature-transformations\" class=\"headerlink\" title=\"[1] nonlinear feature 和 feature transformations\"></a>[1] nonlinear feature 和 feature transformations</h3><p>我们一般讲到的分类模型（线性分类、LR等等）基本都是线性的（假设数据空间线性可分），但是实际上有时候不是简单的一条线分分开，比如圆形分割线，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173057.png\" alt=\"\"></p>\n<p>更多内容可以参考（相关链接：<a href=\"http://beader.me/2014/08/30/nonlinear-transformation/），我们一般说的非线性变换，指的是多项式变换(Polynomial\" target=\"_blank\" rel=\"external\">http://beader.me/2014/08/30/nonlinear-transformation/），我们一般说的非线性变换，指的是多项式变换(Polynomial</a> Transform)。</p>\n<p>用符号ϕQ来表示Q次多项式变换:<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173333.png\" alt=\"\"></p>\n<h3 id=\"2-feed-forward-neural-network\"><a href=\"#2-feed-forward-neural-network\" class=\"headerlink\" title=\"[2] feed-forward neural network\"></a>[2] feed-forward neural network</h3><p>前馈网络中各个神经元按接受信息的先后分为不同的组。每一组可以看 作一个神经层。每一层中的神经元接受前一层神经元的输出，并输出到下一层 神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播。前馈网 络可以用一个有向无环路图表示。前馈网络可以看作一个函数，通过简单非线 性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单， 易于实现。前馈网络包括全连接前馈网络和卷积神经网络等。</p>\n<p>前馈神经网络也经常称为多层感知器（Multilayer Perceptron，MLP）。但 多层感知器的叫法并不是十分合理，因为前馈神经网络其实是由多层的logistic 回归模型（连续的非线性函数）组成，而不是由多层的感知器（不连续的非线 性函数）组成，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173518.png\" alt=\"\"><br>更多内容参考（<a href=\"https://nndl.github.io/ch5.pdf）\" target=\"_blank\" rel=\"external\">https://nndl.github.io/ch5.pdf）</a></p>\n<h3 id=\"3-embeded-vector\"><a href=\"#3-embeded-vector\" class=\"headerlink\" title=\"[3] embeded vector\"></a>[3] embeded vector</h3><p>Embedding在数学上表示一个maping, f: X -&gt; Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 &lt; X2,那么映射后在Y所属空间上同理 Y1 &lt; Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。</p>\n<p>通俗来讲，embeded vector是将词向量化的概念，将一个词映射到一个多维空间向量，比如 苹果 = （0.1，0.6，-0.5…），说到词嵌入，不得不说word2vec，它是谷歌提出一种word embedding 的工具或者算法集合，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。</p>\n<p>更多相关（<a href=\"https://d0a31e64-a-62cb3a1a-s-sites.googlegroups.com/site/shaohua03/intro-word-embedding.pdf?attachauth=ANoY7crX2AorNaDwF5V6k05ZL8YaBdb21hwJttgKlbZushozFyGpgN2fKm7thjg8SwtUjy-qsZoGrEP8epdQ7vI4HUMD6gJhfRdaomV-I3XqrJyi08hiYJZk3pAd63vydiftrgIBfGh1jZorpzCmEzou6cZRo1i0j3GaZXLeaJkIIXihfvXXyWCA1bmT9mpB4QcWxDcMdWDIS0mFZx-jw4ZTT4tHK50QVfp08kJuag7dddk4lQjrGHI%3D&amp;attredirects=0\" target=\"_blank\" rel=\"external\">词嵌入原理</a>）</p>\n<h3 id=\"4-ReLU（Rectified-Linear-Units）激活函数\"><a href=\"#4-ReLU（Rectified-Linear-Units）激活函数\" class=\"headerlink\" title=\"[4] ReLU（Rectified Linear Units）激活函数\"></a>[4] ReLU（Rectified Linear Units）激活函数</h3><p>神经网络中经常使用的激活函数有：Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid），2001年根据生物学角度提出了新的激活函数，即ReLU，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173736.png!origin\" alt=\"\"><br>图出自：<a href=\"https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf\" target=\"_blank\" rel=\"external\">https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf</a></p>\n<p>这个模型对比Sigmoid系主要变化有三点：①单侧抑制 ②相对宽阔的兴奋边界 ③稀疏激活性，这三个变化更加准确，</p>\n<p>sigmoid系的导数根据图中可以看到，斜率变化速度比较小，梯度更新可能丢失。<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-173747.png!origin\" alt=\"\"></p>\n<p>ReLU相比sigmoid系的激活函数，有以下的优点：</p>\n<ul>\n<li>梯度不饱和。梯度计算公式为：x&gt;0则为1,因此在反向传播过程中，减轻了梯度弥散的问题，神经网络前几层的参数也可以很快的更新。</li>\n<li>计算速度快。正向传播过程中，sigmoid和tanh函数计算激活值时需要计算指数，而Relu函数仅需要设置阈值。如果x<0,f(x)=0，如果x>0,f(x)=x，加快了正向传播的计算速度。</0,f(x)=0，如果x></li>\n</ul>\n<p>更多参考（<a href=\"http://www.cnblogs.com/neopenx/p/4453161.html）\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/neopenx/p/4453161.html）</a></p>\n<h3 id=\"5-Follow-the-regularized-leader-FTRL\"><a href=\"#5-Follow-the-regularized-leader-FTRL\" class=\"headerlink\" title=\"[5] Follow-the-regularized-leader(FTRL)\"></a>[5] Follow-the-regularized-leader(FTRL)</h3><p>由Google的H. Brendan McMahan在2010年提出的[2]，作者后来在2011年发表了一篇关于FTRL和AOGD、FOBOS、RDA比较的论文[3]，2013年又和Gary Holt, D. Sculley, Michael Young等人发表了一篇关于FTRL工程化实现的论文[4]。如论文[4]的内容所述，FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，</p>\n<p>下面这两篇文章不错，移步读一下：<a href=\"http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm，http://www.cnblogs.com/EE-NovRain/p/3810737.html\" target=\"_blank\" rel=\"external\">http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm，http://www.cnblogs.com/EE-NovRain/p/3810737.html</a></p>\n<h3 id=\"6-L1正则化\"><a href=\"#6-L1正则化\" class=\"headerlink\" title=\"[6] L1正则化\"></a>[6] L1正则化</h3><p>机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制，防止某一个参数变化太大影响其他参数的拟合。</p>\n<ul>\n<li>L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1</li>\n<li>L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2</li>\n<li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</li>\n<li>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</li>\n</ul>\n<p>更多参考（<a href=\"http://blog.csdn.net/zouxy09/article/details/24971995）\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zouxy09/article/details/24971995）</a></p>\n<h3 id=\"7-AdaGrad自适应学习率调整\"><a href=\"#7-AdaGrad自适应学习率调整\" class=\"headerlink\" title=\"[7] AdaGrad自适应学习率调整\"></a>[7] AdaGrad自适应学习率调整</h3><p>这个算法是非常nb的，神经网络中有经典五大超参数:学习率(Leraning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)单层神经元数(Units)、正则惩罚项（Regularizer|Normalization），调整这些参数是非常困难的，Matthew D. Zeiler在2012年提出这个方法，AdaGrad思路基本是借鉴L2 Regularizer，不过此时调节的不是W，是Gradient，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png\" alt=\"\"><br>，AdaGrad过程，是一个递推过程，每次从τ=1，推到τ=t，把沿路的Gradient的平方根，作为Regularizer。分母作为Regularizer项的工作机制如下，</p>\n<ul>\n<li>训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]</li>\n<li><p>训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]，<br>  由于Regularizer是专门针对Gradient的，所以有利于解决Gradient Vanish/Expoloding问题。当然也有缺点，</p>\n</li>\n<li><p>初始化W影响初始化梯度，初始化W过大，会导致初始梯度被惩罚得很小。<br>此时可以人工加大η的值，但过大的η会使得Regularizer过于敏感，调节幅度很大。</p>\n</li>\n<li>训练到中后期，递推路径上累加的梯度平方和越打越多，迅速使得Gradinet被惩罚逼近0，提前结束训练。</li>\n</ul>\n<p>更多（<a href=\"http://www.cnblogs.com/neopenx/p/4768388.html、https://zhuanlan.zhihu.com/p/22252270）\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/neopenx/p/4768388.html、https://zhuanlan.zhihu.com/p/22252270）</a></p>\n<p>论文：<a href=\"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\" target=\"_blank\" rel=\"external\">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a></p>\n<h3 id=\"8-factorization-machine-FM-隐因子分解机\"><a href=\"#8-factorization-machine-FM-隐因子分解机\" class=\"headerlink\" title=\"[8] factorization machine(FM)隐因子分解机\"></a>[8] factorization machine(FM)隐因子分解机</h3><p>FM旨在解决是稀疏矩阵下的特征组合问题，上面论文中用户点击采用one-hot编码之后，生成的举证是稀疏的，所以有采用这个方法，举例来说，线性模型有如下式子，<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174148.png\" alt=\"\"><br>红箭头所指的两个互异特征分量相互作用也加入进去了，用factor来描述特征分量之间的关系，与LR中的交叉组合特征类似。<br>由于特征矩阵是稀疏的，所以将方程式改写为：<br><img src=\"http://bed-image.oss-cn-beijing.aliyuncs.com/mweb/wide_deep_rec/WX20171013-174319.png\" alt=\"\"><br>上下两个公式比较，得到W=VT,通过学习隐含主题向量间接来学习W矩阵，减少了学习参数的个数，同时由主成分分析PCA推出VVT能表达W，针对数据不足或者矩阵稀疏时参数过多的问题。</p>\n<p>更多参考（<a href=\"https://my.oschina.net/keyven/blog/648747）\" target=\"_blank\" rel=\"external\">https://my.oschina.net/keyven/blog/648747）</a></p>\n<p>论文（<a href=\"http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf）\" target=\"_blank\" rel=\"external\">http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf）</a></p>\n<h2 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h2><p>（1）<a href=\"https://arxiv.org/pdf/1606.07792.pdf\" target=\"_blank\" rel=\"external\">论文：Wide &amp; Deep Learning for Recommender Systems</a></p>\n<p>(完)</p>\n"},{"title":"elasticsearch 技术手册(持续更新版本) v1.01","author":"Atlas","author_id":"Atlas","date":"2017-09-26T02:47:00.000Z","english_title":"elasticsearch_tec_book","toc":true,"_content":"\n# elasticsearch技术手册 v1.01\n\n## 1. 基础\n本手册内容是基于`elasticsearch5+`版本。准确的说是5.0.1版本。\n### 1.概念\n集群（cluster）、节点（node）、索引（index）、分片（shards）、副本（replicas）；\nterm、tf-idf、boost等\n### 2. Elasticsearch features\n\n1. [Based in lucene, write in java]()\n2. [Realtime analytics]()\n3. [Full Text search engine]()\n4. [Distributed, easy to scale]()\n5. [High availability]()\n6. [Document oriented(json)]()\n7. [Schema free]()\n8. [Restful API, json over http]()\n9. [Open source:Apache License 6.0 (ES:5.x)]()\n10. [Plugins & Community support]()\n\n### 3. elasticsearch do what on lucene?\nElasticsearch 构建在lucene之上，提供json方式的rest api进行交互；\n\n1. Elasticsearch在lucene之上提供一个完整的分布式系统；\n2. Elasticsearch提供了一个分布式的抽象的数据结构；\n3. 提供了一些特性，例如线程池、队列、node/cluster监控api、数据监控api、以及集群管理等等；\n\n## 2. 生产环境\n### 1. 监控（Monitoring）\n对于已经初步部署完成的elasticsearch集群来说，接下来的集群监控就变的更重要了。集群的重要参数，比如集群状态，分片状态等是集群健康的体现。elasticsearch提供了很对的现成api供我们管理和监控cluster。\n其中，(1)marvel是一个很容易监控elasticsearch的工具。它可以整合大量的统计数据通过kibana。\n(2) cluster health\n### 2. 生产环境部署（Production Deploying）\n生产环境的部署有很多考究的地方，接下来我从以下三个方面来说。\n#### 运维部署考虑（硬件以及部署策略）\n（1）memory，elasticsearch是比较吃内存的，尤其像排序、聚合操作，所以保证足够的heap内存是重要的。如果对内存不够的话，会交换到系统的缓存，由于lucene的数据结构是disk-based的格式，这势必会影响搜索的性能；一般建议使用16g-64gRAM的机器。如果大于64g，则会出现[另外的一些问题](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html)\n（2）cpus,和内存相比，搜索对cpu的要求不是特别高，一般使用多核cpu就行，比如2-8核的；\n（3）disk，硬盘的性能对搜索集群非常重要，磁盘的性能直接影响索引的构建和读写操作，很多时候是搜索的一个瓶颈。ssd硬盘是目前最好的方式，但是由于其价钱看看阿里云，是同样的`喜人`，所以看业务需要，力所能及吧，我们目前使用的是高性能磁盘（high-performance server disks, 15k RPM drives），可以满足业务需求。\n（4）network，一个快速稳定的网络环境对分布式系统非常的重要，低延迟、高带宽有利于节点间的交互以及分片的拷贝和恢复\n（5）其他，尽量避免使用小配置机器组合一个超大的集群，这样管理起来就是一个大坑\n#### 优化配置参数\n(1)Java Virtual Machine\n(2)Transport Client Versus Node Client\nvs:\nTransport Client 可以解耦你的应用和搜索服务，应用可以很快的创建和销毁连接；\nNode Client 可以和搜索服务保持一个持久连接，可以查看搜索的结构信息；\n(3)Important Configuration Changes\nelasticsearch配置文件有非常好的默认设置，都是在实际的工作环境中实践过的。当遇到性能问题的时候，更多的是需要考虑数据存储布局和添加更多的node（elasticsearch文档中特意说明了配置文件的重要性，不让随便更改，大多数情况下是正确的）。\n\n- name\n\n```\n1. Assign Names\ncluster.name: elasticsearch_production\nnode.name: elasticsearch_005_data\n\n2. Paths\npath.data: /path/to/data1,/path/to/data2 \n\n# Path to log files:\npath.logs: /path/to/logs\n\n# Path to where plugins are installed:\npath.plugins: /path/to/plugins\n\n```\n\n- minimum_master_nodes，这个参数是在配置文件中比较重要的一个，如果配置不对的话，会发生split brains（俗称“脑裂”），就是说会存在多个master节点，继而可能发生丢失data现象。这个参数的计算公式：(number of master-eligible nodes / 2) + 1，举例说明：\n\n\t- 假如你有10个node（可存储数据，可成为master），则设置为6；\n\t- 假如你有三个可选为master的节点，100+个数据节点，则设置为2；\n\t- 假如你有2个常规节点，这个值设置为2，但是如果丢失一个则会造成集群不可用，如果设置为1，则不能保证脑裂的不存在，最好的方法是保证最小的节点数为3.\n\n```\ndiscovery.zen.minimum_master_nodes: 2\n```\n因为elasticsearch是自适应的，节点随时添加或者下线，不过还好，有api我们可以实时调整这个参数，\n\n```\nPUT /_cluster/settings\n{\n    \"persistent\" : {\n        \"discovery.zen.minimum_master_nodes\" : 2\n    }\n}\n```\n\n- Recovery Settings\n恢复策略对elasticsearch是必不可少的，举例来说，假如现在集群（10 nodes）集体下线进行维修升级，当重新启动的时候，先启动了5 nodes，此时集群发现有5个node启动了，会执行shard的备份和交换，直到达到分片平衡，此时如果另外5 nodes加入到集群中，会发生什么呢？cluster会继续rebalance，新加入的节点发现数据集群中已经有了，首先删除本地数据，通知集群发动rebalance，平衡各个shards，这整个过程中shard会发生copy、sweap、delete等操作，耗费好多资源和时间，对一个大集群来说，耗费的更多，不可忍受。所以，elasticsearch有三个参数可以配置这些。\n\n```\ngateway.recover_after_nodes: 8   // 集群中恢复的节点数，就是说当改集群启动了8个几点，才尽兴rebalance\n\n// 这两项说明，本集群有10的node，当10个nodes都启动或者启动了8个node且超过5分钟后就会发起rebalance\ngateway.expected_nodes: 10\ngateway.recover_after_time: 5m\n```\n这些策略只和`整个cluster重启`时生效。\n\n- Prefer Unicast over Multicast\nelasticsearch建议使用单播的方式，虽然依然提供了多播的方式，但存在找不到master等尴尬的问题，不建议使用。\n\n```\ndiscovery.zen.ping.unicast.hosts: [\"host1\", \"host2:port\"]\n```\n（4）不要轻易修改的参数\n\n1. Garbage Collector\nelasticsearch中默认采用Concurrent-Mark and Sweep (CMS)的gc回收器；\n\n2. Threadpools\nelasticsearch中设置线程池非常合理的，如果没有特别情况下不要修改这个值\n\n```\nSearch gets a larger threadpool, and is configured to int((# of cores * 3) / 2) + 1\n```\n(5)Heap: Sizing and Swapping\n(6)File Descriptors and MMap\nelasticsearch混合使用nioFS和MMapFS。\n\n### 3. 插件\n1. 使用[head](https://github.com/mobz/elasticsearch-head)插件来查看索引数据\n2. 使用[kopf](https://github.com/lmenezes/elasticsearch-kopf)来备份集群节点\n3. 使用[bigdesk](https://github.com/lukas-vlcek/bigdesk)查看集群性能\n4. [elasticsearch-sql](https://github.com/NLPchina/elasticsearch-sql) 通过sql进行聚合检索, 可以将sql语句翻译成ES的JSON检索语句\n5. 中文分词（ik、pinying）\n6. [Curator](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html)\n\n## 3. 参数配置\n暂空（后补）\n\n### java优化配置\n(1)Heap不要超过系统可用内存的一半，并且不要超过32GB。\n\n(2) cluster集群jvm调优\n当时我们配置ES的JVM(Xms=Xmx=8G)的垃圾回收器主要是CMS,具体配置如下:\n\n```\n# reduce the per-thread stack size\nJAVA_OPTS=\"$JAVA_OPTS -Xss256k\"\n\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseParNewGC\"\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseConcMarkSweepGC\"\n\nJAVA_OPTS=\"$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75\"\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly\"\n```\n这块在官方说明中，特意强调了不建议替换java垃圾回收器，[官方并不推荐使用G1](https://www.elastic.co/guide/en/elasticsearch/guide/current/_don_8217_t_touch_these_settings.html#_garbage_collector)。\n\n[其他博文](https://www.geekhub.cn/a/1256.html)中有试过使用其他垃圾回收器。他的G1的具体配置如下:\n\n```\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseG1GC \"\n#init_globals()末尾打印日志\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintFlagsFinal \"\n#打印gc引用\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintReferenceGC \"\n#输出虚拟机中GC的详细情况.\nJAVA_OPTS=\"$JAVA_OPTS -verbose:gc \"\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintGCDetails \"\n#Enables printing of time stamps at every GC. By default, this option is disabled.\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintGCTimeStamps \"\n#Enables printing of information about adaptive generation sizing. By default, this option is disabled.\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintAdaptiveSizePolicy \"\n# unlocks diagnostic JVM options\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UnlockDiagnosticVMOptions \"\n#to measure where the time is spent\nJAVA_OPTS=\"$JAVA_OPTS -XX:+G1SummarizeConcMark \"\n#设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。\n#JAVA_OPTS=\"$JAVA_OPTS -XX:InitiatingHeapOccupancyPercent=45 \"\n```\n\n(3) elastic 开启jmx 监控\n有时候监控是必不可少的，所以在有条件的时候可以加上jmx监控\n\n```\n/usr/local/elastic/bin/elasticsearch.in.sh\nJMX_PORT=9305\nJAVA_OPTS=\"$JAVA_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT\"\nJAVA_OPTS=\"$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false\"\nJAVA_OPTS=\"$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false\"\nJAVA_OPTS=\"$JAVA_OPTS -Djava.rmi.server.hostname=xx.xx.xx..xx\"\n```\n\n### elasticsearch.yml\n这个是最重要的配置，只有在你明白之后在修改，之后我在单独写一篇文章介绍目前elasticsearch默认参数是如何影响系统的。\n\n目前配置包括以下几个部分：\n（1）cluster\n（2）节点node\n（3）log／data路径\n（4）内存\n（5）网络\n（5）发现Discovery\n（6）Gateway\n（7）其他变量\n\n```\n# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please see the documentation for further information on configuration options:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/settings.html>\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\ncluster.name: elastic-pro\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\nnode.name: node-0\n#\n# Add custom attributes to the node:\n#\nnode.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: /apps/home/worker/zhangxiaolong/data/index0\n#\n# Path to log files:\n#\npath.logs: /apps/home/worker/zhangxiaolong/data/log0\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\nbootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# Set the bind address to a specific IP (IPv4 or IPv6):\n#\nnetwork.host: 172.16.7.1\n#\n# Set a custom port for HTTP:\n#\nhttp.port: 9201\n#\n# For more information, see the documentation at:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-network.html>\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when new node is started:\n#The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\ndiscovery.zen.ping.unicast.hosts: [\"172.16.7.1:9300\"]\n#\n# Prevent the \"split brain\" by configuring the majority of nodes (total number of nodes / 2 + 1):\n#\ndiscovery.zen.minimum_master_nodes: 2\n#\n# For more information, see the documentation at:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-discovery-zen.html>\n#\n# ---------------------------------- Gateway -----------------------------------\n#\n# Block initial recovery after a full cluster restart until N nodes are started:\n#\ngateway.recover_after_nodes: 2\n#\n# For more information, see the documentation at:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-gateway.html>\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Require explicit names when deleting indices:\n#\n#action.destructive_requires_name: true\n```\n\n### 其他\n（1）线程池设置成内核数，比如八核机器就设置成8，很多阻塞的操作都是Lucene来操作的，比如硬盘读写。搜索的线程设置可以设置成内核数的三倍\n（2）内存交换\n这个对于性能影响是致命的，可以使用命令sudo swapoff -a来暂时关闭，永久关闭需要编辑文件/etc/fstab，也可以在配置文件中添加配置bootstrap.mlockall: true，这样jvm可以锁定这些内存，避免被交换到物理存储介质\n（3）其他\n\n- 如果你不需要近实时功能，则设置index的刷新时间；\n- 如果在进行一个大bulk导入，可以优先考虑设置副本数为0；\n- 如果在index中的doc你没有一个自然增长的id，可以使用Elasticsearch’s 的自动id做标示，如果有自己的id，尽量设计对lucene友好的id；\n\n## 4. Rolling Restarts & 备份数据 & 备份恢复 \n### Rolling Restarts\n一般下线一个node（升级、维修等），elasticsearch会进行rebalance操作，如果你是真正的下线一个node，这个操作是十分正确的，但是你知道这台node会之后重新加入到cluster中，则rebalance操作不恰当了，当shard比较大或者多的时候会严重耗费系统资源。\n\n那我们正确的操作是什么？\n\n``` shell\n1. 查看集群设置\ncurl -XGET http://10.10.160.129:9200/_cluster/settings\n\n2. 如果可能的话，停止正在索引的数据；\n\n3. 停止分片同步，阻止elasticsearch进行rebalance操作\nPUT /_cluster/settings\n{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"none\"\n    }\n}\n\n4.关闭单个node\n\n5.维护或者升级节点node\n\n6.重启node，确认加入cluster\n\n7.重新打开分片同步\nPUT /_cluster/settings\n{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"all\"\n    }\n}\n\n8.针对需要的node重复执行3-7操作\n\n9.到这里就重新恢复了cluster；\n```\n\n### 备份数据\n\n``` shell\n1. 先导入一些数据进行备份\ncurl -XPOST 'http://192.168.56.11:9200/bank/account/_bulk?pretty' --data-binary @accounts.json\ncurl -XPOST 'http://192.168.56.11:9200/shakespeare/_bulk?pretty' --data-binary @shakespeare.json\ncurl -XPOST 'http://192.168.56.11:9200/_bulk?pretty' --data-binary @logs.jsonl\n\n2. 使用API创建一个镜像仓库\ncurl -XPOST http://192.168.56.11:9200/_snapshot/my_backup -d '\n{\n    \"type\": \"fs\", \n    \"settings\": { \n        \"location\": \"/data/mount\"\n        \"compress\":  true \n    }\n}'\n## 解释：\n镜像仓库的名称：my_backup\n镜像仓库的类型：fs。还支持curl，hdfs等。\n镜像仓库的位置：/data/mount 。这个位置必须在配置文件中定义。\n是否启用压缩：compres：true 表示启用压缩。\n\n3. 备份前检查配置\n必须确定备份使用的目录在配置文件中声明了，否则会爆如下错误\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"repository_exception\",\n        \"reason\": \"[test-bakcup] failed to create repository\"\n      }\n    ],\n    \"type\": \"repository_exception\",\n    \"reason\": \"[test-bakcup] failed to create repository\",\n    \"caused_by\": {\n      \"type\": \"creation_exception\",\n      \"reason\": \"Guice creation errors:\\n\\n1) Error injecting constructor, RepositoryException[[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty]\\n  at org.elasticsearch.repositories.fs.FsRepository.<init>(Unknown Source)\\n  while locating org.elasticsearch.repositories.fs.FsRepository\\n  while locating org.elasticsearch.repositories.Repository\\n\\n1 error\",\n      \"caused_by\": {\n        \"type\": \"repository_exception\",\n        \"reason\": \"[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty\"\n      }\n    }\n  },\n  \"status\": 500\n}\n\n4. 开始创建一个快照\n##在后头创建一个快照\ncurl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_1 \n##也可以在前台运行。\ncurl -XPUT  http://192.168.56.11:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\n##上面的参数会在my_backup仓库里创建一个snapshot_1 的快照。\n\n5. 可以选择相应的索引进行备份\ncurl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2 -d '\n{\n    \"indices\": \"bank,logstash-2015.05.18\"\n}'\n## 解释：\n创建一个snapshot_2的快照，只备份bank,logstash-2015.05.18这两个索引。\n\n6. 查看备份状态\n整个备份过程中，可以通过如下命令查看备份进度\n\ncurl -XGET http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_status\n主要由如下几种状态：\na. INITIALIZING 集群状态检查，检查当前集群是否可以做快照，通常这个过程会非常快\nb. STARTED 正在转移数据到仓库\nc. FINALIZING 数据转移完成，正在转移元信息\nd. DONE　完成\ne. FAILED 备份失败\n\n7. 取消备份\ncurl -XDELETE http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812\n\n8. 获取所有快照信息。\ncurl -XGET http://192.168.56.20:9200/_snapshot/my_backup/_all |python -mjson.tool\n##解释\n查看my_backup仓库下的所有快照。\n\n9. 手动删除快照\ncurl -XDELETE http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2\n## 解释\n删除my_backup仓库下的snapshot_2的快照。\n\n```\n\n### 备份恢复\n\n``` json\n1. 恢复备份\ncurl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore\n同备份一样，也可以设置wait_for_completion=true等待恢复结果\n\ncurl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore?wait_for_completion=true\n默认情况下，是恢复所有的索引，我们也可以设置一些参数来指定恢复的索引，以及重命令恢复的索引，这样可以避免覆盖原有的数据.\n\ncurl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore\n{\n    \"indices\": \"index_1\",\n    \"rename_pattern\": \"index_(.+)\",\n    \"rename_replacement\": \"restored_index_$1\"\n}\n上面的indices, 表示只恢复索引’index_1’\nrename_pattern: 表示重命名索引以’index_’开头的索引.\nrename_replacement: 表示将所有的索引重命名为’restored_index_xxx’.如index_1会被重命名为restored_index_1.\n\n2. 查看所有索引的恢复进度\ncurl -XGET http://192.168.0.1:9200/_recovery/\n\n3. 查看索引restored_index_1的恢复进度\ncurl -XGET http://192.168.0.1:9200/_recovery/restored_index_1\n\n4. 取消恢复\n只需要删除索引，即可取消恢复\ncurl -XDELETE http://192.168.0.1:9200/restored_index_1\n```\n\n## 5. 性能优化\n在讲性能优化之前，首先要知道：\n\n\t过早的优化是万恶之源 Premature optimization is the root of all evil.\n\t\t\t\t\t\t\t\t\t\t                —— Donald Knuth\n\n`优化总是发生在目前的情况下不能满足当前的需求，其他我想不出什么理由去优化它。`\n\n`优化很多时候和业务是紧密关联的，优化业务可能比优化程序效率更高、成本更低！`\n\n### 索引性能优化\n索引性能（Index Performance），我们这样定义它，索引的速度是否提高，可以无缝的提供近实时的功能。\n什么时候会发生索引慢呢？\n（1）你读的慢（doc from db，file，inputstream等等）\n（2）你处理的慢（中文下的分词等）\n（3）你写的慢（还是老式的机械盘？！ 高性能的盘或者ssd）\n\n还有就是针对不同场景选择的判断，如果你索引的文件非常大，数量多，那应该选择elasticsearch提供的bulk接口，在create doc速度能跟上的时候，bulk 是可以提高速度的。\n\n### 查询性能优化\n查询性能（Query Perofrmance），说起来比索引更麻烦一些，面对的场景也更多一些；\n\n面对海量数据以及不同的集群，针对业务需求去查询往往会很慢，有什么策略可以搞定这种情况？有，那就是`routing`[Routing a Document to a Shard](https://www.elastic.co/guide/en/elasticsearch/guide/current/routing-value.html)、[_routing field](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/mapping-routing-field.html).\n\n查询策略，分别查询vs合并查询？索引越来越大，单个 shard 也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的 shards 会带来额外的索引压力，即 IO 压力。我们选择了分索引。比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。\n\n索引越来越大，资源使用也越来越多。若是要进行更细的集群分配，大索引使用的资源成倍增加。有什么办法能减小索引？\n根据具体业务需求，减少某些大的索引，这是一个很好的办法，这样这个集群各方面占用的资源会有一定程度的下降，当让你要说这些少的索引怎么办，这些索引可以放在单独的集群中。\n\n## 应用性能优化 - [from youzan](http://tech.youzan.com/search-engine1/)\n一、使用应用级队列防止雪崩\nES一个问题是在高峰期时候极容易发生雪崩. ES有健全的线程池系统来保证并发与稳定性问题. 但是在流量突变的情况下(比如双十一秒杀)还是很容易发生瘫痪的现象, 主要的原因如下:\n\nES几乎为每类操作配置一个线程池; 只能保证每个线程池的资源使用时合理的, 当2个以上的线程池竞争资源时容易造成资源响应不过来.\n\nES没有考虑网络负载导致稳定的问题.\n\n在AS里我们实现了面向请求的全局队列来保证稳定性. 它主要做了3件事情.\n![](http://image.zhangxiaolong.org/mweb/14979525149331.png!medium)\n\n1. 根据业务把请求分成一个个slide, 每个slide对应一个队列. 默认一个应用就是一个slide, 一个应用也可以区分不同的slide, 这样可以保护一个应用内重要的查询.\n2. 每个队列配置一个队列长度, 默认为50.\n3. 每个队列计算这个队列的平均响应时间. 当队列平均响应时间超过200ms, 停止工作1s, 如果请求溢出就写入溢出日志留数据恢复使用. 如果连续10次队列平均响应时间超过500ms就报警, 以便工程师第一时间处理.\n\n二、自动降级\n应用级队列解决雪崩问题有点粗暴, 如果一个应用本身查询就非常慢, 很容易让一个应用持续超时很久. 我们根据搜索引擎的特点编写了自动降级功能.\n\n比如商品搜索的例子, 商品搜索最基本的功能是布尔查询, 但是还需要按照相关性分数和质量度排序等功能, 甚至还有个性化需求. 完成简单的布尔查询, ES使用bitsets操作就可以做到, 但是如果如果需要相关性分, 就必须使用倒排索引, 并有大量CPU消耗来计算分数. ES的bitsets比倒排索引快50倍左右.\n\n对于有降级方案的slide, AS在队列响应过慢时候直接使用降级query代替正常query. 这种方法让我们在不扩容的情况下成功度过了双十一的流量陡增.\n\n三、善用filtered query\n理解lucence filter工作原理对于写出高性能查询语句至关重要. 许多搜索性能优化都和filter的使用有关. filter使用bitsets进行布尔运算, quey使用倒排索引进行计算, 这是filter比query快的原因. bitsets的优势主要体现在: \n\n1. bitsetcache在内存里面, 永不消失(除非被LRU). \n2. bitsets利用CPU原生支持的位运算操作, 比倒排索引快个数量级 \n3. 多个bitsets的与运算也是非常的快(一个64位CPU可以同时计算64个DOC的与运算) \n4. bitsets 在内存的存储是独立于query的, 有很强的复用性 \n5. 如果一个bitset片段全是0, 计算会自动跳过这些片段, 让bitsets在数据稀疏情况下同样表现优于倒排索引.\n\n举个例子:\n``` java \nquery:bool:  \n    tag:'mac'\n    region:'beijing'\n    title: \"apple\"\n```\n\nlucence处理这个query的方式是在倒排索引中寻找这三个term的倒排链 ,并使用跳指针技术求交, 在运算过程中需要对每个doc进行算分. 实际上tag和region对于算分并没有作用, 他们充当是过滤器的作用.\n\n这就是过滤器使用场景, 它只存储存在和不存在两种状态. 如果我们把tag和region使用bitsets进行存储, 这样这两个过滤器可以一直都被缓存在内存里面, 这样会快很多. 另外tag和region之间的求交非常迅速, 因为64位机器可以时间一个CPU周期同时处理64个doc的位运算.\n\n一个lucence金科玉律是: 能用filter就用filter, 除非必须使用query(当且仅当你需要算分的时候).\n正确的写法为:\n\n``` java\nquery:  \n    filtered: \n        query:  \n             title: \"apple\" \n         filter:\n            tag:\"mac\"\n             region:\"beijing\"\n```\n四、其他\n\n1. 线上集群关闭分片自动均衡. 分片的自动均衡主要目的防止更新造成各个分片数据分布不均匀. 但是如果线上一个节点挂掉后, 很容易触发自动均衡, 一时间集群内部的数据移动占用所有带宽. 建议采用闲时定时均衡策略来保证数据的均匀.\n\n2. 尽可能延长refresh时间间隔. 为了确保实时索引es索引刷新时间间隔默认为1秒, 索引刷新会导致查询性能受影响, 在确保业务时效性保证的基础上可以适当延长refresh时间间隔保证查询的性能.\n\n3. 除非有必要把all字段去掉. 索引默认除了索引每个字段外, 还有额外创建一个all的字段, 保存所有文本, 去掉这个字段可以把索引大小降低50%.\n\n4. 创建索引时候, 尽可能把查询比较慢的索引和快的索引物理分离.\n\n##6. 参考（Reference）\n1. [elastic调优参考](http://www.cnblogs.com/guguli/p/5218297.html)\n2. [elastic监控](https://github.com/Wprosdocimo/Elasticsearch-zabbix)\n3. [Mastering Elasticsearch(中文版)](http://udn.yyuap.com/doc/mastering-elasticsearch/chapter-4/41_README.html)\n4. [ELK-权威指南](http://kibana.logstash.es/content/logstash/plugins/input/file.html)\n5. [Elasticsearch 权威指南](http://www.learnes.net/index.html)\n6. [elasticsearch 生产环境配置](http://www.biglittleant.cn/2016/12/01/elastic-study1/)\n7. [有赞搜索引擎实践(工程篇)](http://tech.youzan.com/search-engine1/)\n\n\n**[更新于2017-05-22 - v1.0 ]\n[更新于2017-09-26 - v1.01]**\n\n(完)\n\n","source":"_posts/elasticsearch_tec_book_v1.01.md","raw":"---\ntitle: elasticsearch 技术手册(持续更新版本) v1.01\nauthor: Atlas\nauthor_id: Atlas\ntags:\n  - search\ncategories:\n  - search\ndate: 2017-09-26 10:47:00\nenglish_title: elasticsearch_tec_book\ntoc: true\n---\n\n# elasticsearch技术手册 v1.01\n\n## 1. 基础\n本手册内容是基于`elasticsearch5+`版本。准确的说是5.0.1版本。\n### 1.概念\n集群（cluster）、节点（node）、索引（index）、分片（shards）、副本（replicas）；\nterm、tf-idf、boost等\n### 2. Elasticsearch features\n\n1. [Based in lucene, write in java]()\n2. [Realtime analytics]()\n3. [Full Text search engine]()\n4. [Distributed, easy to scale]()\n5. [High availability]()\n6. [Document oriented(json)]()\n7. [Schema free]()\n8. [Restful API, json over http]()\n9. [Open source:Apache License 6.0 (ES:5.x)]()\n10. [Plugins & Community support]()\n\n### 3. elasticsearch do what on lucene?\nElasticsearch 构建在lucene之上，提供json方式的rest api进行交互；\n\n1. Elasticsearch在lucene之上提供一个完整的分布式系统；\n2. Elasticsearch提供了一个分布式的抽象的数据结构；\n3. 提供了一些特性，例如线程池、队列、node/cluster监控api、数据监控api、以及集群管理等等；\n\n## 2. 生产环境\n### 1. 监控（Monitoring）\n对于已经初步部署完成的elasticsearch集群来说，接下来的集群监控就变的更重要了。集群的重要参数，比如集群状态，分片状态等是集群健康的体现。elasticsearch提供了很对的现成api供我们管理和监控cluster。\n其中，(1)marvel是一个很容易监控elasticsearch的工具。它可以整合大量的统计数据通过kibana。\n(2) cluster health\n### 2. 生产环境部署（Production Deploying）\n生产环境的部署有很多考究的地方，接下来我从以下三个方面来说。\n#### 运维部署考虑（硬件以及部署策略）\n（1）memory，elasticsearch是比较吃内存的，尤其像排序、聚合操作，所以保证足够的heap内存是重要的。如果对内存不够的话，会交换到系统的缓存，由于lucene的数据结构是disk-based的格式，这势必会影响搜索的性能；一般建议使用16g-64gRAM的机器。如果大于64g，则会出现[另外的一些问题](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html)\n（2）cpus,和内存相比，搜索对cpu的要求不是特别高，一般使用多核cpu就行，比如2-8核的；\n（3）disk，硬盘的性能对搜索集群非常重要，磁盘的性能直接影响索引的构建和读写操作，很多时候是搜索的一个瓶颈。ssd硬盘是目前最好的方式，但是由于其价钱看看阿里云，是同样的`喜人`，所以看业务需要，力所能及吧，我们目前使用的是高性能磁盘（high-performance server disks, 15k RPM drives），可以满足业务需求。\n（4）network，一个快速稳定的网络环境对分布式系统非常的重要，低延迟、高带宽有利于节点间的交互以及分片的拷贝和恢复\n（5）其他，尽量避免使用小配置机器组合一个超大的集群，这样管理起来就是一个大坑\n#### 优化配置参数\n(1)Java Virtual Machine\n(2)Transport Client Versus Node Client\nvs:\nTransport Client 可以解耦你的应用和搜索服务，应用可以很快的创建和销毁连接；\nNode Client 可以和搜索服务保持一个持久连接，可以查看搜索的结构信息；\n(3)Important Configuration Changes\nelasticsearch配置文件有非常好的默认设置，都是在实际的工作环境中实践过的。当遇到性能问题的时候，更多的是需要考虑数据存储布局和添加更多的node（elasticsearch文档中特意说明了配置文件的重要性，不让随便更改，大多数情况下是正确的）。\n\n- name\n\n```\n1. Assign Names\ncluster.name: elasticsearch_production\nnode.name: elasticsearch_005_data\n\n2. Paths\npath.data: /path/to/data1,/path/to/data2 \n\n# Path to log files:\npath.logs: /path/to/logs\n\n# Path to where plugins are installed:\npath.plugins: /path/to/plugins\n\n```\n\n- minimum_master_nodes，这个参数是在配置文件中比较重要的一个，如果配置不对的话，会发生split brains（俗称“脑裂”），就是说会存在多个master节点，继而可能发生丢失data现象。这个参数的计算公式：(number of master-eligible nodes / 2) + 1，举例说明：\n\n\t- 假如你有10个node（可存储数据，可成为master），则设置为6；\n\t- 假如你有三个可选为master的节点，100+个数据节点，则设置为2；\n\t- 假如你有2个常规节点，这个值设置为2，但是如果丢失一个则会造成集群不可用，如果设置为1，则不能保证脑裂的不存在，最好的方法是保证最小的节点数为3.\n\n```\ndiscovery.zen.minimum_master_nodes: 2\n```\n因为elasticsearch是自适应的，节点随时添加或者下线，不过还好，有api我们可以实时调整这个参数，\n\n```\nPUT /_cluster/settings\n{\n    \"persistent\" : {\n        \"discovery.zen.minimum_master_nodes\" : 2\n    }\n}\n```\n\n- Recovery Settings\n恢复策略对elasticsearch是必不可少的，举例来说，假如现在集群（10 nodes）集体下线进行维修升级，当重新启动的时候，先启动了5 nodes，此时集群发现有5个node启动了，会执行shard的备份和交换，直到达到分片平衡，此时如果另外5 nodes加入到集群中，会发生什么呢？cluster会继续rebalance，新加入的节点发现数据集群中已经有了，首先删除本地数据，通知集群发动rebalance，平衡各个shards，这整个过程中shard会发生copy、sweap、delete等操作，耗费好多资源和时间，对一个大集群来说，耗费的更多，不可忍受。所以，elasticsearch有三个参数可以配置这些。\n\n```\ngateway.recover_after_nodes: 8   // 集群中恢复的节点数，就是说当改集群启动了8个几点，才尽兴rebalance\n\n// 这两项说明，本集群有10的node，当10个nodes都启动或者启动了8个node且超过5分钟后就会发起rebalance\ngateway.expected_nodes: 10\ngateway.recover_after_time: 5m\n```\n这些策略只和`整个cluster重启`时生效。\n\n- Prefer Unicast over Multicast\nelasticsearch建议使用单播的方式，虽然依然提供了多播的方式，但存在找不到master等尴尬的问题，不建议使用。\n\n```\ndiscovery.zen.ping.unicast.hosts: [\"host1\", \"host2:port\"]\n```\n（4）不要轻易修改的参数\n\n1. Garbage Collector\nelasticsearch中默认采用Concurrent-Mark and Sweep (CMS)的gc回收器；\n\n2. Threadpools\nelasticsearch中设置线程池非常合理的，如果没有特别情况下不要修改这个值\n\n```\nSearch gets a larger threadpool, and is configured to int((# of cores * 3) / 2) + 1\n```\n(5)Heap: Sizing and Swapping\n(6)File Descriptors and MMap\nelasticsearch混合使用nioFS和MMapFS。\n\n### 3. 插件\n1. 使用[head](https://github.com/mobz/elasticsearch-head)插件来查看索引数据\n2. 使用[kopf](https://github.com/lmenezes/elasticsearch-kopf)来备份集群节点\n3. 使用[bigdesk](https://github.com/lukas-vlcek/bigdesk)查看集群性能\n4. [elasticsearch-sql](https://github.com/NLPchina/elasticsearch-sql) 通过sql进行聚合检索, 可以将sql语句翻译成ES的JSON检索语句\n5. 中文分词（ik、pinying）\n6. [Curator](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html)\n\n## 3. 参数配置\n暂空（后补）\n\n### java优化配置\n(1)Heap不要超过系统可用内存的一半，并且不要超过32GB。\n\n(2) cluster集群jvm调优\n当时我们配置ES的JVM(Xms=Xmx=8G)的垃圾回收器主要是CMS,具体配置如下:\n\n```\n# reduce the per-thread stack size\nJAVA_OPTS=\"$JAVA_OPTS -Xss256k\"\n\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseParNewGC\"\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseConcMarkSweepGC\"\n\nJAVA_OPTS=\"$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75\"\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly\"\n```\n这块在官方说明中，特意强调了不建议替换java垃圾回收器，[官方并不推荐使用G1](https://www.elastic.co/guide/en/elasticsearch/guide/current/_don_8217_t_touch_these_settings.html#_garbage_collector)。\n\n[其他博文](https://www.geekhub.cn/a/1256.html)中有试过使用其他垃圾回收器。他的G1的具体配置如下:\n\n```\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UseG1GC \"\n#init_globals()末尾打印日志\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintFlagsFinal \"\n#打印gc引用\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintReferenceGC \"\n#输出虚拟机中GC的详细情况.\nJAVA_OPTS=\"$JAVA_OPTS -verbose:gc \"\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintGCDetails \"\n#Enables printing of time stamps at every GC. By default, this option is disabled.\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintGCTimeStamps \"\n#Enables printing of information about adaptive generation sizing. By default, this option is disabled.\nJAVA_OPTS=\"$JAVA_OPTS -XX:+PrintAdaptiveSizePolicy \"\n# unlocks diagnostic JVM options\nJAVA_OPTS=\"$JAVA_OPTS -XX:+UnlockDiagnosticVMOptions \"\n#to measure where the time is spent\nJAVA_OPTS=\"$JAVA_OPTS -XX:+G1SummarizeConcMark \"\n#设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。\n#JAVA_OPTS=\"$JAVA_OPTS -XX:InitiatingHeapOccupancyPercent=45 \"\n```\n\n(3) elastic 开启jmx 监控\n有时候监控是必不可少的，所以在有条件的时候可以加上jmx监控\n\n```\n/usr/local/elastic/bin/elasticsearch.in.sh\nJMX_PORT=9305\nJAVA_OPTS=\"$JAVA_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT\"\nJAVA_OPTS=\"$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false\"\nJAVA_OPTS=\"$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false\"\nJAVA_OPTS=\"$JAVA_OPTS -Djava.rmi.server.hostname=xx.xx.xx..xx\"\n```\n\n### elasticsearch.yml\n这个是最重要的配置，只有在你明白之后在修改，之后我在单独写一篇文章介绍目前elasticsearch默认参数是如何影响系统的。\n\n目前配置包括以下几个部分：\n（1）cluster\n（2）节点node\n（3）log／data路径\n（4）内存\n（5）网络\n（5）发现Discovery\n（6）Gateway\n（7）其他变量\n\n```\n# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please see the documentation for further information on configuration options:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/settings.html>\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\ncluster.name: elastic-pro\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\nnode.name: node-0\n#\n# Add custom attributes to the node:\n#\nnode.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: /apps/home/worker/zhangxiaolong/data/index0\n#\n# Path to log files:\n#\npath.logs: /apps/home/worker/zhangxiaolong/data/log0\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\nbootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# Set the bind address to a specific IP (IPv4 or IPv6):\n#\nnetwork.host: 172.16.7.1\n#\n# Set a custom port for HTTP:\n#\nhttp.port: 9201\n#\n# For more information, see the documentation at:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-network.html>\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when new node is started:\n#The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\ndiscovery.zen.ping.unicast.hosts: [\"172.16.7.1:9300\"]\n#\n# Prevent the \"split brain\" by configuring the majority of nodes (total number of nodes / 2 + 1):\n#\ndiscovery.zen.minimum_master_nodes: 2\n#\n# For more information, see the documentation at:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-discovery-zen.html>\n#\n# ---------------------------------- Gateway -----------------------------------\n#\n# Block initial recovery after a full cluster restart until N nodes are started:\n#\ngateway.recover_after_nodes: 2\n#\n# For more information, see the documentation at:\n# <https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-gateway.html>\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Require explicit names when deleting indices:\n#\n#action.destructive_requires_name: true\n```\n\n### 其他\n（1）线程池设置成内核数，比如八核机器就设置成8，很多阻塞的操作都是Lucene来操作的，比如硬盘读写。搜索的线程设置可以设置成内核数的三倍\n（2）内存交换\n这个对于性能影响是致命的，可以使用命令sudo swapoff -a来暂时关闭，永久关闭需要编辑文件/etc/fstab，也可以在配置文件中添加配置bootstrap.mlockall: true，这样jvm可以锁定这些内存，避免被交换到物理存储介质\n（3）其他\n\n- 如果你不需要近实时功能，则设置index的刷新时间；\n- 如果在进行一个大bulk导入，可以优先考虑设置副本数为0；\n- 如果在index中的doc你没有一个自然增长的id，可以使用Elasticsearch’s 的自动id做标示，如果有自己的id，尽量设计对lucene友好的id；\n\n## 4. Rolling Restarts & 备份数据 & 备份恢复 \n### Rolling Restarts\n一般下线一个node（升级、维修等），elasticsearch会进行rebalance操作，如果你是真正的下线一个node，这个操作是十分正确的，但是你知道这台node会之后重新加入到cluster中，则rebalance操作不恰当了，当shard比较大或者多的时候会严重耗费系统资源。\n\n那我们正确的操作是什么？\n\n``` shell\n1. 查看集群设置\ncurl -XGET http://10.10.160.129:9200/_cluster/settings\n\n2. 如果可能的话，停止正在索引的数据；\n\n3. 停止分片同步，阻止elasticsearch进行rebalance操作\nPUT /_cluster/settings\n{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"none\"\n    }\n}\n\n4.关闭单个node\n\n5.维护或者升级节点node\n\n6.重启node，确认加入cluster\n\n7.重新打开分片同步\nPUT /_cluster/settings\n{\n    \"transient\" : {\n        \"cluster.routing.allocation.enable\" : \"all\"\n    }\n}\n\n8.针对需要的node重复执行3-7操作\n\n9.到这里就重新恢复了cluster；\n```\n\n### 备份数据\n\n``` shell\n1. 先导入一些数据进行备份\ncurl -XPOST 'http://192.168.56.11:9200/bank/account/_bulk?pretty' --data-binary @accounts.json\ncurl -XPOST 'http://192.168.56.11:9200/shakespeare/_bulk?pretty' --data-binary @shakespeare.json\ncurl -XPOST 'http://192.168.56.11:9200/_bulk?pretty' --data-binary @logs.jsonl\n\n2. 使用API创建一个镜像仓库\ncurl -XPOST http://192.168.56.11:9200/_snapshot/my_backup -d '\n{\n    \"type\": \"fs\", \n    \"settings\": { \n        \"location\": \"/data/mount\"\n        \"compress\":  true \n    }\n}'\n## 解释：\n镜像仓库的名称：my_backup\n镜像仓库的类型：fs。还支持curl，hdfs等。\n镜像仓库的位置：/data/mount 。这个位置必须在配置文件中定义。\n是否启用压缩：compres：true 表示启用压缩。\n\n3. 备份前检查配置\n必须确定备份使用的目录在配置文件中声明了，否则会爆如下错误\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"repository_exception\",\n        \"reason\": \"[test-bakcup] failed to create repository\"\n      }\n    ],\n    \"type\": \"repository_exception\",\n    \"reason\": \"[test-bakcup] failed to create repository\",\n    \"caused_by\": {\n      \"type\": \"creation_exception\",\n      \"reason\": \"Guice creation errors:\\n\\n1) Error injecting constructor, RepositoryException[[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty]\\n  at org.elasticsearch.repositories.fs.FsRepository.<init>(Unknown Source)\\n  while locating org.elasticsearch.repositories.fs.FsRepository\\n  while locating org.elasticsearch.repositories.Repository\\n\\n1 error\",\n      \"caused_by\": {\n        \"type\": \"repository_exception\",\n        \"reason\": \"[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty\"\n      }\n    }\n  },\n  \"status\": 500\n}\n\n4. 开始创建一个快照\n##在后头创建一个快照\ncurl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_1 \n##也可以在前台运行。\ncurl -XPUT  http://192.168.56.11:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\n##上面的参数会在my_backup仓库里创建一个snapshot_1 的快照。\n\n5. 可以选择相应的索引进行备份\ncurl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2 -d '\n{\n    \"indices\": \"bank,logstash-2015.05.18\"\n}'\n## 解释：\n创建一个snapshot_2的快照，只备份bank,logstash-2015.05.18这两个索引。\n\n6. 查看备份状态\n整个备份过程中，可以通过如下命令查看备份进度\n\ncurl -XGET http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_status\n主要由如下几种状态：\na. INITIALIZING 集群状态检查，检查当前集群是否可以做快照，通常这个过程会非常快\nb. STARTED 正在转移数据到仓库\nc. FINALIZING 数据转移完成，正在转移元信息\nd. DONE　完成\ne. FAILED 备份失败\n\n7. 取消备份\ncurl -XDELETE http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812\n\n8. 获取所有快照信息。\ncurl -XGET http://192.168.56.20:9200/_snapshot/my_backup/_all |python -mjson.tool\n##解释\n查看my_backup仓库下的所有快照。\n\n9. 手动删除快照\ncurl -XDELETE http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2\n## 解释\n删除my_backup仓库下的snapshot_2的快照。\n\n```\n\n### 备份恢复\n\n``` json\n1. 恢复备份\ncurl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore\n同备份一样，也可以设置wait_for_completion=true等待恢复结果\n\ncurl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore?wait_for_completion=true\n默认情况下，是恢复所有的索引，我们也可以设置一些参数来指定恢复的索引，以及重命令恢复的索引，这样可以避免覆盖原有的数据.\n\ncurl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore\n{\n    \"indices\": \"index_1\",\n    \"rename_pattern\": \"index_(.+)\",\n    \"rename_replacement\": \"restored_index_$1\"\n}\n上面的indices, 表示只恢复索引’index_1’\nrename_pattern: 表示重命名索引以’index_’开头的索引.\nrename_replacement: 表示将所有的索引重命名为’restored_index_xxx’.如index_1会被重命名为restored_index_1.\n\n2. 查看所有索引的恢复进度\ncurl -XGET http://192.168.0.1:9200/_recovery/\n\n3. 查看索引restored_index_1的恢复进度\ncurl -XGET http://192.168.0.1:9200/_recovery/restored_index_1\n\n4. 取消恢复\n只需要删除索引，即可取消恢复\ncurl -XDELETE http://192.168.0.1:9200/restored_index_1\n```\n\n## 5. 性能优化\n在讲性能优化之前，首先要知道：\n\n\t过早的优化是万恶之源 Premature optimization is the root of all evil.\n\t\t\t\t\t\t\t\t\t\t                —— Donald Knuth\n\n`优化总是发生在目前的情况下不能满足当前的需求，其他我想不出什么理由去优化它。`\n\n`优化很多时候和业务是紧密关联的，优化业务可能比优化程序效率更高、成本更低！`\n\n### 索引性能优化\n索引性能（Index Performance），我们这样定义它，索引的速度是否提高，可以无缝的提供近实时的功能。\n什么时候会发生索引慢呢？\n（1）你读的慢（doc from db，file，inputstream等等）\n（2）你处理的慢（中文下的分词等）\n（3）你写的慢（还是老式的机械盘？！ 高性能的盘或者ssd）\n\n还有就是针对不同场景选择的判断，如果你索引的文件非常大，数量多，那应该选择elasticsearch提供的bulk接口，在create doc速度能跟上的时候，bulk 是可以提高速度的。\n\n### 查询性能优化\n查询性能（Query Perofrmance），说起来比索引更麻烦一些，面对的场景也更多一些；\n\n面对海量数据以及不同的集群，针对业务需求去查询往往会很慢，有什么策略可以搞定这种情况？有，那就是`routing`[Routing a Document to a Shard](https://www.elastic.co/guide/en/elasticsearch/guide/current/routing-value.html)、[_routing field](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/mapping-routing-field.html).\n\n查询策略，分别查询vs合并查询？索引越来越大，单个 shard 也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的 shards 会带来额外的索引压力，即 IO 压力。我们选择了分索引。比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。\n\n索引越来越大，资源使用也越来越多。若是要进行更细的集群分配，大索引使用的资源成倍增加。有什么办法能减小索引？\n根据具体业务需求，减少某些大的索引，这是一个很好的办法，这样这个集群各方面占用的资源会有一定程度的下降，当让你要说这些少的索引怎么办，这些索引可以放在单独的集群中。\n\n## 应用性能优化 - [from youzan](http://tech.youzan.com/search-engine1/)\n一、使用应用级队列防止雪崩\nES一个问题是在高峰期时候极容易发生雪崩. ES有健全的线程池系统来保证并发与稳定性问题. 但是在流量突变的情况下(比如双十一秒杀)还是很容易发生瘫痪的现象, 主要的原因如下:\n\nES几乎为每类操作配置一个线程池; 只能保证每个线程池的资源使用时合理的, 当2个以上的线程池竞争资源时容易造成资源响应不过来.\n\nES没有考虑网络负载导致稳定的问题.\n\n在AS里我们实现了面向请求的全局队列来保证稳定性. 它主要做了3件事情.\n![](http://image.zhangxiaolong.org/mweb/14979525149331.png!medium)\n\n1. 根据业务把请求分成一个个slide, 每个slide对应一个队列. 默认一个应用就是一个slide, 一个应用也可以区分不同的slide, 这样可以保护一个应用内重要的查询.\n2. 每个队列配置一个队列长度, 默认为50.\n3. 每个队列计算这个队列的平均响应时间. 当队列平均响应时间超过200ms, 停止工作1s, 如果请求溢出就写入溢出日志留数据恢复使用. 如果连续10次队列平均响应时间超过500ms就报警, 以便工程师第一时间处理.\n\n二、自动降级\n应用级队列解决雪崩问题有点粗暴, 如果一个应用本身查询就非常慢, 很容易让一个应用持续超时很久. 我们根据搜索引擎的特点编写了自动降级功能.\n\n比如商品搜索的例子, 商品搜索最基本的功能是布尔查询, 但是还需要按照相关性分数和质量度排序等功能, 甚至还有个性化需求. 完成简单的布尔查询, ES使用bitsets操作就可以做到, 但是如果如果需要相关性分, 就必须使用倒排索引, 并有大量CPU消耗来计算分数. ES的bitsets比倒排索引快50倍左右.\n\n对于有降级方案的slide, AS在队列响应过慢时候直接使用降级query代替正常query. 这种方法让我们在不扩容的情况下成功度过了双十一的流量陡增.\n\n三、善用filtered query\n理解lucence filter工作原理对于写出高性能查询语句至关重要. 许多搜索性能优化都和filter的使用有关. filter使用bitsets进行布尔运算, quey使用倒排索引进行计算, 这是filter比query快的原因. bitsets的优势主要体现在: \n\n1. bitsetcache在内存里面, 永不消失(除非被LRU). \n2. bitsets利用CPU原生支持的位运算操作, 比倒排索引快个数量级 \n3. 多个bitsets的与运算也是非常的快(一个64位CPU可以同时计算64个DOC的与运算) \n4. bitsets 在内存的存储是独立于query的, 有很强的复用性 \n5. 如果一个bitset片段全是0, 计算会自动跳过这些片段, 让bitsets在数据稀疏情况下同样表现优于倒排索引.\n\n举个例子:\n``` java \nquery:bool:  \n    tag:'mac'\n    region:'beijing'\n    title: \"apple\"\n```\n\nlucence处理这个query的方式是在倒排索引中寻找这三个term的倒排链 ,并使用跳指针技术求交, 在运算过程中需要对每个doc进行算分. 实际上tag和region对于算分并没有作用, 他们充当是过滤器的作用.\n\n这就是过滤器使用场景, 它只存储存在和不存在两种状态. 如果我们把tag和region使用bitsets进行存储, 这样这两个过滤器可以一直都被缓存在内存里面, 这样会快很多. 另外tag和region之间的求交非常迅速, 因为64位机器可以时间一个CPU周期同时处理64个doc的位运算.\n\n一个lucence金科玉律是: 能用filter就用filter, 除非必须使用query(当且仅当你需要算分的时候).\n正确的写法为:\n\n``` java\nquery:  \n    filtered: \n        query:  \n             title: \"apple\" \n         filter:\n            tag:\"mac\"\n             region:\"beijing\"\n```\n四、其他\n\n1. 线上集群关闭分片自动均衡. 分片的自动均衡主要目的防止更新造成各个分片数据分布不均匀. 但是如果线上一个节点挂掉后, 很容易触发自动均衡, 一时间集群内部的数据移动占用所有带宽. 建议采用闲时定时均衡策略来保证数据的均匀.\n\n2. 尽可能延长refresh时间间隔. 为了确保实时索引es索引刷新时间间隔默认为1秒, 索引刷新会导致查询性能受影响, 在确保业务时效性保证的基础上可以适当延长refresh时间间隔保证查询的性能.\n\n3. 除非有必要把all字段去掉. 索引默认除了索引每个字段外, 还有额外创建一个all的字段, 保存所有文本, 去掉这个字段可以把索引大小降低50%.\n\n4. 创建索引时候, 尽可能把查询比较慢的索引和快的索引物理分离.\n\n##6. 参考（Reference）\n1. [elastic调优参考](http://www.cnblogs.com/guguli/p/5218297.html)\n2. [elastic监控](https://github.com/Wprosdocimo/Elasticsearch-zabbix)\n3. [Mastering Elasticsearch(中文版)](http://udn.yyuap.com/doc/mastering-elasticsearch/chapter-4/41_README.html)\n4. [ELK-权威指南](http://kibana.logstash.es/content/logstash/plugins/input/file.html)\n5. [Elasticsearch 权威指南](http://www.learnes.net/index.html)\n6. [elasticsearch 生产环境配置](http://www.biglittleant.cn/2016/12/01/elastic-study1/)\n7. [有赞搜索引擎实践(工程篇)](http://tech.youzan.com/search-engine1/)\n\n\n**[更新于2017-05-22 - v1.0 ]\n[更新于2017-09-26 - v1.01]**\n\n(完)\n\n","slug":"elasticsearch_tec_book_v1.01","published":1,"updated":"2017-09-30T09:35:55.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjasynwpc00096ls62mdh1z6p","content":"<h1 id=\"elasticsearch技术手册-v1-01\"><a href=\"#elasticsearch技术手册-v1-01\" class=\"headerlink\" title=\"elasticsearch技术手册 v1.01\"></a>elasticsearch技术手册 v1.01</h1><h2 id=\"1-基础\"><a href=\"#1-基础\" class=\"headerlink\" title=\"1. 基础\"></a>1. 基础</h2><p>本手册内容是基于<code>elasticsearch5+</code>版本。准确的说是5.0.1版本。</p>\n<h3 id=\"1-概念\"><a href=\"#1-概念\" class=\"headerlink\" title=\"1.概念\"></a>1.概念</h3><p>集群（cluster）、节点（node）、索引（index）、分片（shards）、副本（replicas）；<br>term、tf-idf、boost等</p>\n<h3 id=\"2-Elasticsearch-features\"><a href=\"#2-Elasticsearch-features\" class=\"headerlink\" title=\"2. Elasticsearch features\"></a>2. Elasticsearch features</h3><ol>\n<li><a href=\"\">Based in lucene, write in java</a></li>\n<li><a href=\"\">Realtime analytics</a></li>\n<li><a href=\"\">Full Text search engine</a></li>\n<li><a href=\"\">Distributed, easy to scale</a></li>\n<li><a href=\"\">High availability</a></li>\n<li><a href=\"\">Document oriented(json)</a></li>\n<li><a href=\"\">Schema free</a></li>\n<li><a href=\"\">Restful API, json over http</a></li>\n<li><a href=\"\">Open source:Apache License 6.0 (ES:5.x)</a></li>\n<li><a href=\"\">Plugins &amp; Community support</a></li>\n</ol>\n<h3 id=\"3-elasticsearch-do-what-on-lucene\"><a href=\"#3-elasticsearch-do-what-on-lucene\" class=\"headerlink\" title=\"3. elasticsearch do what on lucene?\"></a>3. elasticsearch do what on lucene?</h3><p>Elasticsearch 构建在lucene之上，提供json方式的rest api进行交互；</p>\n<ol>\n<li>Elasticsearch在lucene之上提供一个完整的分布式系统；</li>\n<li>Elasticsearch提供了一个分布式的抽象的数据结构；</li>\n<li>提供了一些特性，例如线程池、队列、node/cluster监控api、数据监控api、以及集群管理等等；</li>\n</ol>\n<h2 id=\"2-生产环境\"><a href=\"#2-生产环境\" class=\"headerlink\" title=\"2. 生产环境\"></a>2. 生产环境</h2><h3 id=\"1-监控（Monitoring）\"><a href=\"#1-监控（Monitoring）\" class=\"headerlink\" title=\"1. 监控（Monitoring）\"></a>1. 监控（Monitoring）</h3><p>对于已经初步部署完成的elasticsearch集群来说，接下来的集群监控就变的更重要了。集群的重要参数，比如集群状态，分片状态等是集群健康的体现。elasticsearch提供了很对的现成api供我们管理和监控cluster。<br>其中，(1)marvel是一个很容易监控elasticsearch的工具。它可以整合大量的统计数据通过kibana。<br>(2) cluster health</p>\n<h3 id=\"2-生产环境部署（Production-Deploying）\"><a href=\"#2-生产环境部署（Production-Deploying）\" class=\"headerlink\" title=\"2. 生产环境部署（Production Deploying）\"></a>2. 生产环境部署（Production Deploying）</h3><p>生产环境的部署有很多考究的地方，接下来我从以下三个方面来说。</p>\n<h4 id=\"运维部署考虑（硬件以及部署策略）\"><a href=\"#运维部署考虑（硬件以及部署策略）\" class=\"headerlink\" title=\"运维部署考虑（硬件以及部署策略）\"></a>运维部署考虑（硬件以及部署策略）</h4><p>（1）memory，elasticsearch是比较吃内存的，尤其像排序、聚合操作，所以保证足够的heap内存是重要的。如果对内存不够的话，会交换到系统的缓存，由于lucene的数据结构是disk-based的格式，这势必会影响搜索的性能；一般建议使用16g-64gRAM的机器。如果大于64g，则会出现<a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html\" target=\"_blank\" rel=\"external\">另外的一些问题</a><br>（2）cpus,和内存相比，搜索对cpu的要求不是特别高，一般使用多核cpu就行，比如2-8核的；<br>（3）disk，硬盘的性能对搜索集群非常重要，磁盘的性能直接影响索引的构建和读写操作，很多时候是搜索的一个瓶颈。ssd硬盘是目前最好的方式，但是由于其价钱看看阿里云，是同样的<code>喜人</code>，所以看业务需要，力所能及吧，我们目前使用的是高性能磁盘（high-performance server disks, 15k RPM drives），可以满足业务需求。<br>（4）network，一个快速稳定的网络环境对分布式系统非常的重要，低延迟、高带宽有利于节点间的交互以及分片的拷贝和恢复<br>（5）其他，尽量避免使用小配置机器组合一个超大的集群，这样管理起来就是一个大坑</p>\n<h4 id=\"优化配置参数\"><a href=\"#优化配置参数\" class=\"headerlink\" title=\"优化配置参数\"></a>优化配置参数</h4><p>(1)Java Virtual Machine<br>(2)Transport Client Versus Node Client<br>vs:<br>Transport Client 可以解耦你的应用和搜索服务，应用可以很快的创建和销毁连接；<br>Node Client 可以和搜索服务保持一个持久连接，可以查看搜索的结构信息；<br>(3)Important Configuration Changes<br>elasticsearch配置文件有非常好的默认设置，都是在实际的工作环境中实践过的。当遇到性能问题的时候，更多的是需要考虑数据存储布局和添加更多的node（elasticsearch文档中特意说明了配置文件的重要性，不让随便更改，大多数情况下是正确的）。</p>\n<ul>\n<li>name</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. Assign Names</div><div class=\"line\">cluster.name: elasticsearch_production</div><div class=\"line\">node.name: elasticsearch_005_data</div><div class=\"line\"></div><div class=\"line\">2. Paths</div><div class=\"line\">path.data: /path/to/data1,/path/to/data2 </div><div class=\"line\"></div><div class=\"line\"># Path to log files:</div><div class=\"line\">path.logs: /path/to/logs</div><div class=\"line\"></div><div class=\"line\"># Path to where plugins are installed:</div><div class=\"line\">path.plugins: /path/to/plugins</div></pre></td></tr></table></figure>\n<ul>\n<li><p>minimum_master_nodes，这个参数是在配置文件中比较重要的一个，如果配置不对的话，会发生split brains（俗称“脑裂”），就是说会存在多个master节点，继而可能发生丢失data现象。这个参数的计算公式：(number of master-eligible nodes / 2) + 1，举例说明：</p>\n<ul>\n<li>假如你有10个node（可存储数据，可成为master），则设置为6；</li>\n<li>假如你有三个可选为master的节点，100+个数据节点，则设置为2；</li>\n<li>假如你有2个常规节点，这个值设置为2，但是如果丢失一个则会造成集群不可用，如果设置为1，则不能保证脑裂的不存在，最好的方法是保证最小的节点数为3.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">discovery.zen.minimum_master_nodes: 2</div></pre></td></tr></table></figure>\n<p>因为elasticsearch是自适应的，节点随时添加或者下线，不过还好，有api我们可以实时调整这个参数，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">PUT /_cluster/settings</div><div class=\"line\">&#123;</div><div class=\"line\">    &quot;persistent&quot; : &#123;</div><div class=\"line\">        &quot;discovery.zen.minimum_master_nodes&quot; : 2</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>Recovery Settings<br>恢复策略对elasticsearch是必不可少的，举例来说，假如现在集群（10 nodes）集体下线进行维修升级，当重新启动的时候，先启动了5 nodes，此时集群发现有5个node启动了，会执行shard的备份和交换，直到达到分片平衡，此时如果另外5 nodes加入到集群中，会发生什么呢？cluster会继续rebalance，新加入的节点发现数据集群中已经有了，首先删除本地数据，通知集群发动rebalance，平衡各个shards，这整个过程中shard会发生copy、sweap、delete等操作，耗费好多资源和时间，对一个大集群来说，耗费的更多，不可忍受。所以，elasticsearch有三个参数可以配置这些。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">gateway.recover_after_nodes: 8   // 集群中恢复的节点数，就是说当改集群启动了8个几点，才尽兴rebalance</div><div class=\"line\"></div><div class=\"line\">// 这两项说明，本集群有10的node，当10个nodes都启动或者启动了8个node且超过5分钟后就会发起rebalance</div><div class=\"line\">gateway.expected_nodes: 10</div><div class=\"line\">gateway.recover_after_time: 5m</div></pre></td></tr></table></figure>\n<p>这些策略只和<code>整个cluster重启</code>时生效。</p>\n<ul>\n<li>Prefer Unicast over Multicast<br>elasticsearch建议使用单播的方式，虽然依然提供了多播的方式，但存在找不到master等尴尬的问题，不建议使用。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;]</div></pre></td></tr></table></figure>\n<p>（4）不要轻易修改的参数</p>\n<ol>\n<li><p>Garbage Collector<br>elasticsearch中默认采用Concurrent-Mark and Sweep (CMS)的gc回收器；</p>\n</li>\n<li><p>Threadpools<br>elasticsearch中设置线程池非常合理的，如果没有特别情况下不要修改这个值</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Search gets a larger threadpool, and is configured to int((# of cores * 3) / 2) + 1</div></pre></td></tr></table></figure>\n<p>(5)Heap: Sizing and Swapping<br>(6)File Descriptors and MMap<br>elasticsearch混合使用nioFS和MMapFS。</p>\n<h3 id=\"3-插件\"><a href=\"#3-插件\" class=\"headerlink\" title=\"3. 插件\"></a>3. 插件</h3><ol>\n<li>使用<a href=\"https://github.com/mobz/elasticsearch-head\" target=\"_blank\" rel=\"external\">head</a>插件来查看索引数据</li>\n<li>使用<a href=\"https://github.com/lmenezes/elasticsearch-kopf\" target=\"_blank\" rel=\"external\">kopf</a>来备份集群节点</li>\n<li>使用<a href=\"https://github.com/lukas-vlcek/bigdesk\" target=\"_blank\" rel=\"external\">bigdesk</a>查看集群性能</li>\n<li><a href=\"https://github.com/NLPchina/elasticsearch-sql\" target=\"_blank\" rel=\"external\">elasticsearch-sql</a> 通过sql进行聚合检索, 可以将sql语句翻译成ES的JSON检索语句</li>\n<li>中文分词（ik、pinying）</li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html\" target=\"_blank\" rel=\"external\">Curator</a></li>\n</ol>\n<h2 id=\"3-参数配置\"><a href=\"#3-参数配置\" class=\"headerlink\" title=\"3. 参数配置\"></a>3. 参数配置</h2><p>暂空（后补）</p>\n<h3 id=\"java优化配置\"><a href=\"#java优化配置\" class=\"headerlink\" title=\"java优化配置\"></a>java优化配置</h3><p>(1)Heap不要超过系统可用内存的一半，并且不要超过32GB。</p>\n<p>(2) cluster集群jvm调优<br>当时我们配置ES的JVM(Xms=Xmx=8G)的垃圾回收器主要是CMS,具体配置如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"># reduce the per-thread stack size</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Xss256k&quot;</div><div class=\"line\"></div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseParNewGC&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseConcMarkSweepGC&quot;</div><div class=\"line\"></div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly&quot;</div></pre></td></tr></table></figure>\n<p>这块在官方说明中，特意强调了不建议替换java垃圾回收器，<a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/_don_8217_t_touch_these_settings.html#_garbage_collector\" target=\"_blank\" rel=\"external\">官方并不推荐使用G1</a>。</p>\n<p><a href=\"https://www.geekhub.cn/a/1256.html\" target=\"_blank\" rel=\"external\">其他博文</a>中有试过使用其他垃圾回收器。他的G1的具体配置如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseG1GC &quot;</div><div class=\"line\">#init_globals()末尾打印日志</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintFlagsFinal &quot;</div><div class=\"line\">#打印gc引用</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintReferenceGC &quot;</div><div class=\"line\">#输出虚拟机中GC的详细情况.</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -verbose:gc &quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintGCDetails &quot;</div><div class=\"line\">#Enables printing of time stamps at every GC. By default, this option is disabled.</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintGCTimeStamps &quot;</div><div class=\"line\">#Enables printing of information about adaptive generation sizing. By default, this option is disabled.</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintAdaptiveSizePolicy &quot;</div><div class=\"line\"># unlocks diagnostic JVM options</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UnlockDiagnosticVMOptions &quot;</div><div class=\"line\">#to measure where the time is spent</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+G1SummarizeConcMark &quot;</div><div class=\"line\">#设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。</div><div class=\"line\">#JAVA_OPTS=&quot;$JAVA_OPTS -XX:InitiatingHeapOccupancyPercent=45 &quot;</div></pre></td></tr></table></figure>\n<p>(3) elastic 开启jmx 监控<br>有时候监控是必不可少的，所以在有条件的时候可以加上jmx监控</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">/usr/local/elastic/bin/elasticsearch.in.sh</div><div class=\"line\">JMX_PORT=9305</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Djava.rmi.server.hostname=xx.xx.xx..xx&quot;</div></pre></td></tr></table></figure>\n<h3 id=\"elasticsearch-yml\"><a href=\"#elasticsearch-yml\" class=\"headerlink\" title=\"elasticsearch.yml\"></a>elasticsearch.yml</h3><p>这个是最重要的配置，只有在你明白之后在修改，之后我在单独写一篇文章介绍目前elasticsearch默认参数是如何影响系统的。</p>\n<p>目前配置包括以下几个部分：<br>（1）cluster<br>（2）节点node<br>（3）log／data路径<br>（4）内存<br>（5）网络<br>（5）发现Discovery<br>（6）Gateway<br>（7）其他变量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div></pre></td><td class=\"code\"><pre><div class=\"line\"># ======================== Elasticsearch Configuration =========================</div><div class=\"line\">#</div><div class=\"line\"># NOTE: Elasticsearch comes with reasonable defaults for most settings.</div><div class=\"line\">#       Before you set out to tweak and tune the configuration, make sure you</div><div class=\"line\">#       understand what are you trying to accomplish and the consequences.</div><div class=\"line\">#</div><div class=\"line\"># The primary way of configuring a node is via this file. This template lists</div><div class=\"line\"># the most important settings you may want to configure for a production cluster.</div><div class=\"line\">#</div><div class=\"line\"># Please see the documentation for further information on configuration options:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/settings.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Cluster -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Use a descriptive name for your cluster:</div><div class=\"line\">#</div><div class=\"line\">cluster.name: elastic-pro</div><div class=\"line\">#</div><div class=\"line\"># ------------------------------------ Node ------------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Use a descriptive name for the node:</div><div class=\"line\">#</div><div class=\"line\">node.name: node-0</div><div class=\"line\">#</div><div class=\"line\"># Add custom attributes to the node:</div><div class=\"line\">#</div><div class=\"line\">node.attr.rack: r1</div><div class=\"line\">#</div><div class=\"line\"># ----------------------------------- Paths ------------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Path to directory where to store the data (separate multiple locations by comma):</div><div class=\"line\">#</div><div class=\"line\">path.data: /apps/home/worker/zhangxiaolong/data/index0</div><div class=\"line\">#</div><div class=\"line\"># Path to log files:</div><div class=\"line\">#</div><div class=\"line\">path.logs: /apps/home/worker/zhangxiaolong/data/log0</div><div class=\"line\">#</div><div class=\"line\"># ----------------------------------- Memory -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Lock the memory on startup:</div><div class=\"line\">#</div><div class=\"line\">bootstrap.memory_lock: true</div><div class=\"line\">#</div><div class=\"line\"># Make sure that the heap size is set to about half the memory available</div><div class=\"line\"># on the system and that the owner of the process is allowed to use this</div><div class=\"line\"># limit.</div><div class=\"line\">#</div><div class=\"line\"># Elasticsearch performs poorly when the system is swapping the memory.</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Network -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Set the bind address to a specific IP (IPv4 or IPv6):</div><div class=\"line\">#</div><div class=\"line\">network.host: 172.16.7.1</div><div class=\"line\">#</div><div class=\"line\"># Set a custom port for HTTP:</div><div class=\"line\">#</div><div class=\"line\">http.port: 9201</div><div class=\"line\">#</div><div class=\"line\"># For more information, see the documentation at:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-network.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># --------------------------------- Discovery ----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Pass an initial list of hosts to perform discovery when new node is started:</div><div class=\"line\">#The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]</div><div class=\"line\">#</div><div class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;172.16.7.1:9300&quot;]</div><div class=\"line\">#</div><div class=\"line\"># Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of nodes / 2 + 1):</div><div class=\"line\">#</div><div class=\"line\">discovery.zen.minimum_master_nodes: 2</div><div class=\"line\">#</div><div class=\"line\"># For more information, see the documentation at:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-discovery-zen.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Gateway -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Block initial recovery after a full cluster restart until N nodes are started:</div><div class=\"line\">#</div><div class=\"line\">gateway.recover_after_nodes: 2</div><div class=\"line\">#</div><div class=\"line\"># For more information, see the documentation at:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-gateway.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Various -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Require explicit names when deleting indices:</div><div class=\"line\">#</div><div class=\"line\">#action.destructive_requires_name: true</div></pre></td></tr></table></figure>\n<h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><p>（1）线程池设置成内核数，比如八核机器就设置成8，很多阻塞的操作都是Lucene来操作的，比如硬盘读写。搜索的线程设置可以设置成内核数的三倍<br>（2）内存交换<br>这个对于性能影响是致命的，可以使用命令sudo swapoff -a来暂时关闭，永久关闭需要编辑文件/etc/fstab，也可以在配置文件中添加配置bootstrap.mlockall: true，这样jvm可以锁定这些内存，避免被交换到物理存储介质<br>（3）其他</p>\n<ul>\n<li>如果你不需要近实时功能，则设置index的刷新时间；</li>\n<li>如果在进行一个大bulk导入，可以优先考虑设置副本数为0；</li>\n<li>如果在index中的doc你没有一个自然增长的id，可以使用Elasticsearch’s 的自动id做标示，如果有自己的id，尽量设计对lucene友好的id；</li>\n</ul>\n<h2 id=\"4-Rolling-Restarts-amp-备份数据-amp-备份恢复\"><a href=\"#4-Rolling-Restarts-amp-备份数据-amp-备份恢复\" class=\"headerlink\" title=\"4. Rolling Restarts &amp; 备份数据 &amp; 备份恢复\"></a>4. Rolling Restarts &amp; 备份数据 &amp; 备份恢复</h2><h3 id=\"Rolling-Restarts\"><a href=\"#Rolling-Restarts\" class=\"headerlink\" title=\"Rolling Restarts\"></a>Rolling Restarts</h3><p>一般下线一个node（升级、维修等），elasticsearch会进行rebalance操作，如果你是真正的下线一个node，这个操作是十分正确的，但是你知道这台node会之后重新加入到cluster中，则rebalance操作不恰当了，当shard比较大或者多的时候会严重耗费系统资源。</p>\n<p>那我们正确的操作是什么？</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. 查看集群设置</div><div class=\"line\">curl -XGET http://10.10.160.129:9200/_cluster/settings</div><div class=\"line\"></div><div class=\"line\">2. 如果可能的话，停止正在索引的数据；</div><div class=\"line\"></div><div class=\"line\">3. 停止分片同步，阻止elasticsearch进行rebalance操作</div><div class=\"line\">PUT /_cluster/settings</div><div class=\"line\">&#123;</div><div class=\"line\">    \"transient\" : &#123;</div><div class=\"line\">        \"cluster.routing.allocation.enable\" : \"none\"</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">4.关闭单个node</div><div class=\"line\"></div><div class=\"line\">5.维护或者升级节点node</div><div class=\"line\"></div><div class=\"line\">6.重启node，确认加入cluster</div><div class=\"line\"></div><div class=\"line\">7.重新打开分片同步</div><div class=\"line\">PUT /_cluster/settings</div><div class=\"line\">&#123;</div><div class=\"line\">    \"transient\" : &#123;</div><div class=\"line\">        \"cluster.routing.allocation.enable\" : \"all\"</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">8.针对需要的node重复执行3-7操作</div><div class=\"line\"></div><div class=\"line\">9.到这里就重新恢复了cluster；</div></pre></td></tr></table></figure>\n<h3 id=\"备份数据\"><a href=\"#备份数据\" class=\"headerlink\" title=\"备份数据\"></a>备份数据</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. 先导入一些数据进行备份</div><div class=\"line\">curl -XPOST 'http://192.168.56.11:9200/bank/account/_bulk?pretty' --data-binary @accounts.json</div><div class=\"line\">curl -XPOST 'http://192.168.56.11:9200/shakespeare/_bulk?pretty' --data-binary @shakespeare.json</div><div class=\"line\">curl -XPOST 'http://192.168.56.11:9200/_bulk?pretty' --data-binary @logs.jsonl</div><div class=\"line\"></div><div class=\"line\">2. 使用API创建一个镜像仓库</div><div class=\"line\">curl -XPOST http://192.168.56.11:9200/_snapshot/my_backup -d '</div><div class=\"line\">&#123;</div><div class=\"line\">    \"type\": \"fs\", </div><div class=\"line\">    \"settings\": &#123; </div><div class=\"line\">        \"location\": \"/data/mount\"</div><div class=\"line\">        \"compress\":  true </div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;'</div><div class=\"line\"><span class=\"meta\">#</span># 解释：</div><div class=\"line\">镜像仓库的名称：my_backup</div><div class=\"line\">镜像仓库的类型：fs。还支持curl，hdfs等。</div><div class=\"line\">镜像仓库的位置：/data/mount 。这个位置必须在配置文件中定义。</div><div class=\"line\">是否启用压缩：compres：true 表示启用压缩。</div><div class=\"line\"></div><div class=\"line\">3. 备份前检查配置</div><div class=\"line\">必须确定备份使用的目录在配置文件中声明了，否则会爆如下错误</div><div class=\"line\">&#123;</div><div class=\"line\">  \"error\": &#123;</div><div class=\"line\">    \"root_cause\": [</div><div class=\"line\">      &#123;</div><div class=\"line\">        \"type\": \"repository_exception\",</div><div class=\"line\">        \"reason\": \"[test-bakcup] failed to create repository\"</div><div class=\"line\">      &#125;</div><div class=\"line\">    ],</div><div class=\"line\">    \"type\": \"repository_exception\",</div><div class=\"line\">    \"reason\": \"[test-bakcup] failed to create repository\",</div><div class=\"line\">    \"caused_by\": &#123;</div><div class=\"line\">      \"type\": \"creation_exception\",</div><div class=\"line\">      \"reason\": \"Guice creation errors:\\n\\n1) Error injecting constructor, RepositoryException[[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty]\\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)\\n  while locating org.elasticsearch.repositories.fs.FsRepository\\n  while locating org.elasticsearch.repositories.Repository\\n\\n1 error\",</div><div class=\"line\">      \"caused_by\": &#123;</div><div class=\"line\">        \"type\": \"repository_exception\",</div><div class=\"line\">        \"reason\": \"[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty\"</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;,</div><div class=\"line\">  \"status\": 500</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">4. 开始创建一个快照</div><div class=\"line\"><span class=\"meta\">#</span>#在后头创建一个快照</div><div class=\"line\">curl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_1 </div><div class=\"line\"><span class=\"meta\">#</span>#也可以在前台运行。</div><div class=\"line\">curl -XPUT  http://192.168.56.11:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true</div><div class=\"line\"><span class=\"meta\">#</span>#上面的参数会在my_backup仓库里创建一个snapshot_1 的快照。</div><div class=\"line\"></div><div class=\"line\">5. 可以选择相应的索引进行备份</div><div class=\"line\">curl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2 -d '</div><div class=\"line\">&#123;</div><div class=\"line\">    \"indices\": \"bank,logstash-2015.05.18\"</div><div class=\"line\">&#125;'</div><div class=\"line\"><span class=\"meta\">#</span># 解释：</div><div class=\"line\">创建一个snapshot_2的快照，只备份bank,logstash-2015.05.18这两个索引。</div><div class=\"line\"></div><div class=\"line\">6. 查看备份状态</div><div class=\"line\">整个备份过程中，可以通过如下命令查看备份进度</div><div class=\"line\"></div><div class=\"line\">curl -XGET http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_status</div><div class=\"line\">主要由如下几种状态：</div><div class=\"line\">a. INITIALIZING 集群状态检查，检查当前集群是否可以做快照，通常这个过程会非常快</div><div class=\"line\">b. STARTED 正在转移数据到仓库</div><div class=\"line\">c. FINALIZING 数据转移完成，正在转移元信息</div><div class=\"line\">d. DONE　完成</div><div class=\"line\">e. FAILED 备份失败</div><div class=\"line\"></div><div class=\"line\">7. 取消备份</div><div class=\"line\">curl -XDELETE http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812</div><div class=\"line\"></div><div class=\"line\">8. 获取所有快照信息。</div><div class=\"line\">curl -XGET http://192.168.56.20:9200/_snapshot/my_backup/_all |python -mjson.tool</div><div class=\"line\"><span class=\"meta\">#</span>#解释</div><div class=\"line\">查看my_backup仓库下的所有快照。</div><div class=\"line\"></div><div class=\"line\">9. 手动删除快照</div><div class=\"line\">curl -XDELETE http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2</div><div class=\"line\"><span class=\"meta\">#</span># 解释</div><div class=\"line\">删除my_backup仓库下的snapshot_2的快照。</div></pre></td></tr></table></figure>\n<h3 id=\"备份恢复\"><a href=\"#备份恢复\" class=\"headerlink\" title=\"备份恢复\"></a>备份恢复</h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. 恢复备份</div><div class=\"line\">curl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore</div><div class=\"line\">同备份一样，也可以设置wait_for_completion=true等待恢复结果</div><div class=\"line\"></div><div class=\"line\">curl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore?wait_for_completion=true</div><div class=\"line\">默认情况下，是恢复所有的索引，我们也可以设置一些参数来指定恢复的索引，以及重命令恢复的索引，这样可以避免覆盖原有的数据.</div><div class=\"line\"></div><div class=\"line\">curl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore</div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"attr\">\"indices\"</span>: <span class=\"string\">\"index_1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"rename_pattern\"</span>: <span class=\"string\">\"index_(.+)\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"rename_replacement\"</span>: <span class=\"string\">\"restored_index_$1\"</span></div><div class=\"line\">&#125;</div><div class=\"line\">上面的indices, 表示只恢复索引’index_1’</div><div class=\"line\">rename_pattern: 表示重命名索引以’index_’开头的索引.</div><div class=\"line\">rename_replacement: 表示将所有的索引重命名为’restored_index_xxx’.如index_1会被重命名为restored_index_1.</div><div class=\"line\"></div><div class=\"line\">2. 查看所有索引的恢复进度</div><div class=\"line\">curl -XGET http://192.168.0.1:9200/_recovery/</div><div class=\"line\"></div><div class=\"line\">3. 查看索引restored_index_1的恢复进度</div><div class=\"line\">curl -XGET http://192.168.0.1:9200/_recovery/restored_index_1</div><div class=\"line\"></div><div class=\"line\">4. 取消恢复</div><div class=\"line\">只需要删除索引，即可取消恢复</div><div class=\"line\">curl -XDELETE http://192.168.0.1:9200/restored_index_1</div></pre></td></tr></table></figure>\n<h2 id=\"5-性能优化\"><a href=\"#5-性能优化\" class=\"headerlink\" title=\"5. 性能优化\"></a>5. 性能优化</h2><p>在讲性能优化之前，首先要知道：</p>\n<pre><code>过早的优化是万恶之源 Premature optimization is the root of all evil.\n                                                    —— Donald Knuth\n</code></pre><p><code>优化总是发生在目前的情况下不能满足当前的需求，其他我想不出什么理由去优化它。</code></p>\n<p><code>优化很多时候和业务是紧密关联的，优化业务可能比优化程序效率更高、成本更低！</code></p>\n<h3 id=\"索引性能优化\"><a href=\"#索引性能优化\" class=\"headerlink\" title=\"索引性能优化\"></a>索引性能优化</h3><p>索引性能（Index Performance），我们这样定义它，索引的速度是否提高，可以无缝的提供近实时的功能。<br>什么时候会发生索引慢呢？<br>（1）你读的慢（doc from db，file，inputstream等等）<br>（2）你处理的慢（中文下的分词等）<br>（3）你写的慢（还是老式的机械盘？！ 高性能的盘或者ssd）</p>\n<p>还有就是针对不同场景选择的判断，如果你索引的文件非常大，数量多，那应该选择elasticsearch提供的bulk接口，在create doc速度能跟上的时候，bulk 是可以提高速度的。</p>\n<h3 id=\"查询性能优化\"><a href=\"#查询性能优化\" class=\"headerlink\" title=\"查询性能优化\"></a>查询性能优化</h3><p>查询性能（Query Perofrmance），说起来比索引更麻烦一些，面对的场景也更多一些；</p>\n<p>面对海量数据以及不同的集群，针对业务需求去查询往往会很慢，有什么策略可以搞定这种情况？有，那就是<code>routing</code><a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/routing-value.html\" target=\"_blank\" rel=\"external\">Routing a Document to a Shard</a>、<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.1/mapping-routing-field.html\" target=\"_blank\" rel=\"external\">_routing field</a>.</p>\n<p>查询策略，分别查询vs合并查询？索引越来越大，单个 shard 也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的 shards 会带来额外的索引压力，即 IO 压力。我们选择了分索引。比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。</p>\n<p>索引越来越大，资源使用也越来越多。若是要进行更细的集群分配，大索引使用的资源成倍增加。有什么办法能减小索引？<br>根据具体业务需求，减少某些大的索引，这是一个很好的办法，这样这个集群各方面占用的资源会有一定程度的下降，当让你要说这些少的索引怎么办，这些索引可以放在单独的集群中。</p>\n<h2 id=\"应用性能优化-from-youzan\"><a href=\"#应用性能优化-from-youzan\" class=\"headerlink\" title=\"应用性能优化 - from youzan\"></a>应用性能优化 - <a href=\"http://tech.youzan.com/search-engine1/\" target=\"_blank\" rel=\"external\">from youzan</a></h2><p>一、使用应用级队列防止雪崩<br>ES一个问题是在高峰期时候极容易发生雪崩. ES有健全的线程池系统来保证并发与稳定性问题. 但是在流量突变的情况下(比如双十一秒杀)还是很容易发生瘫痪的现象, 主要的原因如下:</p>\n<p>ES几乎为每类操作配置一个线程池; 只能保证每个线程池的资源使用时合理的, 当2个以上的线程池竞争资源时容易造成资源响应不过来.</p>\n<p>ES没有考虑网络负载导致稳定的问题.</p>\n<p>在AS里我们实现了面向请求的全局队列来保证稳定性. 它主要做了3件事情.<br><img src=\"http://image.zhangxiaolong.org/mweb/14979525149331.png!medium\" alt=\"\"></p>\n<ol>\n<li>根据业务把请求分成一个个slide, 每个slide对应一个队列. 默认一个应用就是一个slide, 一个应用也可以区分不同的slide, 这样可以保护一个应用内重要的查询.</li>\n<li>每个队列配置一个队列长度, 默认为50.</li>\n<li>每个队列计算这个队列的平均响应时间. 当队列平均响应时间超过200ms, 停止工作1s, 如果请求溢出就写入溢出日志留数据恢复使用. 如果连续10次队列平均响应时间超过500ms就报警, 以便工程师第一时间处理.</li>\n</ol>\n<p>二、自动降级<br>应用级队列解决雪崩问题有点粗暴, 如果一个应用本身查询就非常慢, 很容易让一个应用持续超时很久. 我们根据搜索引擎的特点编写了自动降级功能.</p>\n<p>比如商品搜索的例子, 商品搜索最基本的功能是布尔查询, 但是还需要按照相关性分数和质量度排序等功能, 甚至还有个性化需求. 完成简单的布尔查询, ES使用bitsets操作就可以做到, 但是如果如果需要相关性分, 就必须使用倒排索引, 并有大量CPU消耗来计算分数. ES的bitsets比倒排索引快50倍左右.</p>\n<p>对于有降级方案的slide, AS在队列响应过慢时候直接使用降级query代替正常query. 这种方法让我们在不扩容的情况下成功度过了双十一的流量陡增.</p>\n<p>三、善用filtered query<br>理解lucence filter工作原理对于写出高性能查询语句至关重要. 许多搜索性能优化都和filter的使用有关. filter使用bitsets进行布尔运算, quey使用倒排索引进行计算, 这是filter比query快的原因. bitsets的优势主要体现在: </p>\n<ol>\n<li>bitsetcache在内存里面, 永不消失(除非被LRU). </li>\n<li>bitsets利用CPU原生支持的位运算操作, 比倒排索引快个数量级 </li>\n<li>多个bitsets的与运算也是非常的快(一个64位CPU可以同时计算64个DOC的与运算) </li>\n<li>bitsets 在内存的存储是独立于query的, 有很强的复用性 </li>\n<li>如果一个bitset片段全是0, 计算会自动跳过这些片段, 让bitsets在数据稀疏情况下同样表现优于倒排索引.</li>\n</ol>\n<p>举个例子:<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">query:bool:  </div><div class=\"line\">    tag:<span class=\"string\">'mac'</span></div><div class=\"line\">    region:<span class=\"string\">'beijing'</span></div><div class=\"line\">    title: <span class=\"string\">\"apple\"</span></div></pre></td></tr></table></figure></p>\n<p>lucence处理这个query的方式是在倒排索引中寻找这三个term的倒排链 ,并使用跳指针技术求交, 在运算过程中需要对每个doc进行算分. 实际上tag和region对于算分并没有作用, 他们充当是过滤器的作用.</p>\n<p>这就是过滤器使用场景, 它只存储存在和不存在两种状态. 如果我们把tag和region使用bitsets进行存储, 这样这两个过滤器可以一直都被缓存在内存里面, 这样会快很多. 另外tag和region之间的求交非常迅速, 因为64位机器可以时间一个CPU周期同时处理64个doc的位运算.</p>\n<p>一个lucence金科玉律是: 能用filter就用filter, 除非必须使用query(当且仅当你需要算分的时候).<br>正确的写法为:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">query:  </div><div class=\"line\">    filtered: </div><div class=\"line\">        query:  </div><div class=\"line\">             title: <span class=\"string\">\"apple\"</span> </div><div class=\"line\">         filter:</div><div class=\"line\">            tag:<span class=\"string\">\"mac\"</span></div><div class=\"line\">             region:<span class=\"string\">\"beijing\"</span></div></pre></td></tr></table></figure>\n<p>四、其他</p>\n<ol>\n<li><p>线上集群关闭分片自动均衡. 分片的自动均衡主要目的防止更新造成各个分片数据分布不均匀. 但是如果线上一个节点挂掉后, 很容易触发自动均衡, 一时间集群内部的数据移动占用所有带宽. 建议采用闲时定时均衡策略来保证数据的均匀.</p>\n</li>\n<li><p>尽可能延长refresh时间间隔. 为了确保实时索引es索引刷新时间间隔默认为1秒, 索引刷新会导致查询性能受影响, 在确保业务时效性保证的基础上可以适当延长refresh时间间隔保证查询的性能.</p>\n</li>\n<li><p>除非有必要把all字段去掉. 索引默认除了索引每个字段外, 还有额外创建一个all的字段, 保存所有文本, 去掉这个字段可以把索引大小降低50%.</p>\n</li>\n<li><p>创建索引时候, 尽可能把查询比较慢的索引和快的索引物理分离.</p>\n</li>\n</ol>\n<p>##6. 参考（Reference）</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/guguli/p/5218297.html\" target=\"_blank\" rel=\"external\">elastic调优参考</a></li>\n<li><a href=\"https://github.com/Wprosdocimo/Elasticsearch-zabbix\" target=\"_blank\" rel=\"external\">elastic监控</a></li>\n<li><a href=\"http://udn.yyuap.com/doc/mastering-elasticsearch/chapter-4/41_README.html\" target=\"_blank\" rel=\"external\">Mastering Elasticsearch(中文版)</a></li>\n<li><a href=\"http://kibana.logstash.es/content/logstash/plugins/input/file.html\" target=\"_blank\" rel=\"external\">ELK-权威指南</a></li>\n<li><a href=\"http://www.learnes.net/index.html\" target=\"_blank\" rel=\"external\">Elasticsearch 权威指南</a></li>\n<li><a href=\"http://www.biglittleant.cn/2016/12/01/elastic-study1/\" target=\"_blank\" rel=\"external\">elasticsearch 生产环境配置</a></li>\n<li><a href=\"http://tech.youzan.com/search-engine1/\" target=\"_blank\" rel=\"external\">有赞搜索引擎实践(工程篇)</a></li>\n</ol>\n<p><strong>[更新于2017-05-22 - v1.0 ]<br>[更新于2017-09-26 - v1.01]</strong></p>\n<p>(完)</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"elasticsearch技术手册-v1-01\"><a href=\"#elasticsearch技术手册-v1-01\" class=\"headerlink\" title=\"elasticsearch技术手册 v1.01\"></a>elasticsearch技术手册 v1.01</h1><h2 id=\"1-基础\"><a href=\"#1-基础\" class=\"headerlink\" title=\"1. 基础\"></a>1. 基础</h2><p>本手册内容是基于<code>elasticsearch5+</code>版本。准确的说是5.0.1版本。</p>\n<h3 id=\"1-概念\"><a href=\"#1-概念\" class=\"headerlink\" title=\"1.概念\"></a>1.概念</h3><p>集群（cluster）、节点（node）、索引（index）、分片（shards）、副本（replicas）；<br>term、tf-idf、boost等</p>\n<h3 id=\"2-Elasticsearch-features\"><a href=\"#2-Elasticsearch-features\" class=\"headerlink\" title=\"2. Elasticsearch features\"></a>2. Elasticsearch features</h3><ol>\n<li><a href=\"\">Based in lucene, write in java</a></li>\n<li><a href=\"\">Realtime analytics</a></li>\n<li><a href=\"\">Full Text search engine</a></li>\n<li><a href=\"\">Distributed, easy to scale</a></li>\n<li><a href=\"\">High availability</a></li>\n<li><a href=\"\">Document oriented(json)</a></li>\n<li><a href=\"\">Schema free</a></li>\n<li><a href=\"\">Restful API, json over http</a></li>\n<li><a href=\"\">Open source:Apache License 6.0 (ES:5.x)</a></li>\n<li><a href=\"\">Plugins &amp; Community support</a></li>\n</ol>\n<h3 id=\"3-elasticsearch-do-what-on-lucene\"><a href=\"#3-elasticsearch-do-what-on-lucene\" class=\"headerlink\" title=\"3. elasticsearch do what on lucene?\"></a>3. elasticsearch do what on lucene?</h3><p>Elasticsearch 构建在lucene之上，提供json方式的rest api进行交互；</p>\n<ol>\n<li>Elasticsearch在lucene之上提供一个完整的分布式系统；</li>\n<li>Elasticsearch提供了一个分布式的抽象的数据结构；</li>\n<li>提供了一些特性，例如线程池、队列、node/cluster监控api、数据监控api、以及集群管理等等；</li>\n</ol>\n<h2 id=\"2-生产环境\"><a href=\"#2-生产环境\" class=\"headerlink\" title=\"2. 生产环境\"></a>2. 生产环境</h2><h3 id=\"1-监控（Monitoring）\"><a href=\"#1-监控（Monitoring）\" class=\"headerlink\" title=\"1. 监控（Monitoring）\"></a>1. 监控（Monitoring）</h3><p>对于已经初步部署完成的elasticsearch集群来说，接下来的集群监控就变的更重要了。集群的重要参数，比如集群状态，分片状态等是集群健康的体现。elasticsearch提供了很对的现成api供我们管理和监控cluster。<br>其中，(1)marvel是一个很容易监控elasticsearch的工具。它可以整合大量的统计数据通过kibana。<br>(2) cluster health</p>\n<h3 id=\"2-生产环境部署（Production-Deploying）\"><a href=\"#2-生产环境部署（Production-Deploying）\" class=\"headerlink\" title=\"2. 生产环境部署（Production Deploying）\"></a>2. 生产环境部署（Production Deploying）</h3><p>生产环境的部署有很多考究的地方，接下来我从以下三个方面来说。</p>\n<h4 id=\"运维部署考虑（硬件以及部署策略）\"><a href=\"#运维部署考虑（硬件以及部署策略）\" class=\"headerlink\" title=\"运维部署考虑（硬件以及部署策略）\"></a>运维部署考虑（硬件以及部署策略）</h4><p>（1）memory，elasticsearch是比较吃内存的，尤其像排序、聚合操作，所以保证足够的heap内存是重要的。如果对内存不够的话，会交换到系统的缓存，由于lucene的数据结构是disk-based的格式，这势必会影响搜索的性能；一般建议使用16g-64gRAM的机器。如果大于64g，则会出现<a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html\" target=\"_blank\" rel=\"external\">另外的一些问题</a><br>（2）cpus,和内存相比，搜索对cpu的要求不是特别高，一般使用多核cpu就行，比如2-8核的；<br>（3）disk，硬盘的性能对搜索集群非常重要，磁盘的性能直接影响索引的构建和读写操作，很多时候是搜索的一个瓶颈。ssd硬盘是目前最好的方式，但是由于其价钱看看阿里云，是同样的<code>喜人</code>，所以看业务需要，力所能及吧，我们目前使用的是高性能磁盘（high-performance server disks, 15k RPM drives），可以满足业务需求。<br>（4）network，一个快速稳定的网络环境对分布式系统非常的重要，低延迟、高带宽有利于节点间的交互以及分片的拷贝和恢复<br>（5）其他，尽量避免使用小配置机器组合一个超大的集群，这样管理起来就是一个大坑</p>\n<h4 id=\"优化配置参数\"><a href=\"#优化配置参数\" class=\"headerlink\" title=\"优化配置参数\"></a>优化配置参数</h4><p>(1)Java Virtual Machine<br>(2)Transport Client Versus Node Client<br>vs:<br>Transport Client 可以解耦你的应用和搜索服务，应用可以很快的创建和销毁连接；<br>Node Client 可以和搜索服务保持一个持久连接，可以查看搜索的结构信息；<br>(3)Important Configuration Changes<br>elasticsearch配置文件有非常好的默认设置，都是在实际的工作环境中实践过的。当遇到性能问题的时候，更多的是需要考虑数据存储布局和添加更多的node（elasticsearch文档中特意说明了配置文件的重要性，不让随便更改，大多数情况下是正确的）。</p>\n<ul>\n<li>name</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. Assign Names</div><div class=\"line\">cluster.name: elasticsearch_production</div><div class=\"line\">node.name: elasticsearch_005_data</div><div class=\"line\"></div><div class=\"line\">2. Paths</div><div class=\"line\">path.data: /path/to/data1,/path/to/data2 </div><div class=\"line\"></div><div class=\"line\"># Path to log files:</div><div class=\"line\">path.logs: /path/to/logs</div><div class=\"line\"></div><div class=\"line\"># Path to where plugins are installed:</div><div class=\"line\">path.plugins: /path/to/plugins</div></pre></td></tr></table></figure>\n<ul>\n<li><p>minimum_master_nodes，这个参数是在配置文件中比较重要的一个，如果配置不对的话，会发生split brains（俗称“脑裂”），就是说会存在多个master节点，继而可能发生丢失data现象。这个参数的计算公式：(number of master-eligible nodes / 2) + 1，举例说明：</p>\n<ul>\n<li>假如你有10个node（可存储数据，可成为master），则设置为6；</li>\n<li>假如你有三个可选为master的节点，100+个数据节点，则设置为2；</li>\n<li>假如你有2个常规节点，这个值设置为2，但是如果丢失一个则会造成集群不可用，如果设置为1，则不能保证脑裂的不存在，最好的方法是保证最小的节点数为3.</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">discovery.zen.minimum_master_nodes: 2</div></pre></td></tr></table></figure>\n<p>因为elasticsearch是自适应的，节点随时添加或者下线，不过还好，有api我们可以实时调整这个参数，</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">PUT /_cluster/settings</div><div class=\"line\">&#123;</div><div class=\"line\">    &quot;persistent&quot; : &#123;</div><div class=\"line\">        &quot;discovery.zen.minimum_master_nodes&quot; : 2</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li>Recovery Settings<br>恢复策略对elasticsearch是必不可少的，举例来说，假如现在集群（10 nodes）集体下线进行维修升级，当重新启动的时候，先启动了5 nodes，此时集群发现有5个node启动了，会执行shard的备份和交换，直到达到分片平衡，此时如果另外5 nodes加入到集群中，会发生什么呢？cluster会继续rebalance，新加入的节点发现数据集群中已经有了，首先删除本地数据，通知集群发动rebalance，平衡各个shards，这整个过程中shard会发生copy、sweap、delete等操作，耗费好多资源和时间，对一个大集群来说，耗费的更多，不可忍受。所以，elasticsearch有三个参数可以配置这些。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">gateway.recover_after_nodes: 8   // 集群中恢复的节点数，就是说当改集群启动了8个几点，才尽兴rebalance</div><div class=\"line\"></div><div class=\"line\">// 这两项说明，本集群有10的node，当10个nodes都启动或者启动了8个node且超过5分钟后就会发起rebalance</div><div class=\"line\">gateway.expected_nodes: 10</div><div class=\"line\">gateway.recover_after_time: 5m</div></pre></td></tr></table></figure>\n<p>这些策略只和<code>整个cluster重启</code>时生效。</p>\n<ul>\n<li>Prefer Unicast over Multicast<br>elasticsearch建议使用单播的方式，虽然依然提供了多播的方式，但存在找不到master等尴尬的问题，不建议使用。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;]</div></pre></td></tr></table></figure>\n<p>（4）不要轻易修改的参数</p>\n<ol>\n<li><p>Garbage Collector<br>elasticsearch中默认采用Concurrent-Mark and Sweep (CMS)的gc回收器；</p>\n</li>\n<li><p>Threadpools<br>elasticsearch中设置线程池非常合理的，如果没有特别情况下不要修改这个值</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Search gets a larger threadpool, and is configured to int((# of cores * 3) / 2) + 1</div></pre></td></tr></table></figure>\n<p>(5)Heap: Sizing and Swapping<br>(6)File Descriptors and MMap<br>elasticsearch混合使用nioFS和MMapFS。</p>\n<h3 id=\"3-插件\"><a href=\"#3-插件\" class=\"headerlink\" title=\"3. 插件\"></a>3. 插件</h3><ol>\n<li>使用<a href=\"https://github.com/mobz/elasticsearch-head\" target=\"_blank\" rel=\"external\">head</a>插件来查看索引数据</li>\n<li>使用<a href=\"https://github.com/lmenezes/elasticsearch-kopf\" target=\"_blank\" rel=\"external\">kopf</a>来备份集群节点</li>\n<li>使用<a href=\"https://github.com/lukas-vlcek/bigdesk\" target=\"_blank\" rel=\"external\">bigdesk</a>查看集群性能</li>\n<li><a href=\"https://github.com/NLPchina/elasticsearch-sql\" target=\"_blank\" rel=\"external\">elasticsearch-sql</a> 通过sql进行聚合检索, 可以将sql语句翻译成ES的JSON检索语句</li>\n<li>中文分词（ik、pinying）</li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html\" target=\"_blank\" rel=\"external\">Curator</a></li>\n</ol>\n<h2 id=\"3-参数配置\"><a href=\"#3-参数配置\" class=\"headerlink\" title=\"3. 参数配置\"></a>3. 参数配置</h2><p>暂空（后补）</p>\n<h3 id=\"java优化配置\"><a href=\"#java优化配置\" class=\"headerlink\" title=\"java优化配置\"></a>java优化配置</h3><p>(1)Heap不要超过系统可用内存的一半，并且不要超过32GB。</p>\n<p>(2) cluster集群jvm调优<br>当时我们配置ES的JVM(Xms=Xmx=8G)的垃圾回收器主要是CMS,具体配置如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"># reduce the per-thread stack size</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Xss256k&quot;</div><div class=\"line\"></div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseParNewGC&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseConcMarkSweepGC&quot;</div><div class=\"line\"></div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly&quot;</div></pre></td></tr></table></figure>\n<p>这块在官方说明中，特意强调了不建议替换java垃圾回收器，<a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/_don_8217_t_touch_these_settings.html#_garbage_collector\" target=\"_blank\" rel=\"external\">官方并不推荐使用G1</a>。</p>\n<p><a href=\"https://www.geekhub.cn/a/1256.html\" target=\"_blank\" rel=\"external\">其他博文</a>中有试过使用其他垃圾回收器。他的G1的具体配置如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseG1GC &quot;</div><div class=\"line\">#init_globals()末尾打印日志</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintFlagsFinal &quot;</div><div class=\"line\">#打印gc引用</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintReferenceGC &quot;</div><div class=\"line\">#输出虚拟机中GC的详细情况.</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -verbose:gc &quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintGCDetails &quot;</div><div class=\"line\">#Enables printing of time stamps at every GC. By default, this option is disabled.</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintGCTimeStamps &quot;</div><div class=\"line\">#Enables printing of information about adaptive generation sizing. By default, this option is disabled.</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+PrintAdaptiveSizePolicy &quot;</div><div class=\"line\"># unlocks diagnostic JVM options</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UnlockDiagnosticVMOptions &quot;</div><div class=\"line\">#to measure where the time is spent</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -XX:+G1SummarizeConcMark &quot;</div><div class=\"line\">#设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。</div><div class=\"line\">#JAVA_OPTS=&quot;$JAVA_OPTS -XX:InitiatingHeapOccupancyPercent=45 &quot;</div></pre></td></tr></table></figure>\n<p>(3) elastic 开启jmx 监控<br>有时候监控是必不可少的，所以在有条件的时候可以加上jmx监控</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">/usr/local/elastic/bin/elasticsearch.in.sh</div><div class=\"line\">JMX_PORT=9305</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.port=$JMX_PORT&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;</div><div class=\"line\">JAVA_OPTS=&quot;$JAVA_OPTS -Djava.rmi.server.hostname=xx.xx.xx..xx&quot;</div></pre></td></tr></table></figure>\n<h3 id=\"elasticsearch-yml\"><a href=\"#elasticsearch-yml\" class=\"headerlink\" title=\"elasticsearch.yml\"></a>elasticsearch.yml</h3><p>这个是最重要的配置，只有在你明白之后在修改，之后我在单独写一篇文章介绍目前elasticsearch默认参数是如何影响系统的。</p>\n<p>目前配置包括以下几个部分：<br>（1）cluster<br>（2）节点node<br>（3）log／data路径<br>（4）内存<br>（5）网络<br>（5）发现Discovery<br>（6）Gateway<br>（7）其他变量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div></pre></td><td class=\"code\"><pre><div class=\"line\"># ======================== Elasticsearch Configuration =========================</div><div class=\"line\">#</div><div class=\"line\"># NOTE: Elasticsearch comes with reasonable defaults for most settings.</div><div class=\"line\">#       Before you set out to tweak and tune the configuration, make sure you</div><div class=\"line\">#       understand what are you trying to accomplish and the consequences.</div><div class=\"line\">#</div><div class=\"line\"># The primary way of configuring a node is via this file. This template lists</div><div class=\"line\"># the most important settings you may want to configure for a production cluster.</div><div class=\"line\">#</div><div class=\"line\"># Please see the documentation for further information on configuration options:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/settings.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Cluster -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Use a descriptive name for your cluster:</div><div class=\"line\">#</div><div class=\"line\">cluster.name: elastic-pro</div><div class=\"line\">#</div><div class=\"line\"># ------------------------------------ Node ------------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Use a descriptive name for the node:</div><div class=\"line\">#</div><div class=\"line\">node.name: node-0</div><div class=\"line\">#</div><div class=\"line\"># Add custom attributes to the node:</div><div class=\"line\">#</div><div class=\"line\">node.attr.rack: r1</div><div class=\"line\">#</div><div class=\"line\"># ----------------------------------- Paths ------------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Path to directory where to store the data (separate multiple locations by comma):</div><div class=\"line\">#</div><div class=\"line\">path.data: /apps/home/worker/zhangxiaolong/data/index0</div><div class=\"line\">#</div><div class=\"line\"># Path to log files:</div><div class=\"line\">#</div><div class=\"line\">path.logs: /apps/home/worker/zhangxiaolong/data/log0</div><div class=\"line\">#</div><div class=\"line\"># ----------------------------------- Memory -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Lock the memory on startup:</div><div class=\"line\">#</div><div class=\"line\">bootstrap.memory_lock: true</div><div class=\"line\">#</div><div class=\"line\"># Make sure that the heap size is set to about half the memory available</div><div class=\"line\"># on the system and that the owner of the process is allowed to use this</div><div class=\"line\"># limit.</div><div class=\"line\">#</div><div class=\"line\"># Elasticsearch performs poorly when the system is swapping the memory.</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Network -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Set the bind address to a specific IP (IPv4 or IPv6):</div><div class=\"line\">#</div><div class=\"line\">network.host: 172.16.7.1</div><div class=\"line\">#</div><div class=\"line\"># Set a custom port for HTTP:</div><div class=\"line\">#</div><div class=\"line\">http.port: 9201</div><div class=\"line\">#</div><div class=\"line\"># For more information, see the documentation at:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-network.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># --------------------------------- Discovery ----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Pass an initial list of hosts to perform discovery when new node is started:</div><div class=\"line\">#The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]</div><div class=\"line\">#</div><div class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;172.16.7.1:9300&quot;]</div><div class=\"line\">#</div><div class=\"line\"># Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of nodes / 2 + 1):</div><div class=\"line\">#</div><div class=\"line\">discovery.zen.minimum_master_nodes: 2</div><div class=\"line\">#</div><div class=\"line\"># For more information, see the documentation at:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-discovery-zen.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Gateway -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Block initial recovery after a full cluster restart until N nodes are started:</div><div class=\"line\">#</div><div class=\"line\">gateway.recover_after_nodes: 2</div><div class=\"line\">#</div><div class=\"line\"># For more information, see the documentation at:</div><div class=\"line\"># &lt;https://www.elastic.co/guide/en/elasticsearch/reference/5.0/modules-gateway.html&gt;</div><div class=\"line\">#</div><div class=\"line\"># ---------------------------------- Various -----------------------------------</div><div class=\"line\">#</div><div class=\"line\"># Require explicit names when deleting indices:</div><div class=\"line\">#</div><div class=\"line\">#action.destructive_requires_name: true</div></pre></td></tr></table></figure>\n<h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><p>（1）线程池设置成内核数，比如八核机器就设置成8，很多阻塞的操作都是Lucene来操作的，比如硬盘读写。搜索的线程设置可以设置成内核数的三倍<br>（2）内存交换<br>这个对于性能影响是致命的，可以使用命令sudo swapoff -a来暂时关闭，永久关闭需要编辑文件/etc/fstab，也可以在配置文件中添加配置bootstrap.mlockall: true，这样jvm可以锁定这些内存，避免被交换到物理存储介质<br>（3）其他</p>\n<ul>\n<li>如果你不需要近实时功能，则设置index的刷新时间；</li>\n<li>如果在进行一个大bulk导入，可以优先考虑设置副本数为0；</li>\n<li>如果在index中的doc你没有一个自然增长的id，可以使用Elasticsearch’s 的自动id做标示，如果有自己的id，尽量设计对lucene友好的id；</li>\n</ul>\n<h2 id=\"4-Rolling-Restarts-amp-备份数据-amp-备份恢复\"><a href=\"#4-Rolling-Restarts-amp-备份数据-amp-备份恢复\" class=\"headerlink\" title=\"4. Rolling Restarts &amp; 备份数据 &amp; 备份恢复\"></a>4. Rolling Restarts &amp; 备份数据 &amp; 备份恢复</h2><h3 id=\"Rolling-Restarts\"><a href=\"#Rolling-Restarts\" class=\"headerlink\" title=\"Rolling Restarts\"></a>Rolling Restarts</h3><p>一般下线一个node（升级、维修等），elasticsearch会进行rebalance操作，如果你是真正的下线一个node，这个操作是十分正确的，但是你知道这台node会之后重新加入到cluster中，则rebalance操作不恰当了，当shard比较大或者多的时候会严重耗费系统资源。</p>\n<p>那我们正确的操作是什么？</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. 查看集群设置</div><div class=\"line\">curl -XGET http://10.10.160.129:9200/_cluster/settings</div><div class=\"line\"></div><div class=\"line\">2. 如果可能的话，停止正在索引的数据；</div><div class=\"line\"></div><div class=\"line\">3. 停止分片同步，阻止elasticsearch进行rebalance操作</div><div class=\"line\">PUT /_cluster/settings</div><div class=\"line\">&#123;</div><div class=\"line\">    \"transient\" : &#123;</div><div class=\"line\">        \"cluster.routing.allocation.enable\" : \"none\"</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">4.关闭单个node</div><div class=\"line\"></div><div class=\"line\">5.维护或者升级节点node</div><div class=\"line\"></div><div class=\"line\">6.重启node，确认加入cluster</div><div class=\"line\"></div><div class=\"line\">7.重新打开分片同步</div><div class=\"line\">PUT /_cluster/settings</div><div class=\"line\">&#123;</div><div class=\"line\">    \"transient\" : &#123;</div><div class=\"line\">        \"cluster.routing.allocation.enable\" : \"all\"</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">8.针对需要的node重复执行3-7操作</div><div class=\"line\"></div><div class=\"line\">9.到这里就重新恢复了cluster；</div></pre></td></tr></table></figure>\n<h3 id=\"备份数据\"><a href=\"#备份数据\" class=\"headerlink\" title=\"备份数据\"></a>备份数据</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. 先导入一些数据进行备份</div><div class=\"line\">curl -XPOST 'http://192.168.56.11:9200/bank/account/_bulk?pretty' --data-binary @accounts.json</div><div class=\"line\">curl -XPOST 'http://192.168.56.11:9200/shakespeare/_bulk?pretty' --data-binary @shakespeare.json</div><div class=\"line\">curl -XPOST 'http://192.168.56.11:9200/_bulk?pretty' --data-binary @logs.jsonl</div><div class=\"line\"></div><div class=\"line\">2. 使用API创建一个镜像仓库</div><div class=\"line\">curl -XPOST http://192.168.56.11:9200/_snapshot/my_backup -d '</div><div class=\"line\">&#123;</div><div class=\"line\">    \"type\": \"fs\", </div><div class=\"line\">    \"settings\": &#123; </div><div class=\"line\">        \"location\": \"/data/mount\"</div><div class=\"line\">        \"compress\":  true </div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;'</div><div class=\"line\"><span class=\"meta\">#</span># 解释：</div><div class=\"line\">镜像仓库的名称：my_backup</div><div class=\"line\">镜像仓库的类型：fs。还支持curl，hdfs等。</div><div class=\"line\">镜像仓库的位置：/data/mount 。这个位置必须在配置文件中定义。</div><div class=\"line\">是否启用压缩：compres：true 表示启用压缩。</div><div class=\"line\"></div><div class=\"line\">3. 备份前检查配置</div><div class=\"line\">必须确定备份使用的目录在配置文件中声明了，否则会爆如下错误</div><div class=\"line\">&#123;</div><div class=\"line\">  \"error\": &#123;</div><div class=\"line\">    \"root_cause\": [</div><div class=\"line\">      &#123;</div><div class=\"line\">        \"type\": \"repository_exception\",</div><div class=\"line\">        \"reason\": \"[test-bakcup] failed to create repository\"</div><div class=\"line\">      &#125;</div><div class=\"line\">    ],</div><div class=\"line\">    \"type\": \"repository_exception\",</div><div class=\"line\">    \"reason\": \"[test-bakcup] failed to create repository\",</div><div class=\"line\">    \"caused_by\": &#123;</div><div class=\"line\">      \"type\": \"creation_exception\",</div><div class=\"line\">      \"reason\": \"Guice creation errors:\\n\\n1) Error injecting constructor, RepositoryException[[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty]\\n  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)\\n  while locating org.elasticsearch.repositories.fs.FsRepository\\n  while locating org.elasticsearch.repositories.Repository\\n\\n1 error\",</div><div class=\"line\">      \"caused_by\": &#123;</div><div class=\"line\">        \"type\": \"repository_exception\",</div><div class=\"line\">        \"reason\": \"[test-bakcup] location [/data/mount] doesn't match any of the locations specified by path.repo because this setting is empty\"</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;,</div><div class=\"line\">  \"status\": 500</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">4. 开始创建一个快照</div><div class=\"line\"><span class=\"meta\">#</span>#在后头创建一个快照</div><div class=\"line\">curl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_1 </div><div class=\"line\"><span class=\"meta\">#</span>#也可以在前台运行。</div><div class=\"line\">curl -XPUT  http://192.168.56.11:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true</div><div class=\"line\"><span class=\"meta\">#</span>#上面的参数会在my_backup仓库里创建一个snapshot_1 的快照。</div><div class=\"line\"></div><div class=\"line\">5. 可以选择相应的索引进行备份</div><div class=\"line\">curl -XPUT  http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2 -d '</div><div class=\"line\">&#123;</div><div class=\"line\">    \"indices\": \"bank,logstash-2015.05.18\"</div><div class=\"line\">&#125;'</div><div class=\"line\"><span class=\"meta\">#</span># 解释：</div><div class=\"line\">创建一个snapshot_2的快照，只备份bank,logstash-2015.05.18这两个索引。</div><div class=\"line\"></div><div class=\"line\">6. 查看备份状态</div><div class=\"line\">整个备份过程中，可以通过如下命令查看备份进度</div><div class=\"line\"></div><div class=\"line\">curl -XGET http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_status</div><div class=\"line\">主要由如下几种状态：</div><div class=\"line\">a. INITIALIZING 集群状态检查，检查当前集群是否可以做快照，通常这个过程会非常快</div><div class=\"line\">b. STARTED 正在转移数据到仓库</div><div class=\"line\">c. FINALIZING 数据转移完成，正在转移元信息</div><div class=\"line\">d. DONE　完成</div><div class=\"line\">e. FAILED 备份失败</div><div class=\"line\"></div><div class=\"line\">7. 取消备份</div><div class=\"line\">curl -XDELETE http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812</div><div class=\"line\"></div><div class=\"line\">8. 获取所有快照信息。</div><div class=\"line\">curl -XGET http://192.168.56.20:9200/_snapshot/my_backup/_all |python -mjson.tool</div><div class=\"line\"><span class=\"meta\">#</span>#解释</div><div class=\"line\">查看my_backup仓库下的所有快照。</div><div class=\"line\"></div><div class=\"line\">9. 手动删除快照</div><div class=\"line\">curl -XDELETE http://192.168.56.20:9200/_snapshot/my_backup/snapshot_2</div><div class=\"line\"><span class=\"meta\">#</span># 解释</div><div class=\"line\">删除my_backup仓库下的snapshot_2的快照。</div></pre></td></tr></table></figure>\n<h3 id=\"备份恢复\"><a href=\"#备份恢复\" class=\"headerlink\" title=\"备份恢复\"></a>备份恢复</h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div></pre></td><td class=\"code\"><pre><div class=\"line\">1. 恢复备份</div><div class=\"line\">curl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore</div><div class=\"line\">同备份一样，也可以设置wait_for_completion=true等待恢复结果</div><div class=\"line\"></div><div class=\"line\">curl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore?wait_for_completion=true</div><div class=\"line\">默认情况下，是恢复所有的索引，我们也可以设置一些参数来指定恢复的索引，以及重命令恢复的索引，这样可以避免覆盖原有的数据.</div><div class=\"line\"></div><div class=\"line\">curl -XPOST http://192.168.0.1:9200/_snapshot/my_backup/snapshot_20150812/_restore</div><div class=\"line\">&#123;</div><div class=\"line\">    <span class=\"attr\">\"indices\"</span>: <span class=\"string\">\"index_1\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"rename_pattern\"</span>: <span class=\"string\">\"index_(.+)\"</span>,</div><div class=\"line\">    <span class=\"attr\">\"rename_replacement\"</span>: <span class=\"string\">\"restored_index_$1\"</span></div><div class=\"line\">&#125;</div><div class=\"line\">上面的indices, 表示只恢复索引’index_1’</div><div class=\"line\">rename_pattern: 表示重命名索引以’index_’开头的索引.</div><div class=\"line\">rename_replacement: 表示将所有的索引重命名为’restored_index_xxx’.如index_1会被重命名为restored_index_1.</div><div class=\"line\"></div><div class=\"line\">2. 查看所有索引的恢复进度</div><div class=\"line\">curl -XGET http://192.168.0.1:9200/_recovery/</div><div class=\"line\"></div><div class=\"line\">3. 查看索引restored_index_1的恢复进度</div><div class=\"line\">curl -XGET http://192.168.0.1:9200/_recovery/restored_index_1</div><div class=\"line\"></div><div class=\"line\">4. 取消恢复</div><div class=\"line\">只需要删除索引，即可取消恢复</div><div class=\"line\">curl -XDELETE http://192.168.0.1:9200/restored_index_1</div></pre></td></tr></table></figure>\n<h2 id=\"5-性能优化\"><a href=\"#5-性能优化\" class=\"headerlink\" title=\"5. 性能优化\"></a>5. 性能优化</h2><p>在讲性能优化之前，首先要知道：</p>\n<pre><code>过早的优化是万恶之源 Premature optimization is the root of all evil.\n                                                    —— Donald Knuth\n</code></pre><p><code>优化总是发生在目前的情况下不能满足当前的需求，其他我想不出什么理由去优化它。</code></p>\n<p><code>优化很多时候和业务是紧密关联的，优化业务可能比优化程序效率更高、成本更低！</code></p>\n<h3 id=\"索引性能优化\"><a href=\"#索引性能优化\" class=\"headerlink\" title=\"索引性能优化\"></a>索引性能优化</h3><p>索引性能（Index Performance），我们这样定义它，索引的速度是否提高，可以无缝的提供近实时的功能。<br>什么时候会发生索引慢呢？<br>（1）你读的慢（doc from db，file，inputstream等等）<br>（2）你处理的慢（中文下的分词等）<br>（3）你写的慢（还是老式的机械盘？！ 高性能的盘或者ssd）</p>\n<p>还有就是针对不同场景选择的判断，如果你索引的文件非常大，数量多，那应该选择elasticsearch提供的bulk接口，在create doc速度能跟上的时候，bulk 是可以提高速度的。</p>\n<h3 id=\"查询性能优化\"><a href=\"#查询性能优化\" class=\"headerlink\" title=\"查询性能优化\"></a>查询性能优化</h3><p>查询性能（Query Perofrmance），说起来比索引更麻烦一些，面对的场景也更多一些；</p>\n<p>面对海量数据以及不同的集群，针对业务需求去查询往往会很慢，有什么策略可以搞定这种情况？有，那就是<code>routing</code><a href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/routing-value.html\" target=\"_blank\" rel=\"external\">Routing a Document to a Shard</a>、<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/5.1/mapping-routing-field.html\" target=\"_blank\" rel=\"external\">_routing field</a>.</p>\n<p>查询策略，分别查询vs合并查询？索引越来越大，单个 shard 也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的 shards 会带来额外的索引压力，即 IO 压力。我们选择了分索引。比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。</p>\n<p>索引越来越大，资源使用也越来越多。若是要进行更细的集群分配，大索引使用的资源成倍增加。有什么办法能减小索引？<br>根据具体业务需求，减少某些大的索引，这是一个很好的办法，这样这个集群各方面占用的资源会有一定程度的下降，当让你要说这些少的索引怎么办，这些索引可以放在单独的集群中。</p>\n<h2 id=\"应用性能优化-from-youzan\"><a href=\"#应用性能优化-from-youzan\" class=\"headerlink\" title=\"应用性能优化 - from youzan\"></a>应用性能优化 - <a href=\"http://tech.youzan.com/search-engine1/\" target=\"_blank\" rel=\"external\">from youzan</a></h2><p>一、使用应用级队列防止雪崩<br>ES一个问题是在高峰期时候极容易发生雪崩. ES有健全的线程池系统来保证并发与稳定性问题. 但是在流量突变的情况下(比如双十一秒杀)还是很容易发生瘫痪的现象, 主要的原因如下:</p>\n<p>ES几乎为每类操作配置一个线程池; 只能保证每个线程池的资源使用时合理的, 当2个以上的线程池竞争资源时容易造成资源响应不过来.</p>\n<p>ES没有考虑网络负载导致稳定的问题.</p>\n<p>在AS里我们实现了面向请求的全局队列来保证稳定性. 它主要做了3件事情.<br><img src=\"http://image.zhangxiaolong.org/mweb/14979525149331.png!medium\" alt=\"\"></p>\n<ol>\n<li>根据业务把请求分成一个个slide, 每个slide对应一个队列. 默认一个应用就是一个slide, 一个应用也可以区分不同的slide, 这样可以保护一个应用内重要的查询.</li>\n<li>每个队列配置一个队列长度, 默认为50.</li>\n<li>每个队列计算这个队列的平均响应时间. 当队列平均响应时间超过200ms, 停止工作1s, 如果请求溢出就写入溢出日志留数据恢复使用. 如果连续10次队列平均响应时间超过500ms就报警, 以便工程师第一时间处理.</li>\n</ol>\n<p>二、自动降级<br>应用级队列解决雪崩问题有点粗暴, 如果一个应用本身查询就非常慢, 很容易让一个应用持续超时很久. 我们根据搜索引擎的特点编写了自动降级功能.</p>\n<p>比如商品搜索的例子, 商品搜索最基本的功能是布尔查询, 但是还需要按照相关性分数和质量度排序等功能, 甚至还有个性化需求. 完成简单的布尔查询, ES使用bitsets操作就可以做到, 但是如果如果需要相关性分, 就必须使用倒排索引, 并有大量CPU消耗来计算分数. ES的bitsets比倒排索引快50倍左右.</p>\n<p>对于有降级方案的slide, AS在队列响应过慢时候直接使用降级query代替正常query. 这种方法让我们在不扩容的情况下成功度过了双十一的流量陡增.</p>\n<p>三、善用filtered query<br>理解lucence filter工作原理对于写出高性能查询语句至关重要. 许多搜索性能优化都和filter的使用有关. filter使用bitsets进行布尔运算, quey使用倒排索引进行计算, 这是filter比query快的原因. bitsets的优势主要体现在: </p>\n<ol>\n<li>bitsetcache在内存里面, 永不消失(除非被LRU). </li>\n<li>bitsets利用CPU原生支持的位运算操作, 比倒排索引快个数量级 </li>\n<li>多个bitsets的与运算也是非常的快(一个64位CPU可以同时计算64个DOC的与运算) </li>\n<li>bitsets 在内存的存储是独立于query的, 有很强的复用性 </li>\n<li>如果一个bitset片段全是0, 计算会自动跳过这些片段, 让bitsets在数据稀疏情况下同样表现优于倒排索引.</li>\n</ol>\n<p>举个例子:<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">query:bool:  </div><div class=\"line\">    tag:<span class=\"string\">'mac'</span></div><div class=\"line\">    region:<span class=\"string\">'beijing'</span></div><div class=\"line\">    title: <span class=\"string\">\"apple\"</span></div></pre></td></tr></table></figure></p>\n<p>lucence处理这个query的方式是在倒排索引中寻找这三个term的倒排链 ,并使用跳指针技术求交, 在运算过程中需要对每个doc进行算分. 实际上tag和region对于算分并没有作用, 他们充当是过滤器的作用.</p>\n<p>这就是过滤器使用场景, 它只存储存在和不存在两种状态. 如果我们把tag和region使用bitsets进行存储, 这样这两个过滤器可以一直都被缓存在内存里面, 这样会快很多. 另外tag和region之间的求交非常迅速, 因为64位机器可以时间一个CPU周期同时处理64个doc的位运算.</p>\n<p>一个lucence金科玉律是: 能用filter就用filter, 除非必须使用query(当且仅当你需要算分的时候).<br>正确的写法为:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">query:  </div><div class=\"line\">    filtered: </div><div class=\"line\">        query:  </div><div class=\"line\">             title: <span class=\"string\">\"apple\"</span> </div><div class=\"line\">         filter:</div><div class=\"line\">            tag:<span class=\"string\">\"mac\"</span></div><div class=\"line\">             region:<span class=\"string\">\"beijing\"</span></div></pre></td></tr></table></figure>\n<p>四、其他</p>\n<ol>\n<li><p>线上集群关闭分片自动均衡. 分片的自动均衡主要目的防止更新造成各个分片数据分布不均匀. 但是如果线上一个节点挂掉后, 很容易触发自动均衡, 一时间集群内部的数据移动占用所有带宽. 建议采用闲时定时均衡策略来保证数据的均匀.</p>\n</li>\n<li><p>尽可能延长refresh时间间隔. 为了确保实时索引es索引刷新时间间隔默认为1秒, 索引刷新会导致查询性能受影响, 在确保业务时效性保证的基础上可以适当延长refresh时间间隔保证查询的性能.</p>\n</li>\n<li><p>除非有必要把all字段去掉. 索引默认除了索引每个字段外, 还有额外创建一个all的字段, 保存所有文本, 去掉这个字段可以把索引大小降低50%.</p>\n</li>\n<li><p>创建索引时候, 尽可能把查询比较慢的索引和快的索引物理分离.</p>\n</li>\n</ol>\n<p>##6. 参考（Reference）</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/guguli/p/5218297.html\" target=\"_blank\" rel=\"external\">elastic调优参考</a></li>\n<li><a href=\"https://github.com/Wprosdocimo/Elasticsearch-zabbix\" target=\"_blank\" rel=\"external\">elastic监控</a></li>\n<li><a href=\"http://udn.yyuap.com/doc/mastering-elasticsearch/chapter-4/41_README.html\" target=\"_blank\" rel=\"external\">Mastering Elasticsearch(中文版)</a></li>\n<li><a href=\"http://kibana.logstash.es/content/logstash/plugins/input/file.html\" target=\"_blank\" rel=\"external\">ELK-权威指南</a></li>\n<li><a href=\"http://www.learnes.net/index.html\" target=\"_blank\" rel=\"external\">Elasticsearch 权威指南</a></li>\n<li><a href=\"http://www.biglittleant.cn/2016/12/01/elastic-study1/\" target=\"_blank\" rel=\"external\">elasticsearch 生产环境配置</a></li>\n<li><a href=\"http://tech.youzan.com/search-engine1/\" target=\"_blank\" rel=\"external\">有赞搜索引擎实践(工程篇)</a></li>\n</ol>\n<p><strong>[更新于2017-05-22 - v1.0 ]<br>[更新于2017-09-26 - v1.01]</strong></p>\n<p>(完)</p>\n"},{"title":"自己动手实现推荐系统：一.常用推荐算法优势分析","date":"2017-09-28T09:15:17.000Z","english_title":"resys_common_algorithm_analyse","toc":true,"_content":"\n本文包含两部分，一是常见算法类（部分文字摘自《推荐系统实践-项亮》），二是分析目前各个大公司放出来的推荐系统机构。\n## 1、算法分析\n* 简介:通过在用户的一系列行为中寻找特定模式来产生用户特殊 推荐。\n\n\t算法的核心思想就是:如果 两个用户对于一些项的评分相似程度较高,那么一个用户对于一个新项的 评分很有可能类似于另一个用户。\r* 输入:仅仅依赖于惯用数据(例如评价、购买、下载等用户偏好 行为)。\r* 类型:\n\t* 基于邻域的协同过滤(基于用户和基于项);\n\t\t* 邻域方法(即基于内存的 CF)是使用用户 对已有项的评分直接预测该用户对新项的评分。基于邻域的 CF 方法意在找出项与项之间的联系(基于项的 CF),或 者用户与用户之间的联系(基于用户的 CF)。\n\t\t\t* 基于用户的 CF 通过找出对项的偏好与你相似的用户从而基于他们对于新项的喜好来为你进行推荐。\n\t\t\t* 基于项的 CF 会向用户推荐与用户喜欢的项相似的项,这种相似是基于项的共同出现几率(例如用户买了 X,通时也买了 Y)。\n\t* 基于模型的协同过滤(矩阵因子分解、受限玻尔兹曼机、贝叶斯网络等)。 \n\t\t* 基于模型的方 法是使用历史评分数据,基于学习出的预测模型,预测对新项的评分。基于模型的方法会在 使用评分去学习预测模型的基础上,去预测新项，一般的想法是使用机器 学习算法建立用户和项的相互作用模型,从而找出数据中的模式。贝叶斯网络、聚类、分类、回归、矩阵分解、受限玻尔兹曼机等等\n* 优点:\r\t* 需要最小域;\r\t* 不需要用户和项;\r\t* 大部分场景中能够产生足够好的结果。\r* 缺点:\r\t* 冷启动问题;\r\t* 需要标准化产品;\r\t* 需要很高的用户和项的比例(1:10);\r\t* 流行度偏见(有长尾的时候表现不够好); \n\t* 难于提供解释。\n\n## 2、基于内容的推荐算法\n* 简介:向用户推荐和其过去喜欢项的内容(例如元数据、描述、话题等等)相似的项。\n\n\t在基于内容的推荐中,假设可以获取到 item 的描述信息, 并将其作为 item 的特征向量(例如标题、年份、描述)。这些特征向量 被用于创建一个反映用户偏好的模型。各种信息检索(例如 TF-IDF)和 机器学习技术(例如朴素贝叶斯、支持向量机、决策树等)可被用于创建 用户模型,从而为用户产生推荐。\n\t\n* 输入:仅仅依赖于项和用户的内容/描述(除了惯用数据)。\n* 类型:\n\t* 信息检索(例如 tf-idf 和 Okapi BM25);\n\t* 机器学习(例如朴素贝叶斯、支持向量机、决策树等)。\n* 优点:\n\t* 没有冷启动问题;\n\t* 不需要惯用数据;\n\t* 没有流行度偏见,可以推荐有罕见特性的项; ο 可以使用用户内容特性来提供解释。\n* 缺点:\n\t* 项内容必须是机器可读的和有意义的; ο 容易归档用户;\n\t* 很难有意外,缺少多样性;\n\t* 很难联合多个项的特性。\n\n## 3、混合推荐算法\n* 简介:综合利用协同过滤推荐算法和基于内容的推荐算法各自的 优点同时抵消各自的缺点。\n* 输入:同时使用用户和项的内容特性与惯用数据,同时从两种输 入类型中获益。\n* 类型:\n\t* 加权;\n\t* 交换;\n\t* 混合;\n\t* 特性组合;\n\t* 案列\n\t* 特征增强\n\t* 元层次\n\t* ![](http://image.zhangxiaolong.org/mweb/14719522675930.jpg!medium)\n\n* 优点:\n\t* 由于单独使用协同过滤推荐算法和基于内容的推荐算法;\n\t* 没有冷启动问题;\n\t* 没有流行度偏见,可推荐有罕见特性的项;\n\t* 可产生意外,实现多样性。\n* 缺点:\n\t* 需要通过大量的工作才能得到正确的平衡。\n\n## 4、流行度推荐算法\n* 简介:这是一种推荐流行项的方法(例如最多下载、最多看过、最 大影响的项)。\n* 输入:使用惯用数据和项的内容(例如类目)。 \n* 优点:\n\t* 相对容易实现;\n\t* 良好的基准算法;\n\t* 有助于解决新用户冷启动问题。\n* 缺点:\n\t* 需要标准化产品;\n\t* 经常需要一些项的类型进行分类;\n\t* 不会推荐新项(很少有机会被观测到); \n\t* 推荐列表不会改变太大。\n\n## 5、高级非传统推荐算法\n* 类型\n\t* 深度学习\n\t* 学习等级\n\t* multi-armed bandits（探索／开发）\n\t* 上下文感知推荐\n\t* 张量分解\n\t* 分解机\n\t* 社会推荐\n* 优点：\n\t* 利于勉强维持最终性能百分点;\n\t* 你可以说你正在使用渐进的方式。\r* 缺点：\r\t* 难于理解;\n\t* 缺乏推荐工具支持\n\t* 没有为你的首个推荐系统提供推荐的方式。\n\n## 二、推荐系统结构实例\n### netfix\n首先是大名鼎鼎的netfix\n![](http://image.zhangxiaolong.org/mweb/a.png!origin)\n\n### 新浪微博\n国内我们看一下新浪\n![](http://image.zhangxiaolong.org/mweb/14720256571323.jpg!origin)\n\n### 基于内容的推荐\n基于内容和用户画像的个性化推荐属于基于内容的推荐。对于此种推荐，有两个实体：内容和用户，因此需要有一个联系这两者的东西，即为标签。内容转换为标签即为内容特征化，用户则称为用户特征化。对于此种推荐，主要分为以下几个关键部分：\n\n1. 标签库\n2. 内容特征化\n3. 用户特征化\n4. 隐语义推荐\n\n综合上面讲述的各个部分即可实现一个基于内容和用户画像的个性化推荐系统，\n![](http://image.zhangxiaolong.org/mweb/14728202025271.png!medium)\n\n（1）标签库\n标签是联系用户与物品、内容以及物品、内容之间的纽带，也是反应用户兴趣的重要数据源。标签库的最终用途在于对用户进行行为、属性标记。是将其他实体转换为计算机可以理解的语言关键的一步。\n\n标签库则是对标签进行聚合的系统，包括对标签的管理、更新等。\n\n一般来说，标签是以层级的形式组织的。可以有一级维度、二级维度等。\n标签的来源主要有：\n\n1. 已有内容的标签\n2. 网络抓取流行标签\n3. 对运营的内容进行关键词提取\n\n对于内容的关键词提取，使用结巴分词 + TFIDF即可。此外，也可以使用[TextRank](http://www.tuicool.com/articles/UZ77Z3)来提取内容关键词。\n\n这里需要注意的一点是对于**关联标签**的处理，比如用户的标签是足球，而内容的标签是德甲、英超，那么用户和内容是无法联系在一起的。最简单的方式是人工设置关联标签，此外也可以使用**word2vec**一类工具对标签做**聚类处理**，构建主题模型，将德甲、英超聚类到足球下面。\n（2） 内容特征化\n内容特征化即给内容打标签，目前有两种方式：人工打标签和机器自动打标签\n针对机器自动打标签，需要采取机器学习的相关算法来实现，即针对一系列给定的标签，给内容选取其中匹配度最高的几个标签。这不同于通常的分类和聚类算法。可以采取使用分词 + Word2Vec来实现，过程如下：\n\n1. 将文本语料进行分词，以空格,tab隔开都可以，使用结巴分词。\n2. 使用word2vec训练词的相似度模型。\n3. 使用tfidf提取内容的关键词A,B,C。\n4. 遍历每一个标签，计算关键词与此标签的相似度之和。\n5. 取出TopN相似度最高的标签即为此内容的标签。\n\n此外，可以使用文本主题挖掘相关技术，对内容进行特征化。这也分为两种情况:\n\n1. 通用情况下，只是为了效果优化的特征提取，那么可以使用非监督学习的主题模型算法。如LSA、PLSI和GaP模型或者LDA模型。\n2. 在和业务强相关时，需要在业务特定的标签体系下给内容打上适合的标签。这时候需要使用的是监督学习的主题模型。如sLDA、HSLDA等。\n\n（3） 用户特征化\n用户特征化即为用户打标签。通过用户的行为日志和一定的模型算法得到用户的每个标签的权重(泊松分布求解每个行为的权重；有监督的主题模型如SLDA；无监督的主题模型LSA)。\n\n1. 用户对内容的行为：点赞、不感兴趣、点击、浏览。对用户的反馈行为如点赞赋予权值1，不感兴趣赋予-1；对于用户的浏览行为，则可使用点击/浏览作为权值。\n2. 对内容发生的行为可以认为对此内容所带的标签的行为。\n3. 用户的兴趣是时间衰减的，即离当前时间越远的兴趣比重越低。时间衰减函数使用1/[log(t)+1], t为事件发生的时间距离当前时间的大小。\n4. 要考虑到热门内容会干预用户的标签，需要对热门内容进行降权。使用click/pv作为用户浏览行为权值即可达到此目的。\n5. 此外，还需要考虑噪声的干扰，如标题党等。\n\n另外，非业务强相关的情况下，还可以考虑使用LSA主题模型等矩阵分解的方式对用户进行标签化。\n\n（4） 隐语义推荐\n有了内容特征和用户特征，可以使用[隐语义模型](http://blog.csdn.net/harryhuang1990/article/details/9924377)进行推荐。这里可以使用其简化形式，以达到实时计算的目的。\n\n用户对于某一个内容的兴趣度(可以认为是CTR)：\n![](http://image.zhangxiaolong.org/mweb/14728206482938.jpg!origin)\n\n其中i=1…N是内容c具有的标签，m(ci)指的内容c和标签i的关联度(可以简单认为是1),n(ui)指的是用户u的标签i的权重值,当用户不具有此标签时n(ui)=0，q©指的是内容c的质量，可以使用点击率(click/pv)表示。\n\n（完）\n\n**- 写作于2017年9月28号，只是简稿**\n\n\n","source":"_posts/自己动手打造推荐系统-一-常用推荐算法优势分析.md","raw":"---\ntitle: '自己动手实现推荐系统：一.常用推荐算法优势分析'\ndate: 2017-09-28 17:15:17\ntags:\n  - resys\ncategories:\n  - resys\nenglish_title: resys_common_algorithm_analyse\ntoc: true\n---\n\n本文包含两部分，一是常见算法类（部分文字摘自《推荐系统实践-项亮》），二是分析目前各个大公司放出来的推荐系统机构。\n## 1、算法分析\n* 简介:通过在用户的一系列行为中寻找特定模式来产生用户特殊 推荐。\n\n\t算法的核心思想就是:如果 两个用户对于一些项的评分相似程度较高,那么一个用户对于一个新项的 评分很有可能类似于另一个用户。\r* 输入:仅仅依赖于惯用数据(例如评价、购买、下载等用户偏好 行为)。\r* 类型:\n\t* 基于邻域的协同过滤(基于用户和基于项);\n\t\t* 邻域方法(即基于内存的 CF)是使用用户 对已有项的评分直接预测该用户对新项的评分。基于邻域的 CF 方法意在找出项与项之间的联系(基于项的 CF),或 者用户与用户之间的联系(基于用户的 CF)。\n\t\t\t* 基于用户的 CF 通过找出对项的偏好与你相似的用户从而基于他们对于新项的喜好来为你进行推荐。\n\t\t\t* 基于项的 CF 会向用户推荐与用户喜欢的项相似的项,这种相似是基于项的共同出现几率(例如用户买了 X,通时也买了 Y)。\n\t* 基于模型的协同过滤(矩阵因子分解、受限玻尔兹曼机、贝叶斯网络等)。 \n\t\t* 基于模型的方 法是使用历史评分数据,基于学习出的预测模型,预测对新项的评分。基于模型的方法会在 使用评分去学习预测模型的基础上,去预测新项，一般的想法是使用机器 学习算法建立用户和项的相互作用模型,从而找出数据中的模式。贝叶斯网络、聚类、分类、回归、矩阵分解、受限玻尔兹曼机等等\n* 优点:\r\t* 需要最小域;\r\t* 不需要用户和项;\r\t* 大部分场景中能够产生足够好的结果。\r* 缺点:\r\t* 冷启动问题;\r\t* 需要标准化产品;\r\t* 需要很高的用户和项的比例(1:10);\r\t* 流行度偏见(有长尾的时候表现不够好); \n\t* 难于提供解释。\n\n## 2、基于内容的推荐算法\n* 简介:向用户推荐和其过去喜欢项的内容(例如元数据、描述、话题等等)相似的项。\n\n\t在基于内容的推荐中,假设可以获取到 item 的描述信息, 并将其作为 item 的特征向量(例如标题、年份、描述)。这些特征向量 被用于创建一个反映用户偏好的模型。各种信息检索(例如 TF-IDF)和 机器学习技术(例如朴素贝叶斯、支持向量机、决策树等)可被用于创建 用户模型,从而为用户产生推荐。\n\t\n* 输入:仅仅依赖于项和用户的内容/描述(除了惯用数据)。\n* 类型:\n\t* 信息检索(例如 tf-idf 和 Okapi BM25);\n\t* 机器学习(例如朴素贝叶斯、支持向量机、决策树等)。\n* 优点:\n\t* 没有冷启动问题;\n\t* 不需要惯用数据;\n\t* 没有流行度偏见,可以推荐有罕见特性的项; ο 可以使用用户内容特性来提供解释。\n* 缺点:\n\t* 项内容必须是机器可读的和有意义的; ο 容易归档用户;\n\t* 很难有意外,缺少多样性;\n\t* 很难联合多个项的特性。\n\n## 3、混合推荐算法\n* 简介:综合利用协同过滤推荐算法和基于内容的推荐算法各自的 优点同时抵消各自的缺点。\n* 输入:同时使用用户和项的内容特性与惯用数据,同时从两种输 入类型中获益。\n* 类型:\n\t* 加权;\n\t* 交换;\n\t* 混合;\n\t* 特性组合;\n\t* 案列\n\t* 特征增强\n\t* 元层次\n\t* ![](http://image.zhangxiaolong.org/mweb/14719522675930.jpg!medium)\n\n* 优点:\n\t* 由于单独使用协同过滤推荐算法和基于内容的推荐算法;\n\t* 没有冷启动问题;\n\t* 没有流行度偏见,可推荐有罕见特性的项;\n\t* 可产生意外,实现多样性。\n* 缺点:\n\t* 需要通过大量的工作才能得到正确的平衡。\n\n## 4、流行度推荐算法\n* 简介:这是一种推荐流行项的方法(例如最多下载、最多看过、最 大影响的项)。\n* 输入:使用惯用数据和项的内容(例如类目)。 \n* 优点:\n\t* 相对容易实现;\n\t* 良好的基准算法;\n\t* 有助于解决新用户冷启动问题。\n* 缺点:\n\t* 需要标准化产品;\n\t* 经常需要一些项的类型进行分类;\n\t* 不会推荐新项(很少有机会被观测到); \n\t* 推荐列表不会改变太大。\n\n## 5、高级非传统推荐算法\n* 类型\n\t* 深度学习\n\t* 学习等级\n\t* multi-armed bandits（探索／开发）\n\t* 上下文感知推荐\n\t* 张量分解\n\t* 分解机\n\t* 社会推荐\n* 优点：\n\t* 利于勉强维持最终性能百分点;\n\t* 你可以说你正在使用渐进的方式。\r* 缺点：\r\t* 难于理解;\n\t* 缺乏推荐工具支持\n\t* 没有为你的首个推荐系统提供推荐的方式。\n\n## 二、推荐系统结构实例\n### netfix\n首先是大名鼎鼎的netfix\n![](http://image.zhangxiaolong.org/mweb/a.png!origin)\n\n### 新浪微博\n国内我们看一下新浪\n![](http://image.zhangxiaolong.org/mweb/14720256571323.jpg!origin)\n\n### 基于内容的推荐\n基于内容和用户画像的个性化推荐属于基于内容的推荐。对于此种推荐，有两个实体：内容和用户，因此需要有一个联系这两者的东西，即为标签。内容转换为标签即为内容特征化，用户则称为用户特征化。对于此种推荐，主要分为以下几个关键部分：\n\n1. 标签库\n2. 内容特征化\n3. 用户特征化\n4. 隐语义推荐\n\n综合上面讲述的各个部分即可实现一个基于内容和用户画像的个性化推荐系统，\n![](http://image.zhangxiaolong.org/mweb/14728202025271.png!medium)\n\n（1）标签库\n标签是联系用户与物品、内容以及物品、内容之间的纽带，也是反应用户兴趣的重要数据源。标签库的最终用途在于对用户进行行为、属性标记。是将其他实体转换为计算机可以理解的语言关键的一步。\n\n标签库则是对标签进行聚合的系统，包括对标签的管理、更新等。\n\n一般来说，标签是以层级的形式组织的。可以有一级维度、二级维度等。\n标签的来源主要有：\n\n1. 已有内容的标签\n2. 网络抓取流行标签\n3. 对运营的内容进行关键词提取\n\n对于内容的关键词提取，使用结巴分词 + TFIDF即可。此外，也可以使用[TextRank](http://www.tuicool.com/articles/UZ77Z3)来提取内容关键词。\n\n这里需要注意的一点是对于**关联标签**的处理，比如用户的标签是足球，而内容的标签是德甲、英超，那么用户和内容是无法联系在一起的。最简单的方式是人工设置关联标签，此外也可以使用**word2vec**一类工具对标签做**聚类处理**，构建主题模型，将德甲、英超聚类到足球下面。\n（2） 内容特征化\n内容特征化即给内容打标签，目前有两种方式：人工打标签和机器自动打标签\n针对机器自动打标签，需要采取机器学习的相关算法来实现，即针对一系列给定的标签，给内容选取其中匹配度最高的几个标签。这不同于通常的分类和聚类算法。可以采取使用分词 + Word2Vec来实现，过程如下：\n\n1. 将文本语料进行分词，以空格,tab隔开都可以，使用结巴分词。\n2. 使用word2vec训练词的相似度模型。\n3. 使用tfidf提取内容的关键词A,B,C。\n4. 遍历每一个标签，计算关键词与此标签的相似度之和。\n5. 取出TopN相似度最高的标签即为此内容的标签。\n\n此外，可以使用文本主题挖掘相关技术，对内容进行特征化。这也分为两种情况:\n\n1. 通用情况下，只是为了效果优化的特征提取，那么可以使用非监督学习的主题模型算法。如LSA、PLSI和GaP模型或者LDA模型。\n2. 在和业务强相关时，需要在业务特定的标签体系下给内容打上适合的标签。这时候需要使用的是监督学习的主题模型。如sLDA、HSLDA等。\n\n（3） 用户特征化\n用户特征化即为用户打标签。通过用户的行为日志和一定的模型算法得到用户的每个标签的权重(泊松分布求解每个行为的权重；有监督的主题模型如SLDA；无监督的主题模型LSA)。\n\n1. 用户对内容的行为：点赞、不感兴趣、点击、浏览。对用户的反馈行为如点赞赋予权值1，不感兴趣赋予-1；对于用户的浏览行为，则可使用点击/浏览作为权值。\n2. 对内容发生的行为可以认为对此内容所带的标签的行为。\n3. 用户的兴趣是时间衰减的，即离当前时间越远的兴趣比重越低。时间衰减函数使用1/[log(t)+1], t为事件发生的时间距离当前时间的大小。\n4. 要考虑到热门内容会干预用户的标签，需要对热门内容进行降权。使用click/pv作为用户浏览行为权值即可达到此目的。\n5. 此外，还需要考虑噪声的干扰，如标题党等。\n\n另外，非业务强相关的情况下，还可以考虑使用LSA主题模型等矩阵分解的方式对用户进行标签化。\n\n（4） 隐语义推荐\n有了内容特征和用户特征，可以使用[隐语义模型](http://blog.csdn.net/harryhuang1990/article/details/9924377)进行推荐。这里可以使用其简化形式，以达到实时计算的目的。\n\n用户对于某一个内容的兴趣度(可以认为是CTR)：\n![](http://image.zhangxiaolong.org/mweb/14728206482938.jpg!origin)\n\n其中i=1…N是内容c具有的标签，m(ci)指的内容c和标签i的关联度(可以简单认为是1),n(ui)指的是用户u的标签i的权重值,当用户不具有此标签时n(ui)=0，q©指的是内容c的质量，可以使用点击率(click/pv)表示。\n\n（完）\n\n**- 写作于2017年9月28号，只是简稿**\n\n\n","slug":"自己动手打造推荐系统-一-常用推荐算法优势分析","published":1,"updated":"2017-09-30T08:58:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjasynwpe000b6ls6raf3tah3","content":"<p>本文包含两部分，一是常见算法类（部分文字摘自《推荐系统实践-项亮》），二是分析目前各个大公司放出来的推荐系统机构。</p>\n<h2 id=\"1、算法分析\"><a href=\"#1、算法分析\" class=\"headerlink\" title=\"1、算法分析\"></a>1、算法分析</h2><ul>\n<li><p>简介:通过在用户的一系列行为中寻找特定模式来产生用户特殊 推荐。</p>\n<p>  算法的核心思想就是:如果 两个用户对于一些项的评分相似程度较高,那么一个用户对于一个新项的 评分很有可能类似于另一个用户。</p>\n</li>\n<li>输入:仅仅依赖于惯用数据(例如评价、购买、下载等用户偏好 行为)。</li>\n<li>类型:<ul>\n<li>基于邻域的协同过滤(基于用户和基于项);<ul>\n<li>邻域方法(即基于内存的 CF)是使用用户 对已有项的评分直接预测该用户对新项的评分。基于邻域的 CF 方法意在找出项与项之间的联系(基于项的 CF),或 者用户与用户之间的联系(基于用户的 CF)。<ul>\n<li>基于用户的 CF 通过找出对项的偏好与你相似的用户从而基于他们对于新项的喜好来为你进行推荐。</li>\n<li>基于项的 CF 会向用户推荐与用户喜欢的项相似的项,这种相似是基于项的共同出现几率(例如用户买了 X,通时也买了 Y)。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>基于模型的协同过滤(矩阵因子分解、受限玻尔兹曼机、贝叶斯网络等)。 <ul>\n<li>基于模型的方 法是使用历史评分数据,基于学习出的预测模型,预测对新项的评分。基于模型的方法会在 使用评分去学习预测模型的基础上,去预测新项，一般的想法是使用机器 学习算法建立用户和项的相互作用模型,从而找出数据中的模式。贝叶斯网络、聚类、分类、回归、矩阵分解、受限玻尔兹曼机等等</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>优点:<ul>\n<li>需要最小域;</li>\n<li>不需要用户和项;</li>\n<li>大部分场景中能够产生足够好的结果。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>冷启动问题;</li>\n<li>需要标准化产品;</li>\n<li>需要很高的用户和项的比例(1:10);</li>\n<li>流行度偏见(有长尾的时候表现不够好); </li>\n<li>难于提供解释。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2、基于内容的推荐算法\"><a href=\"#2、基于内容的推荐算法\" class=\"headerlink\" title=\"2、基于内容的推荐算法\"></a>2、基于内容的推荐算法</h2><ul>\n<li><p>简介:向用户推荐和其过去喜欢项的内容(例如元数据、描述、话题等等)相似的项。</p>\n<p>  在基于内容的推荐中,假设可以获取到 item 的描述信息, 并将其作为 item 的特征向量(例如标题、年份、描述)。这些特征向量 被用于创建一个反映用户偏好的模型。各种信息检索(例如 TF-IDF)和 机器学习技术(例如朴素贝叶斯、支持向量机、决策树等)可被用于创建 用户模型,从而为用户产生推荐。</p>\n</li>\n<li><p>输入:仅仅依赖于项和用户的内容/描述(除了惯用数据)。</p>\n</li>\n<li>类型:<ul>\n<li>信息检索(例如 tf-idf 和 Okapi BM25);</li>\n<li>机器学习(例如朴素贝叶斯、支持向量机、决策树等)。</li>\n</ul>\n</li>\n<li>优点:<ul>\n<li>没有冷启动问题;</li>\n<li>不需要惯用数据;</li>\n<li>没有流行度偏见,可以推荐有罕见特性的项; ο 可以使用用户内容特性来提供解释。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>项内容必须是机器可读的和有意义的; ο 容易归档用户;</li>\n<li>很难有意外,缺少多样性;</li>\n<li>很难联合多个项的特性。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3、混合推荐算法\"><a href=\"#3、混合推荐算法\" class=\"headerlink\" title=\"3、混合推荐算法\"></a>3、混合推荐算法</h2><ul>\n<li>简介:综合利用协同过滤推荐算法和基于内容的推荐算法各自的 优点同时抵消各自的缺点。</li>\n<li>输入:同时使用用户和项的内容特性与惯用数据,同时从两种输 入类型中获益。</li>\n<li><p>类型:</p>\n<ul>\n<li>加权;</li>\n<li>交换;</li>\n<li>混合;</li>\n<li>特性组合;</li>\n<li>案列</li>\n<li>特征增强</li>\n<li>元层次</li>\n<li><img src=\"http://image.zhangxiaolong.org/mweb/14719522675930.jpg!medium\" alt=\"\"></li>\n</ul>\n</li>\n<li><p>优点:</p>\n<ul>\n<li>由于单独使用协同过滤推荐算法和基于内容的推荐算法;</li>\n<li>没有冷启动问题;</li>\n<li>没有流行度偏见,可推荐有罕见特性的项;</li>\n<li>可产生意外,实现多样性。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>需要通过大量的工作才能得到正确的平衡。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4、流行度推荐算法\"><a href=\"#4、流行度推荐算法\" class=\"headerlink\" title=\"4、流行度推荐算法\"></a>4、流行度推荐算法</h2><ul>\n<li>简介:这是一种推荐流行项的方法(例如最多下载、最多看过、最 大影响的项)。</li>\n<li>输入:使用惯用数据和项的内容(例如类目)。 </li>\n<li>优点:<ul>\n<li>相对容易实现;</li>\n<li>良好的基准算法;</li>\n<li>有助于解决新用户冷启动问题。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>需要标准化产品;</li>\n<li>经常需要一些项的类型进行分类;</li>\n<li>不会推荐新项(很少有机会被观测到); </li>\n<li>推荐列表不会改变太大。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5、高级非传统推荐算法\"><a href=\"#5、高级非传统推荐算法\" class=\"headerlink\" title=\"5、高级非传统推荐算法\"></a>5、高级非传统推荐算法</h2><ul>\n<li>类型<ul>\n<li>深度学习</li>\n<li>学习等级</li>\n<li>multi-armed bandits（探索／开发）</li>\n<li>上下文感知推荐</li>\n<li>张量分解</li>\n<li>分解机</li>\n<li>社会推荐</li>\n</ul>\n</li>\n<li>优点：<ul>\n<li>利于勉强维持最终性能百分点;</li>\n<li>你可以说你正在使用渐进的方式。</li>\n</ul>\n</li>\n<li>缺点：<ul>\n<li>难于理解;</li>\n<li>缺乏推荐工具支持</li>\n<li>没有为你的首个推荐系统提供推荐的方式。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"二、推荐系统结构实例\"><a href=\"#二、推荐系统结构实例\" class=\"headerlink\" title=\"二、推荐系统结构实例\"></a>二、推荐系统结构实例</h2><h3 id=\"netfix\"><a href=\"#netfix\" class=\"headerlink\" title=\"netfix\"></a>netfix</h3><p>首先是大名鼎鼎的netfix<br><img src=\"http://image.zhangxiaolong.org/mweb/a.png!origin\" alt=\"\"></p>\n<h3 id=\"新浪微博\"><a href=\"#新浪微博\" class=\"headerlink\" title=\"新浪微博\"></a>新浪微博</h3><p>国内我们看一下新浪<br><img src=\"http://image.zhangxiaolong.org/mweb/14720256571323.jpg!origin\" alt=\"\"></p>\n<h3 id=\"基于内容的推荐\"><a href=\"#基于内容的推荐\" class=\"headerlink\" title=\"基于内容的推荐\"></a>基于内容的推荐</h3><p>基于内容和用户画像的个性化推荐属于基于内容的推荐。对于此种推荐，有两个实体：内容和用户，因此需要有一个联系这两者的东西，即为标签。内容转换为标签即为内容特征化，用户则称为用户特征化。对于此种推荐，主要分为以下几个关键部分：</p>\n<ol>\n<li>标签库</li>\n<li>内容特征化</li>\n<li>用户特征化</li>\n<li>隐语义推荐</li>\n</ol>\n<p>综合上面讲述的各个部分即可实现一个基于内容和用户画像的个性化推荐系统，<br><img src=\"http://image.zhangxiaolong.org/mweb/14728202025271.png!medium\" alt=\"\"></p>\n<p>（1）标签库<br>标签是联系用户与物品、内容以及物品、内容之间的纽带，也是反应用户兴趣的重要数据源。标签库的最终用途在于对用户进行行为、属性标记。是将其他实体转换为计算机可以理解的语言关键的一步。</p>\n<p>标签库则是对标签进行聚合的系统，包括对标签的管理、更新等。</p>\n<p>一般来说，标签是以层级的形式组织的。可以有一级维度、二级维度等。<br>标签的来源主要有：</p>\n<ol>\n<li>已有内容的标签</li>\n<li>网络抓取流行标签</li>\n<li>对运营的内容进行关键词提取</li>\n</ol>\n<p>对于内容的关键词提取，使用结巴分词 + TFIDF即可。此外，也可以使用<a href=\"http://www.tuicool.com/articles/UZ77Z3\" target=\"_blank\" rel=\"external\">TextRank</a>来提取内容关键词。</p>\n<p>这里需要注意的一点是对于<strong>关联标签</strong>的处理，比如用户的标签是足球，而内容的标签是德甲、英超，那么用户和内容是无法联系在一起的。最简单的方式是人工设置关联标签，此外也可以使用<strong>word2vec</strong>一类工具对标签做<strong>聚类处理</strong>，构建主题模型，将德甲、英超聚类到足球下面。<br>（2） 内容特征化<br>内容特征化即给内容打标签，目前有两种方式：人工打标签和机器自动打标签<br>针对机器自动打标签，需要采取机器学习的相关算法来实现，即针对一系列给定的标签，给内容选取其中匹配度最高的几个标签。这不同于通常的分类和聚类算法。可以采取使用分词 + Word2Vec来实现，过程如下：</p>\n<ol>\n<li>将文本语料进行分词，以空格,tab隔开都可以，使用结巴分词。</li>\n<li>使用word2vec训练词的相似度模型。</li>\n<li>使用tfidf提取内容的关键词A,B,C。</li>\n<li>遍历每一个标签，计算关键词与此标签的相似度之和。</li>\n<li>取出TopN相似度最高的标签即为此内容的标签。</li>\n</ol>\n<p>此外，可以使用文本主题挖掘相关技术，对内容进行特征化。这也分为两种情况:</p>\n<ol>\n<li>通用情况下，只是为了效果优化的特征提取，那么可以使用非监督学习的主题模型算法。如LSA、PLSI和GaP模型或者LDA模型。</li>\n<li>在和业务强相关时，需要在业务特定的标签体系下给内容打上适合的标签。这时候需要使用的是监督学习的主题模型。如sLDA、HSLDA等。</li>\n</ol>\n<p>（3） 用户特征化<br>用户特征化即为用户打标签。通过用户的行为日志和一定的模型算法得到用户的每个标签的权重(泊松分布求解每个行为的权重；有监督的主题模型如SLDA；无监督的主题模型LSA)。</p>\n<ol>\n<li>用户对内容的行为：点赞、不感兴趣、点击、浏览。对用户的反馈行为如点赞赋予权值1，不感兴趣赋予-1；对于用户的浏览行为，则可使用点击/浏览作为权值。</li>\n<li>对内容发生的行为可以认为对此内容所带的标签的行为。</li>\n<li>用户的兴趣是时间衰减的，即离当前时间越远的兴趣比重越低。时间衰减函数使用1/[log(t)+1], t为事件发生的时间距离当前时间的大小。</li>\n<li>要考虑到热门内容会干预用户的标签，需要对热门内容进行降权。使用click/pv作为用户浏览行为权值即可达到此目的。</li>\n<li>此外，还需要考虑噪声的干扰，如标题党等。</li>\n</ol>\n<p>另外，非业务强相关的情况下，还可以考虑使用LSA主题模型等矩阵分解的方式对用户进行标签化。</p>\n<p>（4） 隐语义推荐<br>有了内容特征和用户特征，可以使用<a href=\"http://blog.csdn.net/harryhuang1990/article/details/9924377\" target=\"_blank\" rel=\"external\">隐语义模型</a>进行推荐。这里可以使用其简化形式，以达到实时计算的目的。</p>\n<p>用户对于某一个内容的兴趣度(可以认为是CTR)：<br><img src=\"http://image.zhangxiaolong.org/mweb/14728206482938.jpg!origin\" alt=\"\"></p>\n<p>其中i=1…N是内容c具有的标签，m(ci)指的内容c和标签i的关联度(可以简单认为是1),n(ui)指的是用户u的标签i的权重值,当用户不具有此标签时n(ui)=0，q©指的是内容c的质量，可以使用点击率(click/pv)表示。</p>\n<p>（完）</p>\n<p><strong>- 写作于2017年9月28号，只是简稿</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文包含两部分，一是常见算法类（部分文字摘自《推荐系统实践-项亮》），二是分析目前各个大公司放出来的推荐系统机构。</p>\n<h2 id=\"1、算法分析\"><a href=\"#1、算法分析\" class=\"headerlink\" title=\"1、算法分析\"></a>1、算法分析</h2><ul>\n<li><p>简介:通过在用户的一系列行为中寻找特定模式来产生用户特殊 推荐。</p>\n<p>  算法的核心思想就是:如果 两个用户对于一些项的评分相似程度较高,那么一个用户对于一个新项的 评分很有可能类似于另一个用户。</p>\n</li>\n<li>输入:仅仅依赖于惯用数据(例如评价、购买、下载等用户偏好 行为)。</li>\n<li>类型:<ul>\n<li>基于邻域的协同过滤(基于用户和基于项);<ul>\n<li>邻域方法(即基于内存的 CF)是使用用户 对已有项的评分直接预测该用户对新项的评分。基于邻域的 CF 方法意在找出项与项之间的联系(基于项的 CF),或 者用户与用户之间的联系(基于用户的 CF)。<ul>\n<li>基于用户的 CF 通过找出对项的偏好与你相似的用户从而基于他们对于新项的喜好来为你进行推荐。</li>\n<li>基于项的 CF 会向用户推荐与用户喜欢的项相似的项,这种相似是基于项的共同出现几率(例如用户买了 X,通时也买了 Y)。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>基于模型的协同过滤(矩阵因子分解、受限玻尔兹曼机、贝叶斯网络等)。 <ul>\n<li>基于模型的方 法是使用历史评分数据,基于学习出的预测模型,预测对新项的评分。基于模型的方法会在 使用评分去学习预测模型的基础上,去预测新项，一般的想法是使用机器 学习算法建立用户和项的相互作用模型,从而找出数据中的模式。贝叶斯网络、聚类、分类、回归、矩阵分解、受限玻尔兹曼机等等</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>优点:<ul>\n<li>需要最小域;</li>\n<li>不需要用户和项;</li>\n<li>大部分场景中能够产生足够好的结果。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>冷启动问题;</li>\n<li>需要标准化产品;</li>\n<li>需要很高的用户和项的比例(1:10);</li>\n<li>流行度偏见(有长尾的时候表现不够好); </li>\n<li>难于提供解释。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2、基于内容的推荐算法\"><a href=\"#2、基于内容的推荐算法\" class=\"headerlink\" title=\"2、基于内容的推荐算法\"></a>2、基于内容的推荐算法</h2><ul>\n<li><p>简介:向用户推荐和其过去喜欢项的内容(例如元数据、描述、话题等等)相似的项。</p>\n<p>  在基于内容的推荐中,假设可以获取到 item 的描述信息, 并将其作为 item 的特征向量(例如标题、年份、描述)。这些特征向量 被用于创建一个反映用户偏好的模型。各种信息检索(例如 TF-IDF)和 机器学习技术(例如朴素贝叶斯、支持向量机、决策树等)可被用于创建 用户模型,从而为用户产生推荐。</p>\n</li>\n<li><p>输入:仅仅依赖于项和用户的内容/描述(除了惯用数据)。</p>\n</li>\n<li>类型:<ul>\n<li>信息检索(例如 tf-idf 和 Okapi BM25);</li>\n<li>机器学习(例如朴素贝叶斯、支持向量机、决策树等)。</li>\n</ul>\n</li>\n<li>优点:<ul>\n<li>没有冷启动问题;</li>\n<li>不需要惯用数据;</li>\n<li>没有流行度偏见,可以推荐有罕见特性的项; ο 可以使用用户内容特性来提供解释。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>项内容必须是机器可读的和有意义的; ο 容易归档用户;</li>\n<li>很难有意外,缺少多样性;</li>\n<li>很难联合多个项的特性。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3、混合推荐算法\"><a href=\"#3、混合推荐算法\" class=\"headerlink\" title=\"3、混合推荐算法\"></a>3、混合推荐算法</h2><ul>\n<li>简介:综合利用协同过滤推荐算法和基于内容的推荐算法各自的 优点同时抵消各自的缺点。</li>\n<li>输入:同时使用用户和项的内容特性与惯用数据,同时从两种输 入类型中获益。</li>\n<li><p>类型:</p>\n<ul>\n<li>加权;</li>\n<li>交换;</li>\n<li>混合;</li>\n<li>特性组合;</li>\n<li>案列</li>\n<li>特征增强</li>\n<li>元层次</li>\n<li><img src=\"http://image.zhangxiaolong.org/mweb/14719522675930.jpg!medium\" alt=\"\"></li>\n</ul>\n</li>\n<li><p>优点:</p>\n<ul>\n<li>由于单独使用协同过滤推荐算法和基于内容的推荐算法;</li>\n<li>没有冷启动问题;</li>\n<li>没有流行度偏见,可推荐有罕见特性的项;</li>\n<li>可产生意外,实现多样性。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>需要通过大量的工作才能得到正确的平衡。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4、流行度推荐算法\"><a href=\"#4、流行度推荐算法\" class=\"headerlink\" title=\"4、流行度推荐算法\"></a>4、流行度推荐算法</h2><ul>\n<li>简介:这是一种推荐流行项的方法(例如最多下载、最多看过、最 大影响的项)。</li>\n<li>输入:使用惯用数据和项的内容(例如类目)。 </li>\n<li>优点:<ul>\n<li>相对容易实现;</li>\n<li>良好的基准算法;</li>\n<li>有助于解决新用户冷启动问题。</li>\n</ul>\n</li>\n<li>缺点:<ul>\n<li>需要标准化产品;</li>\n<li>经常需要一些项的类型进行分类;</li>\n<li>不会推荐新项(很少有机会被观测到); </li>\n<li>推荐列表不会改变太大。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5、高级非传统推荐算法\"><a href=\"#5、高级非传统推荐算法\" class=\"headerlink\" title=\"5、高级非传统推荐算法\"></a>5、高级非传统推荐算法</h2><ul>\n<li>类型<ul>\n<li>深度学习</li>\n<li>学习等级</li>\n<li>multi-armed bandits（探索／开发）</li>\n<li>上下文感知推荐</li>\n<li>张量分解</li>\n<li>分解机</li>\n<li>社会推荐</li>\n</ul>\n</li>\n<li>优点：<ul>\n<li>利于勉强维持最终性能百分点;</li>\n<li>你可以说你正在使用渐进的方式。</li>\n</ul>\n</li>\n<li>缺点：<ul>\n<li>难于理解;</li>\n<li>缺乏推荐工具支持</li>\n<li>没有为你的首个推荐系统提供推荐的方式。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"二、推荐系统结构实例\"><a href=\"#二、推荐系统结构实例\" class=\"headerlink\" title=\"二、推荐系统结构实例\"></a>二、推荐系统结构实例</h2><h3 id=\"netfix\"><a href=\"#netfix\" class=\"headerlink\" title=\"netfix\"></a>netfix</h3><p>首先是大名鼎鼎的netfix<br><img src=\"http://image.zhangxiaolong.org/mweb/a.png!origin\" alt=\"\"></p>\n<h3 id=\"新浪微博\"><a href=\"#新浪微博\" class=\"headerlink\" title=\"新浪微博\"></a>新浪微博</h3><p>国内我们看一下新浪<br><img src=\"http://image.zhangxiaolong.org/mweb/14720256571323.jpg!origin\" alt=\"\"></p>\n<h3 id=\"基于内容的推荐\"><a href=\"#基于内容的推荐\" class=\"headerlink\" title=\"基于内容的推荐\"></a>基于内容的推荐</h3><p>基于内容和用户画像的个性化推荐属于基于内容的推荐。对于此种推荐，有两个实体：内容和用户，因此需要有一个联系这两者的东西，即为标签。内容转换为标签即为内容特征化，用户则称为用户特征化。对于此种推荐，主要分为以下几个关键部分：</p>\n<ol>\n<li>标签库</li>\n<li>内容特征化</li>\n<li>用户特征化</li>\n<li>隐语义推荐</li>\n</ol>\n<p>综合上面讲述的各个部分即可实现一个基于内容和用户画像的个性化推荐系统，<br><img src=\"http://image.zhangxiaolong.org/mweb/14728202025271.png!medium\" alt=\"\"></p>\n<p>（1）标签库<br>标签是联系用户与物品、内容以及物品、内容之间的纽带，也是反应用户兴趣的重要数据源。标签库的最终用途在于对用户进行行为、属性标记。是将其他实体转换为计算机可以理解的语言关键的一步。</p>\n<p>标签库则是对标签进行聚合的系统，包括对标签的管理、更新等。</p>\n<p>一般来说，标签是以层级的形式组织的。可以有一级维度、二级维度等。<br>标签的来源主要有：</p>\n<ol>\n<li>已有内容的标签</li>\n<li>网络抓取流行标签</li>\n<li>对运营的内容进行关键词提取</li>\n</ol>\n<p>对于内容的关键词提取，使用结巴分词 + TFIDF即可。此外，也可以使用<a href=\"http://www.tuicool.com/articles/UZ77Z3\" target=\"_blank\" rel=\"external\">TextRank</a>来提取内容关键词。</p>\n<p>这里需要注意的一点是对于<strong>关联标签</strong>的处理，比如用户的标签是足球，而内容的标签是德甲、英超，那么用户和内容是无法联系在一起的。最简单的方式是人工设置关联标签，此外也可以使用<strong>word2vec</strong>一类工具对标签做<strong>聚类处理</strong>，构建主题模型，将德甲、英超聚类到足球下面。<br>（2） 内容特征化<br>内容特征化即给内容打标签，目前有两种方式：人工打标签和机器自动打标签<br>针对机器自动打标签，需要采取机器学习的相关算法来实现，即针对一系列给定的标签，给内容选取其中匹配度最高的几个标签。这不同于通常的分类和聚类算法。可以采取使用分词 + Word2Vec来实现，过程如下：</p>\n<ol>\n<li>将文本语料进行分词，以空格,tab隔开都可以，使用结巴分词。</li>\n<li>使用word2vec训练词的相似度模型。</li>\n<li>使用tfidf提取内容的关键词A,B,C。</li>\n<li>遍历每一个标签，计算关键词与此标签的相似度之和。</li>\n<li>取出TopN相似度最高的标签即为此内容的标签。</li>\n</ol>\n<p>此外，可以使用文本主题挖掘相关技术，对内容进行特征化。这也分为两种情况:</p>\n<ol>\n<li>通用情况下，只是为了效果优化的特征提取，那么可以使用非监督学习的主题模型算法。如LSA、PLSI和GaP模型或者LDA模型。</li>\n<li>在和业务强相关时，需要在业务特定的标签体系下给内容打上适合的标签。这时候需要使用的是监督学习的主题模型。如sLDA、HSLDA等。</li>\n</ol>\n<p>（3） 用户特征化<br>用户特征化即为用户打标签。通过用户的行为日志和一定的模型算法得到用户的每个标签的权重(泊松分布求解每个行为的权重；有监督的主题模型如SLDA；无监督的主题模型LSA)。</p>\n<ol>\n<li>用户对内容的行为：点赞、不感兴趣、点击、浏览。对用户的反馈行为如点赞赋予权值1，不感兴趣赋予-1；对于用户的浏览行为，则可使用点击/浏览作为权值。</li>\n<li>对内容发生的行为可以认为对此内容所带的标签的行为。</li>\n<li>用户的兴趣是时间衰减的，即离当前时间越远的兴趣比重越低。时间衰减函数使用1/[log(t)+1], t为事件发生的时间距离当前时间的大小。</li>\n<li>要考虑到热门内容会干预用户的标签，需要对热门内容进行降权。使用click/pv作为用户浏览行为权值即可达到此目的。</li>\n<li>此外，还需要考虑噪声的干扰，如标题党等。</li>\n</ol>\n<p>另外，非业务强相关的情况下，还可以考虑使用LSA主题模型等矩阵分解的方式对用户进行标签化。</p>\n<p>（4） 隐语义推荐<br>有了内容特征和用户特征，可以使用<a href=\"http://blog.csdn.net/harryhuang1990/article/details/9924377\" target=\"_blank\" rel=\"external\">隐语义模型</a>进行推荐。这里可以使用其简化形式，以达到实时计算的目的。</p>\n<p>用户对于某一个内容的兴趣度(可以认为是CTR)：<br><img src=\"http://image.zhangxiaolong.org/mweb/14728206482938.jpg!origin\" alt=\"\"></p>\n<p>其中i=1…N是内容c具有的标签，m(ci)指的内容c和标签i的关联度(可以简单认为是1),n(ui)指的是用户u的标签i的权重值,当用户不具有此标签时n(ui)=0，q©指的是内容c的质量，可以使用点击率(click/pv)表示。</p>\n<p>（完）</p>\n<p><strong>- 写作于2017年9月28号，只是简稿</strong></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjasynwp200016ls69klkmpnc","category_id":"cjasynwp800046ls6cwq8w193","_id":"cjasynwph000f6ls6en2t13s0"},{"post_id":"cjasynwp600036ls6xxm7ii55","category_id":"cjasynwp800046ls6cwq8w193","_id":"cjasynwpj000j6ls6cmbzl2vr"},{"post_id":"cjasynwpb00076ls6rnrnie0a","category_id":"cjasynwpi000g6ls6f0hdiai3","_id":"cjasynwpk000n6ls60dys4gaf"},{"post_id":"cjasynwpc00096ls62mdh1z6p","category_id":"cjasynwpj000l6ls6og5zb59x","_id":"cjasynwpm000r6ls65rmo8qsu"},{"post_id":"cjasynwpe000b6ls6raf3tah3","category_id":"cjasynwpi000g6ls6f0hdiai3","_id":"cjasynwpm000v6ls6zemxeppx"}],"PostTag":[{"post_id":"cjasynwp200016ls69klkmpnc","tag_id":"cjasynwpa00056ls6aqzmlrrb","_id":"cjasynwpj000i6ls6grckxfx6"},{"post_id":"cjasynwp200016ls69klkmpnc","tag_id":"cjasynwpf000d6ls6x3qxd0pm","_id":"cjasynwpj000k6ls68knsr3ph"},{"post_id":"cjasynwp600036ls6xxm7ii55","tag_id":"cjasynwpa00056ls6aqzmlrrb","_id":"cjasynwpl000q6ls6l18310zg"},{"post_id":"cjasynwp600036ls6xxm7ii55","tag_id":"cjasynwpk000m6ls67c8pmm59","_id":"cjasynwpm000s6ls6ep0beg11"},{"post_id":"cjasynwpb00076ls6rnrnie0a","tag_id":"cjasynwpa00056ls6aqzmlrrb","_id":"cjasynwpm000u6ls6l1en5zyx"},{"post_id":"cjasynwpc00096ls62mdh1z6p","tag_id":"cjasynwpm000t6ls6xcmkw03k","_id":"cjasynwpn000x6ls66l2mhrb1"},{"post_id":"cjasynwpe000b6ls6raf3tah3","tag_id":"cjasynwpn000w6ls600ga5ayu","_id":"cjasynwpn000y6ls68rhmcss8"}],"Tag":[{"name":"deep learning","_id":"cjasynwpa00056ls6aqzmlrrb"},{"name":"regularization","_id":"cjasynwpf000d6ls6x3qxd0pm"},{"name":"GAN","_id":"cjasynwpk000m6ls67c8pmm59"},{"name":"search","_id":"cjasynwpm000t6ls6xcmkw03k"},{"name":"resys","_id":"cjasynwpn000w6ls600ga5ayu"}]}}